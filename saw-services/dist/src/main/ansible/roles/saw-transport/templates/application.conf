play.crypto.secret="PbJ2]H^WhFEBqKT3rlNtJMY1i^PHUX>eMjiPyaC`w5RB[W]jihkhK?Vb1m`gJMw5"

# CORS filter configuration
play.filters.cors {
    # The path prefixes to filter.
    pathPrefixes = ["/"]

    # The allowed origins. If null, all origins are allowed.
    allowedOrigins = null

    # The allowed HTTP methods. If null, all methods are allowed
    allowedHttpMethods = [ "POST", "GET", "OPTIONS" ]

    # The allowed HTTP headers. If null, all headers are allowed.
    allowedHttpHeaders = null

    # The exposed headers
    exposedHeaders = []

    # Whether to support credentials
    supportsCredentials = true

    # The maximum amount of time the CORS meta data should be cached by the client
    preflightMaxAge = 1 hour
}

play.http.filters = "controllers.Filters"

play.server {

  # The server provider class name
  provider = "play.core.server.NettyServerProvider"

  netty {

    # The number of event loop threads. 0 means let Netty decide, which by default will select 2 times the number of
    # available processors.
    eventLoopThreads = 0

    # The maximum length of the initial line. This effectively restricts the maximum length of a URL that the server will
    # accept, the initial line consists of the method (3-7 characters), the URL, and the HTTP version (8 characters),
    # including typical whitespace, the maximum URL length will be this number - 18.
    maxInitialLineLength = 16384

    # The maximum length of the HTTP headers. The most common effect of this is a restriction in cookie length, including
    # number of cookies and size of cookie values.
    maxHeaderSize = 16384

    # The maximum length of body bytes that Netty will read into memory at a time.
    # This is used in many ways.  Note that this setting has no relation to HTTP chunked transfer encoding - Netty will
    # read "chunks", that is, byte buffers worth of content at a time and pass it to Play, regardless of whether the body
    # is using HTTP chunked transfer encoding.  A single HTTP chunk could span multiple Netty chunks if it exceeds this.
    # A body that is not HTTP chunked will span multiple Netty chunks if it exceeds this or if no content length is
    # specified. This only controls the maximum length of the Netty chunk byte buffers.
    maxChunkSize = 8192

    # Whether the Netty wire should be logged
    log.wire = false

    # The transport to use, either jdk or native.
    # Native socket transport has higher performance and produces less garbage but are only available on linux
    transport = "jdk"

    # Netty options. Possible keys here are defined by:
    #
    # http://netty.io/4.0/api/io/netty/channel/ChannelOption.html
    #
    # Options that pertain to the listening server socket are defined at the top level, options for the sockets associated
    # with received client connections are prefixed with child.*
    option {

      # Set the size of the backlog of TCP connections.  The default and exact meaning of this parameter is JDK specific.
      # SO_BACKLOG = 100

      child {
        # Set whether connections should use TCP keep alive
        # SO_KEEPALIVE = false

        # Set whether the TCP no delay flag is set
        # TCP_NODELAY = false
      }

    }

  }
}



# Port must be passed to play as CMD parameter, THERE IS NO DEFAULT VALUE
http.port=9201
play.i18n.langs = [ "en" ]

es = {
  #  es 5
  host = "{{ saw_elasticsearch_host  | default('localhost') }}"
  timeout = 30
  # This has been replaced with '9200' because it's of no use anymore & to avoid exception. 
  # To be safer side we have not removed this section of ES from transport service.
  # This will be part of refactoring task of Analysis Service
  port = 9200
  username = "{{ saw_elasticsearch_username | default('elastic') }}"
  password = "{{ saw_elasticsearch_password | default('elastic') }}"
  protocol = "{{ saw_elasticsearch_protocol | default('http') }}"
  inline-es-report-data-store-limit-rows = {{ saw_elasticsearch_data_store_limit | default('10000') }}
}
semantic = {
  host = "http://localhost:9900"
  endpoint = "/internal/semantic/workbench/"
}
metadata = {
  path = "{{ sip_var_path | default('/var/sip') }}/services/metadata"
  zookeeper-quorum = "{{ saw_zookeeper_quorum | default('localhost') }}"
  user = "mapr"
}

report.executor = {
    path= "{{ sip_var_path | default('/var/sip') }}/services/transport/executor"
  }

{% if saw_execution_history is defined %}
execution.history = "{{ saw_execution_history }}"
{% endif %}

spark = {
  master = "{{ saw_spark_master_url | default('spark://localhost:7077') }}"
{% if saw_spark_driver_port is defined %}
  driver.port = "{{ saw_spark_driver_port }}"
{% endif %}
{% if saw_spark_driver_host is defined %}
  driver.host = "{{ saw_spark_driver_host }}"
{% endif %}
{% if saw_spark_executor_instances is defined %}
  executor.instances = "{{ saw_spark_executor_instances }}"
{% endif %}
{% if saw_spark_driver_bind_address is defined %}
  driver.bindAddress = "{{ saw_spark_driver_bind_address }}"
{% endif %}
{% if saw_spark_driver_blockmanager_port is defined %}
  driver.blockManager.port = "{{ saw_spark_driver_blockmanager_port }}"
{% endif %}
  executor.memory.regular = "{{ saw_executor_memory_regular | default('8G') }}"
  executor.memory.fast = "{{ saw_executor_memory_fast | default('2G') }}"
  cores.max.regular = "{{ saw_executor_cores_regular | default('8') }}"
  cores.max.fast = "{{ saw_executor_cores_fast | default('2') }}"
  {% if saw_spark_executor_instances_regular is defined %}
  executor.instances.regular = "{{ saw_spark_executor_instances_regular }}"
  {% endif %}
  {% if saw_spark_executor_instances_fast is defined %}
  executor.instances.fast = "{{ saw_spark_executor_instances_fast }}"
  {% endif %}
  driver.memory = "2G"
  yarn = {
    {% if saw_spark_yarn_queue_regular is defined %}
    queue.regular = "{{ saw_spark_yarn_queue_regular }}"
    {% endif %}
    {% if saw_spark_yarn_queue_fast is defined %}
    queue.fast = "{{ saw_spark_yarn_queue_fast }}"
    {% endif %}
    {% if saw_spark_yarn_jars is defined %}
    spark.jars = "{{ saw_spark_yarn_jars }}"
    {% endif %}
    {% if saw_spark_yarn_zips is defined %}
    spark.zips = "{{ saw_spark_yarn_zips }}"
    {% endif %}
    {% if saw_spark_yarn_resource_manager is defined %}
    resourcemanager.hostname = "{{ saw_spark_yarn_resource_manager }}"
    {% endif %}
}
  sql-executor = {
    # --------- Not used anymore ------------------------------
    input-file-location = "/main/data/saw/sql-executor/input"
    result-file-location = "/main/data/saw/sql-executor/results"
    script = "/opt/saw/sql-executor/bin/run.sh"
    # ----------------------------------------------------------

    wait-time = "{{ saw_executor_result_wait_time | default('60') }}"
    inline-data-store-limit-bytes = 268435456
    preview-rows-limit = "{{ saw_preview_rows_limit | default('10000') }}"
    publish-rows-limit = "{{ saw_publish_rows_limit | default('-1') }}"

    # The location is to store results genereted by SQL Executor
    output-location = "{{ sip_var_path | default('/var/sip') }}/services/transport/executor/output"
    semantic-layer-tmp-location = "{{ sip_var_path | default('/var/sip') }}/services/transport/tmp"

    # Use line-delimited JSON as the output format by default, for
    # simplicity of streaming results out of the data lake
    output-type = "json"

    jar-location = "/opt/saw/service/sparklib"
  }
}
