play.crypto.secret="PbJ2]H^WhFEBqKT3rlNtJMY1i^PHUX>eMjiPyaC`w5RB[W]jihkhK?Vb1m`gJMw5"

# CORS filter configuration
play.filters.cors {
  # The path prefixes to filter.
  pathPrefixes = ["/"]

  # The allowed origins. If null, all origins are allowed.
  allowedOrigins = null

  # The allowed HTTP methods. If null, all methods are allowed
  allowedHttpMethods = [ "POST", "GET", "OPTIONS" ]

  # The allowed HTTP headers. If null, all headers are allowed.
  allowedHttpHeaders = null

  # The exposed headers
  exposedHeaders = []

  # Whether to support credentials
  supportsCredentials = true

  # The maximum amount of time the CORS meta data should be cached by the client
  preflightMaxAge = 1 hour
}

play.http.filters = "controllers.Filters"

play.server {

  # The server provider class name
  provider = "play.core.server.NettyServerProvider"

  netty {

    # The number of event loop threads. 0 means let Netty decide, which by default will select 2 times the number of
    # available processors.
    eventLoopThreads = 0

    # The maximum length of the initial line. This effectively restricts the maximum length of a URL that the server will
    # accept, the initial line consists of the method (3-7 characters), the URL, and the HTTP version (8 characters),
    # including typical whitespace, the maximum URL length will be this number - 18.
    maxInitialLineLength = 16384

    # The maximum length of the HTTP headers. The most common effect of this is a restriction in cookie length, including
    # number of cookies and size of cookie values.
    maxHeaderSize = 65536

    # The maximum length of body bytes that Netty will read into memory at a time.
    # This is used in many ways.  Note that this setting has no relation to HTTP chunked transfer encoding - Netty will
    # read "chunks", that is, byte buffers worth of content at a time and pass it to Play, regardless of whether the body
    # is using HTTP chunked transfer encoding.  A single HTTP chunk could span multiple Netty chunks if it exceeds this.
    # A body that is not HTTP chunked will span multiple Netty chunks if it exceeds this or if no content length is
    # specified. This only controls the maximum length of the Netty chunk byte buffers.
    maxChunkSize = 8192

    # Whether the Netty wire should be logged
    log.wire = false

    # The transport to use, either jdk or native.
    # Native socket transport has higher performance and produces less garbage but are only available on linux
    transport = "jdk"

    # Netty options. Possible keys here are defined by:
    #
    # http://netty.io/4.0/api/io/netty/channel/ChannelOption.html
    #
    # Options that pertain to the listening server socket are defined at the top level, options for the sockets associated
    # with received client connections are prefixed with child.*
    option {

      # Set the size of the backlog of TCP connections.  The default and exact meaning of this parameter is JDK specific.
      # SO_BACKLOG = 100

      child {
        # Set whether connections should use TCP keep alive
        # SO_KEEPALIVE = false

        # Set whether the TCP no delay flag is set
        # TCP_NODELAY = false
      }

    }

  }
}



# Port must be passed to play as CMD parameter, THERE IS NO DEFAULT VALUE
http.port=9201
play.i18n.langs = [ "en" ]

es = {
  #  es 5
  host = "sip-elastic"
  timeout = 30
  port = 8200
  username = ""
  password = ""
  protocol = "http"
  inline-es-report-data-store-limit-rows = 10000
}

semantic = {
  host = "http://localhost:9900"
  endpoint = "/internal/semantic/workbench/"
}

metadata = {
  path = "/var/sip/services/metadata"
  zookeeper-quorum = "sip-mapr"
  user = "mapr"
}

report.executor = {
  path= "/var/sip"
}

# this property will be use to limit the number of execution history to retain.
execution.history = 5

spark = {
  master = "local[*]"
  executor.memory.regular = "512M"
  executor.memory.fast = "512M"
  executor.instances.fast="1"
  executor.instances.regular="2"
  cores.max.regular = "2"
  cores.max.fast = "2"
  driver.memory = "2G"
  spark.driver.port.fast = "9801"
  spark.driver.port.regular = "9802"
  spark.blockManager.port.fast = "9803"
  spark.blockManager.port.regular = "9803"

  # This will contains all the configuration
  yarn = {

    }

  sql-executor = {
    # --------- Not used anymore ------------------------------
    input-file-location = "/var/sip/services/data/saw/sql-executor/input"
    result-file-location = "/var/sip/services/data/saw/sql-executor/results"
    script = "/opt/saw/sql-executor/bin/run.sh"
    # ----------------------------------------------------------

    wait-time = "120"
    inline-data-store-limit-bytes = 268435456
    preview-rows-limit = 10000
    publish-rows-limit = 100000
    executor-restart-threshold=100

    # The location is to store results genereted by SQL Executor
    output-location = "/var/sip/services/data/saw/sql-executor/output"
    semantic-layer-tmp-location = "/main/data/saw/sql-executor/temp"

    # Use line-delimited JSON as the output format by default, for
    # simplicity of streaming results out of the data lake
    output-type = "json"

    jar-location = "/opt/saw/service/sparklib"
  }
}
