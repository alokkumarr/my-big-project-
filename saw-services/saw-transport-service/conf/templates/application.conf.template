play.crypto.secret=${?APPLICATION_SECRET} 

APPLICATION_SECRET="PbJ2]H^WhFEBqKT3rlNtJMY1i^PHUX>eMjiPyaC`w5RB[W]jihkhK?Vb1m`gJMw5"

# CORS filter configuration
play.filters.cors {

  # The path prefixes to filter.
  pathPrefixes = ["/"]

  # The allowed origins. If null, all origins are allowed.
  allowedOrigins = null

  # The allowed HTTP methods. If null, all methods are allowed
  allowedHttpMethods = [ "POST", "GET", "OPTIONS" ]

  # The allowed HTTP headers. If null, all headers are allowed.
  allowedHttpHeaders = null

  # The exposed headers
  exposedHeaders = []

  # Whether to support credentials
  supportsCredentials = true

  # The maximum amount of time the CORS meta data should be cached by the client
  preflightMaxAge = 1 hour
}

play.http.filters = "controllers.Filters"


# Port must be passed to play as CMD parameter, THERE IS NO DEFAULT VALUE
http.port=9201
play.i18n.langs = [ "en" ]

es = {

  #  es 5
  host = "${:es.host:}" # "10.48.72.74"
  timeout = 30
  port = 9200
  username = "${:es.username:}"
  password = "${:es.password:}"
  protocol = "http"

}

metadata = {

  path = "${:metadata.path:}" # "/main/metadata"
  zookeeper-quorum = "#{:metadata.zookeeper-quorum:}" # mapr-dev01.sncrbda.dev.vacum-np.sncrcorp.net,mapr-dev02.sncrbda.dev.vacum-np.sncrcorp.net,mapr-dev03.sncrbda.dev.vacum-np.sncrcorp.net,mapr-dev04.sncrbda.dev.vacum-np.sncrcorp.net,mapr-dev05.sncrbda.dev.vacum-np.sncrcorp.net"
  user = "${:metadata.user:}" # mapr"

}


spark = {
  master = "${:spark.master:}" #"spark://mapr-dev02.sncrbda.dev.vacum-np.sncrcorp.net:7077"
  executor.memory = "2G"
  cores.max = "2"
  driver.memory = "2G"

  sql-executor = { # spark.sql.

    input-file-location = "${:spark.sql.input:}" # "/main/data/saw/sql-executor/input"
    result-file-location = "${:spark.sql.result:}" # "/main/data/saw/sql-executor/results"
    script = "${:spark.sql.script:}" # "/opt/saw/sql-executor/bin/run.sh"
    wait-time = 5

    inline-data-store-limit-bytes = 268435456
    inline-data-store-limit-rows = 10
    output-location = "${:spark.sql.output:}" # "/mapr/bda_lab_batch/data/bda/RAP/dout"
    semantic-layer-tmp-location = "${:spark.sql.temp:}" # "/mapr/bda_lab_batch/data/bda/RAP/temp"

    jar-location = "${?SQL_EXECUTOR_HOME}/sparklib"

  }

}

# host = "${:es.host:}" # "10.48.72.74"
# username = "${:es.username:}"
# password = "${:es.password:}"
# path = "${:metadata.path:}" # "/main/metadata"
# user = "${:metadata.user:}" # mapr"
# master = "${:spark.master:}" #"spark://mapr-dev02.sncrbda.dev.vacum-np.sncrcorp.net:7077"
# input-file-location = "${:spark.sql.input:}" # "/main/data/saw/sql-executor/input"
# result-file-location = "${:spark.sql.result:}" # "/main/data/saw/sql-executor/results"
# script = "${:spark.sql.script:}" # "/opt/saw/sql-executor/bin/run.sh"
# output-location = "${:spark.sql.output:}" # "/mapr/bda_lab_batch/data/bda/RAP/dout"
# semantic-layer-tmp-location = "${:spark.sql.temp:}" # "/mapr/bda_lab_batch/data/bda/RAP/temp"
