

# Modeler Class Functions -------------------------------------------------



#' New Modeler Object Constructer function
#' @import sparklyr
#' @import dplyr
#' @inheritParams modeler
new_modeler <- function(df,
                        schema,
                        target,
                        type,
                        name,
                        uid,
                        version,
                        desc,
                        scientist,
                        dir,
                        execution_strategy,
                        refit,
                        save_submodels,
                        samples,
                        pipelines,
                        models,
                        measure,
                        performance,
                        final_model,
                        seed,
                        ...){
  
  checkmate::assert_list(schema)
  checkmate::assert_character(target, null.ok = TRUE)
  checkmate::assert_character(type)
  checkmate::assert_character(name)
  checkmate::assert_character(uid)
  checkmate::assert_character(version)
  checkmate::assert_character(desc)
  checkmate::assert_character(scientist)
  checkmate::test_path_for_output(dir)
  checkmate::assert_choice(execution_strategy,
                           c("sequential", "transparent", "multisession", "multicore",
                             "multiprocess", "cluster", "remote"))
  checkmate::assert_flag(refit)
  checkmate::assert_flag(save_submodels)
  checkmate::assert_class(samples, "samples")
  checkmate::assert_list(pipelines)
  checkmate::assert_list(models)
  checkmate::assert_list(measure)
  checkmate::assert_data_frame(performance)
  checkmate::assert_class(final_model, "model", null.ok = TRUE)
  checkmate::assert_number(seed, lower = 1)
  
  structure(
    list(
      type = type,
      name = name,
      uid = uid,
      version = version,
      desc = desc,
      scientist = scientist,
      created_on = Sys.time(),
      data = df,
      schema = schema,
      target = target,
      dir = dir,
      execution_strategy = execution_strategy,
      refit = refit,
      save_submodels = save_submodels,
      samples = samples,
      pipelines = pipelines,
      models = models,
      measure = measure,
      performance = performance,
      final_model = final_model,
      seed = seed,
      ...
    ),
    class = "modeler")
}


#' Modeler Validation function
valid_modeler <- function(obj){
  
  if (! obj$type %in% c("forecaster", "segmenter", "regressor",
                        "classifier", "multiclassifier")) {
    stop("modeler type supplied not supported. Please use one of following: ",
         "\n* forecaster",
         "\n* segmenter",
         "\n* regressor",
         "\n* classifier")
  }
  
  if(! any(class(obj$data) %in% c("data.frame", "tbl_spark"))){
    stop("df input class not supported. modeler supports only data.frame and tbl_spark objects")
  }
  
  if("rn" %in% colnames(obj$data)) {
    stop("data input contains column name 'rn' which conflicts with internal data processing.\n Please update column name or remove")
  }
  
  obj
}


#' Modeler Helper Function
#'
#' Function to create a valid modeler object to support model building use
#' cases. Modeler is the base class from which the other more specific use cases
#' like forecaster, regressor, segmenter, etc... inherit form
#'
#' @param df dataframe with target variable and any potential predictors
#' @param target target variable for supervised use cases such as forecaster,
#'   regressor, and classifier
#' @param type type of use case (forecaster, regressor, classifier, and
#'   segmenter)
#' @param name optional name for use case. Default is to append target with type
#' @param uid optional uid string. A random uid will be generated by default
#' @param version optional version string. will default to '1.0'
#' @param desc optional description of the use case.
#' @param scientist optional input for scientist building model. defaults to
#'   system user
#' @param execution_strategy name of evaluation function to use for resolving a
#'   future. Used for model training execution. default is sequential
#' @param refit logical option to refit model to entire training dataset sample.
#'   If grid parameters supplied, the best param set determined by validation
#'   sample performance will be used. Default is true
#' @param save_submodels logical flag to save sub model fits. Default is TRUE
#' @param dir optional directory path for saving reports, data and models
#' @param seed seed number of any random computations
#' @param ... any addtional parameters
#'
#' @return modeler object
#' @export
#' @family use cases
#' @aliases modeler
#' @importFrom stats coef fitted predict setNames ts
#' @importFrom utils head modifyList
modeler <- function(df,
                    target,
                    type,
                    name = NULL,
                    uid = NULL,
                    version = NULL,
                    desc = NULL,
                    scientist = NULL,
                    execution_strategy = "sequential",
                    refit = TRUE,
                    save_submodels = TRUE,
                    dir = NULL,
                    seed = 319,
                    ...){
  
  if(is.null(name)) name <- paste(target, type, sep = "-")
  if(is.null(uid)) uid <- sparklyr::random_string("modeler")
  if(is.null(version)) version <- "1.0"
  if(is.null(desc)) desc <- ""
  if(is.null(scientist)) scientist <- Sys.info()["user"]
  if(is.null(dir)) dir <- "./"
  default_samples <- add_default_samples(df)
  empty_pipes <- list()
  empty_models <- list()
  measure <- list()
  performance <- data.frame()
  final_model <- NULL
  schema <- a2munge::get_schema(df)
  
  valid_modeler(
    new_modeler(
      df = df,
      schema = schema,
      target = target,
      type = type,
      name = name,
      uid = uid,
      version = version,
      desc = desc,
      scientist = scientist,
      dir = dir,
      execution_strategy = execution_strategy,
      refit = refit,
      save_submodels = save_submodels,
      samples = default_samples,
      pipelines = empty_pipes,
      models = empty_models,
      measure = measure,
      performance = performance,
      final_model = final_model,
      seed = seed,
      ...)
  )
}



# Modeler Functions -------------------------------------------------------



#' Get Models Status
#'
#' Function to return modeler's model statuses
#'
#' @param obj modeler object
#' @param uid one or more model uid characters
#' @export
get_models_status <- function(obj, uid = NULL){
  purrr::map(obj$models, "status")
}


#' Get All Modeler Models
#'
#' Function to access all modeler models. Subsettable by either uid or status
#'
#' @param obj modeler object
#' @param uids one or more model uid characters
#' @param status model status code. accepts "created", "added", "trained", or
#'   "selected"
#'
#' @return list of model objects
#' @export
get_models <- function(obj, uids = NULL, status = NULL) {
  checkmate::assert_class(obj, "modeler")
  checkmate::assert_character(uids, null.ok = TRUE)
  checkmate::assert_choice(status,
                           choices = c("created", "added", "trained", "selected", "final"),
                           null.ok = TRUE)
  
  models <- obj$models
  if(! is.null(uids))
    models <- purrr::keep(obj$models, names(obj$models) %in% uids)
  
  if(! is.null(status))
    models <- purrr::keep(models, get_models_status(models) == status)
  
  models
}



#' Get the Best Performing Model
#'
#' Function to select the best performing model based on modeler measure
#'
#' @param obj modeler object
#' @export
#' @return returns best model
get_best_model <- function(obj) {
  checkmate::assert_class(obj, "modeler")
  
  uid <- obj$performance %>%
    dplyr::arrange_at(obj$measure$method,
                      .funs = ifelse(obj$measure$minimize, identity, dplyr::desc)) %>%
    head(1) %>%
    dplyr::pull(model_uid)
  obj$models[[uid]]
}



#' Deploy Modeler Object
#'
#' Save Modeler object to file that can be restored later. Requires a final
#' model to be set. Deploy uses \code{\link{saveRDS}} function to serialize the
#' R object. Deploy uses \code{\link{sparklyr::ml_save}} to serialize final
#' model fit.
#'
#' Deploy function creates a new directory at path location and writes a2modeler
#' object and final model within
#'
#' @param obj modeler object to save
#' @param path directory path to write a2modeler object to
#' @param ... additional arguments to pass to \link[base]{saveRDS} function
#'
#' @export
deploy <- function(obj, path, ...) {
  checkmate::assert_class(obj, "modeler")
  checkmate::assert_directory(dirname(path))
 
  # Check for final model
  if(is.null(obj$final_model)) {
    stop("Deployment failed - Final Model not set")
  }
  
  # Created model label
  label <- paste(obj$name, obj$version, sep="-")
  modeler_path <- paste(path, label, sep="/")
  modeler_file <- paste(modeler_path, paste0(label, ".rds"), sep="/")
  model_path <- paste(modeler_path, obj$final_model$uid, sep="/")
  obj$final_model$path <- model_path
  
  # Remove Spark objects from obj
  if("tbl_spark" %in% class(obj$data)) obj$data <- NULL
  for(i in seq_along(obj$models)) {
    if("spark_model" %in% class(obj$models[[1]])){
      obj$models[[i]]$fit <- NULL
      obj$models[[i]]$sub_models <- NULL
    }
  }
  for(i in seq_along(obj$pipelines)) {
    if("tbl_spark" %in% class(obj$pipelines[[i]]$output)) {
      obj$pipelines[[i]]$output <- NULL
    }
  }
  
  # Create directory & write modeler and Final Model
  dir.create(modeler_path, recursive = TRUE)
  if("spark_model" %in% class(obj$final_model)) {
    sparklyr::ml_save(obj$final_model$fit, model_path)
  }
  
  saveRDS(object = obj, file = modeler_file, ...)
}


#' a2Model Load
#'
#' Load a2 modeler object into memory to apply on new datasets
#'
#' @param path directory path to write a2modeler object to
#' @param sc optional spark connection argument. Required to load an a2modeler
#'   object with a spark_model final model type
#'
#' @export
a2_load <- function(path, sc = NULL) {
  checkmate::assert_directory(dirname(path))
  checkmate::assert_class(sc, "spark_connection", null.ok = TRUE)
  
  # Get modeler file
  modeler_file <- list.files(path = path, pattern = ".rds", full.names = TRUE)
  
  if(length(modeler_file) == 0) {
    stop("Model Meta-data file not found\nLooking for .rds file")
  }
  
  # Load Meta-data and Model
  obj <- readRDS(file = modeler_file)
  if(! is.null(sc)) {
    obj$final_model$fit <- sparklyr::ml_load(sc, path = obj$final_model$path)
  }
  
  obj
}




# Modeler Class Generics --------------------------------------------------



#' Execute Pipelines Generic Function
#'
#' Function executes all pipelines added to modeler object
#'
#' Only executes previous un-executed pipelines
#'
#' @param obj modeler object
#' @param ... additional arguments to pass to execute function
#'
#' @return updated modeler object
#' @export
execute_pipelines <- function(obj, ...) {
  UseMethod("execute_pipelines")
}


#' Train Models Generic Function
#'
#' Train all models added to a modeler object
#'
#' Function looks for all untrained models added to a modeler project. Function
#' fits model based on modeler samples, model method and model method args.
#' Function saves model fit, all fitted predictions to the training sample and
#' any validation predictions
#'
#' @param obj modeler object
#' @param uids optional input to train single model by its uid. default is NULL
#'   which trains all previously un-trained models
#' @param ... additional arguments to pass to train function
#'
#' @export
#' @return updated modeler object
train_models <- function(obj, uids, ...) {
  UseMethod("train_models")
}


#' Set Final Model Generic
#'
#' Function to select the final model. See method argument for options
#'
#' @param obj forecaster object
#' @param method selection method. Choices are 'manual' which requires a valid
#'   uid or 'best' method which selects the best model based on the measure
#' @param uid optional model uid input required for manual method
#' @param reevaluate logical option to re-evaluate the final model. Requires a
#'   test holdout sample
#' @param refit logical option to re-fit the final model. Requires a test
#'   holdout sample
#' @export
set_final_model <- function(obj, method, uid, reevaluate, refit) {
  UseMethod("set_final_model")
}


#' Get Model Performance
#'
#' Returns data.frame with model performance
#'
#' @param obj object to extract target from
#' @export
get_performance <- function(obj){
  UseMethod("get_performance", obj)
}


#' Get Target Data function
#'
#' Returns modeler target data
#'
#' @param obj object to extract target from
#' @export
get_target <- function(obj) {
  UseMethod("get_target", obj)
}


#' Refit Modeler Object with New Data
#'
#' Refits a modeler's final model to new data. Option to append new data to
#' existing data and fit on combined data or fit on just new data
#'
#' Requires newdata have similar schema to existing modeler data
#'
#' @param obj modeler object
#' @param df new dataframe
#' @param append logical option to append new dataframe to exisiting modeler
#'   data
#' @param ... additional arguments to pass to refit function. Not currently
#'   implemented
#'
#' @return updated modeler object with new final model
#' @export
#'
#' @examples
refit <- function(obj, df, append = TRUE, ...) {
  UseMethod("refit", obj)
  checkmate::assert_class(obj, "modeler")
}

# Modeler Class Methods ---------------------------------------------------


#' @param uids optional string pipeline uid to execute. default is NULL which
#'   executes all pipelines
#' @rdname execute_pipelines
#' @export
execute_pipelines.modeler <- function(obj, uids = NULL, ...) {
  checkmate::assert_character(uids, null.ok = TRUE)
  
  if (is.null(uids))
    uids <- names(obj$pipelines)
  
  for(uid in uids) {
    pipe <- obj$pipelines[[uid]]
    if(is.null(pipe$output))
      obj$pipelines[[uid]] <- execute(obj$data, obj$pipelines[[uid]])
  }
  obj
}


#' @rdname train_models
#' @export
train_models.modeler <- function(obj, uids = NULL, ...) {
  checkmate::assert_character(uids, null.ok = TRUE)
  
  # Execute pipes
  obj <- execute_pipelines(obj)
  
  if (is.null(uids)) {
    # Get Added only Models
    uids <- obj$models %>%
      purrr::map_df(., magrittr::extract, c("uid", "status")) %>%
      dplyr::filter(status == "added") %>%
      dplyr::pull(uid)
  }
  
  for (uid in uids) {
    
    # Get Training Sample
    train_data <- obj$pipelines[[obj$models[[uid]]$pipe]]$output
    
    # Check for holdout
    if (!is.null(obj$samples$test_holdout_prct)) {
      train_data <- train_data %>%
        dplyr::mutate(rn = 1) %>% 
        dplyr::mutate(rn = row_number(rn)) %>% 
        dplyr::filter(! rn %in% obj$samples$test_index) %>%
        dplyr::select(-rn)
    }
    
    obj$models[[uid]] <- train_model(mobj = obj$models[[uid]],
                                     data = train_data,
                                     measure = obj$measure,
                                     samples = obj$samples,
                                     save_submodels = obj$save_submodels,
                                     execution_strategy = obj$execution_strategy,
                                     level = obj$conf_levels,
                                     seed = obj$seed)
    obj$models[[uid]]$status <- "trained"
    obj$models[[uid]]$last_updated <- Sys.time()
    obj$performance <- dplyr::bind_rows(
      obj$performance,
      obj$models[[uid]]$performance %>% 
        dplyr::mutate(model_uid = uid,
                      pipeline_uid = obj$models[[uid]]$pipe,
                      method = obj$models[[uid]]$method) %>% 
        dplyr::select(model_uid, pipeline_uid, submodel_uid, sample, method, param_grid, !!obj$measure$method)
    )
  }
  
  obj
}



#' @rdname set_final_model
#' @export
set_final_model.modeler <- function(obj,
                                    method,
                                    uid = NULL,
                                    reevaluate = TRUE,
                                    refit = TRUE) {
  checkmate::assert_choice(method, c("manual", "best"))
  checkmate::assert_character(uid, null.ok = TRUE)
  checkmate::assert_flag(reevaluate)
  checkmate::assert_flag(refit)
  
  if (!is.null(uid))
    checkmate::assert_choice(uid, names(obj$models))
  if (method == "manual" & is.null(uid))
    stop("final model not selected: uid not provided for manual method")
  
  if (method == "best") {
    uid <- get_best_model(obj)$uid
  }
  
  if (reevaluate) {
    if (is.null(obj$samples$test_holdout_prct)) {
      warning("Missing Test Holdout Sample. Final Model not re-evaluated.")
    } else{
      
      # Evaluate Model
      obj$models[[uid]] <- obj$pipelines[[obj$models[[uid]]$pipe]]$output %>%
        dplyr::mutate(rn = 1) %>% 
        dplyr::mutate(rn = row_number(rn)) %>% 
        dplyr::filter(rn %in% obj$samples$test_index) %>%
        dplyr::select(-rn) %>% 
        evaluate_model(mobj = obj$models[[uid]],
                       data = .,
                       measure = obj$measure)
    }
  }
  
  if (refit) {
    if (is.null(obj$samples$test_holdout_prct)) {
      warning("Missing Test Holdout Sample. Final Model does not need to be refit.")
      obj$final_model <- obj$models[[uid]]
    } else{
      # Retrain Data
      refit_sample <- add_default_samples(obj$data)
      obj$final_model <- train_model(
        mobj = obj$models[[uid]],
        data = obj$pipelines[[obj$models[[uid]]$pipe]]$output,
        measure = obj$measure,
        samples = refit_sample,
        save_submodels = FALSE,
        execution_strategy = obj$execution_strategy,
        level = obj$conf_levels,
        seed = obj$seed
      )
    }
  } else{
    obj$final_model <- obj$models[[uid]]
  }
  
  obj$final_model$status <- "final"
  obj$final_model$last_updated <- Sys.time()
  
  obj
}


#' @rdname refit
#' @export
refit.modeler <- function(obj, df, append = TRUE, ...) {
  checkmate::assert_class(obj, "modeler")
  
  if(! any(class(df) %in% class(obj$data))) { 
    stop(c("new dataframe class not consistent with expected data class\n",
          "expecting class: ", paste(class(obj$data), collapse=", "), "\n", 
          "dataframe class: ", paste(class(df), collapse=", "), "\n"  ))
  }
  
  if(is.null(obj$final_model)) {
    stop("final model not set")
  }
  
  # Schema Check
  schema_compare <- a2munge::get_schema(df) %>% 
    a2munge::schema_check(obj$schema)
  
  # Append Option
  pipe_uid <- obj$final_model$pipe
  if(append) {
    bind_fun <- ifelse("tbl_spark" %in% class(df), 
                       get("sdf_bind_rows", asNamespace("sparklyr")),
                       get("bind_rows", asNamespace("dplyr")))
    obj$data <- bind_fun(obj$data, df)
    obj$pipelines[[pipe_uid]] <- execute(obj$data, obj$pipelines[[pipe_uid]])
    data <- obj$pipelines[[pipe_uid]]$output
  } else {
    data <- flow(df, obj$pipelines[[pipe_uid]])
  }
  
  # Samples
  refit_sample <- add_default_samples(data)
  
  obj$final_model <- train_model(
    mobj = obj$final_model,
    data = data,
    measure = obj$measure,
    samples = refit_sample,
    save_submodels = FALSE,
    execution_strategy = obj$execution_strategy,
    level = obj$conf_levels,
    seed = obj$seed
  )
  
  obj
}


#' @export
print.modeler <- function(obj, ...) {
  cat("---------------------------- \n")
  cat(obj$name, obj$type, "\n")
  cat("---------------------------- \n\n")
  cat("uid:        ", obj$uid, "\n")
  cat("version:    ", obj$version, "\n")
  cat("created on: ", as.character(obj$created_on), "\n")
  cat("created by: ", obj$scientist, "\n")
  cat("description:", obj$desc, "\n\n")
  cat("data ----------------------------", "\n\n")
  cat("target:", obj$target, "\n")
  cat("sample records:", "\n")
  print(head(obj$data))
  cat("\nsampling ----------------------------", "\n\n")
  cat("validation:", obj$samples$validation_method, "\n")
  cat("test holdout prct:",
      ifelse(is.null(obj$samples$test_holdout_prct), 0, obj$samples$test_holdout_prct),
      "\n")
  cat("\nmeasure ----------------------------", "\n\n")
  cat("measure name:", obj$measure$name, "\n")
  cat("\nmodels ----------------------------", "\n\n")
  print(get_models_status(obj))
  cat("\nperformance ----------------------------", "\n\n")
  get_performance(obj)
  cat("\nfinal model ----------------------------", "\n")
  cat("method:", obj$final_model$method, "\n")
  cat("method args:", unlist(obj$final_model$method_args), "\n")
  cat("desc:", obj$final_model$desc, "\n")
}


#' @export
summary.modeler <- print.modeler


#' @rdname get_target
#' @export
get_target.modeler <- function(obj) {
  obj$data %>%
    dplyr::select_at(c(obj$index_var, obj$target))
}


#' @rdname get_performance
#' @export
get_performance <- function(obj){
  if(nrow(obj$performance) == 0) {
    print("no models trained yet")
  }else{
    print(obj$performance %>% 
            dplyr::select(model_uid, submodel_uid, method, !!obj$measure$method))
  }
}



# Helper Functions --------------------------------------------------------
