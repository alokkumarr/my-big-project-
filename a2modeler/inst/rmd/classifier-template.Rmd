---
title: "Classifier Template"
author: "Change Me"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    theme: united
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, error = TRUE, 
                      warning = FALSE,  message = FALSE,
                      fig.align = "center", fig.width = 10, fig.height = 5)
```

## Version Notes


```{r input-data-params, eval = FALSE}

## Input Data Parameters for reader function

# Path to dataset
dataset_path <- "CHANGEME"

# Name to assign to Data (no hyphens)
dataset_name <- "df"

# Dataset type
dataset_type <- "CHANGEME"

```


```{r inputs, eval = FALSE}

## Classifier Configuration Inputs 

# target variable name
target <- "changeme"

# Optional Classifier name
name <- NULL

# Optional Classifier Unique ID
uid <- sparklyr::random_string("classifier")

# Optional Classifier Version 
version <- NULL

# Optional Description of Classifier
desc <- NULL

# Optional Input for Data Scientist Author
scientist <- NULL

# Execution Strategy (sequential or multisession)
execution_strategy <- "sequential"

# Option to refit model on entire training and validation sample
refit <- TRUE

# Option to save sub-models used in tuning
save_submodels <- TRUE

# Optional random seed number
seed <- 319

```


```{r deploy-options, eval = FALSE}

# Deploy Model Option
deploy_model <- FALSE

# Model Deploy Path
deploy_path <- "changeme"
```



```{r r-config, eval = FALSE}

## R Configuration Parameters

# Library Path
lib_path <- .libPaths()[1]

# R packages
packages <- c("a2munge", "a2modeler", "a2charter", "sparklyr", "dplyr", "purrr", "ggplot2", "rlang", "tidyr", "DT", "knitr", "gridExtra")


```


```{r spark-config, eval = FALSE}

## Spark Configuration Parameters

# Create Spark Connection
spark <- TRUE

# Spark Master
spark_master <- "local"

# Spark Version
spark_version <- "2.3.0"

# Additional Spark Configuration Settings. Add configuration with named element
# in list
spark_config_args <- list(spark.memory.fraction = 0.9)

```


```{r r-setup, eval = FALSE}

# Load Libraries
for(pck in packages) library(pck, lib.loc = lib_path, character.only = TRUE)

```


```{r spark-setup, eval = FALSE}

# If Spark Option True, create connection
if(spark) {
  
  conf <- modifyList(spark_config(), spark_config_args)
  sc <- spark_connect(master = spark_master,
                      version = spark_version,
                      config = conf)
}
```


```{r data-import, eval = FALSE}

# Read data into memory
if(spark) {
  df <- reader(sc, name = dataset_name, path = dataset_path, type = dataset_type)
} else {
  df <- read.csv(dataset_path)
}

```


```{r classifier-creation, eval = FALSE}

# Create Classifier
c1 <- classifier(df,
                 target,
                 name = name,
                 uid = uid,
                 version = version,
                 desc = desc,
                 scientist = scientist,
                 execution_strategy = execution_strategy,
                 refit = refit,
                 save_submodels = save_submodels,
                 seed = seed)
```


```{r measure, eval = FALSE}

# Set Measure. AUC is default
c1 <- set_measure(c1, AUC)

```


```{r sampling, eval = FALSE}

# Set Sampling Method. Options include cross_validation, holdout, random or
# default
c1 <- add_holdout_samples(c1, splits = c(.6, .2, .2))

```


```{r feature-generation, eval = FALSE}

# Create one or more feature generation pipelines. Each can be applied in one or
# more models
feature_pipe1 <- pipeline(expr = identity,
                          uid  = "Feature-Pipe-1",
                          desc = "")

```


```{r models, eval = FALSE}

# Add Classification Models
c1 <- c1 %>% 
  add_model(., 
            method = "ml_logistic_regression",
            pipe = feature_pipe1,
             param_map = list(),
            uid = "logistic-baseline",
            desc = "Simple logistic model to create baseline prediction accuracy")
```


```{r train-models, eval = FALSE}

# Train models
c1 <- train_models(c1)

```


```{r set-final, eval = FALSE}

# Set Final Model 
c1 <- set_final_model(c1, "best", reevaluate = TRUE, refit = TRUE)
```


```{r model-deploy, eval = FALSE}

# Deploy Model
if(deploy_model) {
  a2modeler::deploy(c1, path = deploy_path)
}
```


```{r execution, ref.label=c('r-config', 'r-setup', 'spark-config', 'spark-setup', 'input-data-params', 'data-import', 'deploy-options', 'inputs', 'classifier-creation', 'measure', 'sampling', 'feature-generation', 'models', 'train-models', 'set-final', 'model-deploy')}

```



```{r classifier-print, echo=FALSE, results='asis'}
cat("##", c1$name, "Classifier", "\n")

```

* uid: __`r c1$uid` __ 
* Version: __`r c1$version`__  
* Created on: __`r as.character(c1$created_on)`__ 
* Created by: __`r c1$scientist`__
* Description: _`r c1$desc`_  


```{r data-print, echo=FALSE, results='asis'}
cat("###", "Input Data:", dataset_name,"\n")

```

* Location: __`r dataset_path`__
* Type: __`r dataset_type`__


#### Sample Records


```{r sample-records, echo=FALSE}

# Datatable with sample records
c1$data %>% 
  collecter(sample = TRUE, method = "head", size = 100) %>% 
  datatable(.,
            options = list(scrollX = TRUE),
            caption = paste(dataset_name, "Sample Records"))
```


```{r target-print, echo=FALSE, results='asis'}
cat("###", "Target:", c1$target,"\n")

```


```{r target-summary, echo=FALSE}
df %>%
  count(!! sym(c1$target)) %>% 
  mutate(percent = n/sum(n)) %>% 
  collect() %>%
  kable(., 
        digits = 3,
        caption = paste(paste0(c1$name, ":"), c1$target, "Summary"))
```


### Pipelines


```{r pipeline-print, echo=FALSE}
for(p in seq_along(c1$pipelines)) {
  print(c1$pipelines[[p]])
}
```


### Models


```{r models-print, echo=FALSE, results='asis'}

for(i in seq_along(c1$models)) {
  cat("####", names(c1$models)[i])
  cat("\n")
  cat("* Method:", paste0("__", c1$models[[i]]$method, "__"), "\n")
  cat("* Package:", paste0("__", c1$models[[i]]$package, "__"), "\n")
  cat("* Pipeline:", paste0("__", c1$models[[i]]$pipe, "__"), "\n")
  cat("* Status:", paste0("__", c1$models[[i]]$status, "__"), "\n")
  cat("* Last Updated:", paste0("__", c1$models[[i]]$last_updated, "__"), "\n")
  cat("\n")
  
  cat("##### Parameter Tuning:\n")
  for(p in seq_along(c1$models[[i]]$param_map)) {
    cat("*",
        paste0(names(c1$models[[i]]$param_map)[p], ":"),
        paste(c1$models[[i]]$param_map[[p]], collapse = ", "),
        "\n")
  }
  cat("\n")
}

```


### Leaderboard


```{r leaderboard}
c1$performance %>% 
  select(model_uid, pipeline_uid, method, param_grid, sample, !!c1$measure$method) %>% 
  arrange_at(c1$measure$method) %>% 
  kable(.,
        digits = 3,
        caption = paste(c1$name, "Model Leaderboard"))
```


### Final Model


```{r final-model, results='asis'}

cat("* uid:", paste0("__", c1$final_model$uid, "__"), "\n")
cat("* Method:", paste0("__", c1$final_model$method, "__"), "\n")
cat("* Package:", paste0("__", c1$final_model$package, "__"), "\n")
cat("* Status:", paste0("__", c1$final_model$status, "__"), "\n")
cat("* Last Updated:", paste0("__", c1$final_model$last_updated, "__"), "\n")
cat("\n")

```


```{r final-model-params, echo=FALSE}

c1$final_model$fit$stages[[2]]$param_map %>%
  as.data.frame() %>%
  gather(key = "parameter") %>% 
  filter(! grepl("_col|seed", parameter)) %>% 
  kable(.,
        table.attr = "style='width:30%;'",
        caption = "Final Model Parameters")
```


```{r test-fit-print, results='asis'}

if(! is.null(c1$samples$test_holdout_prct)) {
  cat("#### Test Holdout Fit\n")
  cat("Final Model Performance on Test Holdout:\n\n")
  cat("*", c1$measure$method, "=", round(c1$final_model$test_performance[[1]], 3), "\n")
}
```


```{r test-confusion-matrix, echo=FALSE}

if(! is.null(c1$samples$test_holdout_prct)) {
  
  # Test Predictions with Actuals
  tpdf <- c1$final_model$test_predictions %>% 
    collecter(., sample = FALSE) 
  
  pred_var <- colnames(tpdf)[1] 
  
  # count table
  t1 <- tpdf %>% 
    select(!!sym(pred_var), !!sym(c1$target)) %>% 
    table(.)
 
  cat("Confusion Matrix Counts:\n")
  print(addmargins(t1, c(1, 2)))
  cat("\n")
  
  # prop table
  cat("Confusion Matrix Rates:\n")
  print(round(addmargins(prop.table(t1), c(1,2)), 3))
  cat("\n\n")
  
  # Confusion Matrix Counts
  cm <- tpdf %>% 
    count(!!sym(pred_var), !!sym(c1$target)) %>%
    arrange(!!sym(pred_var), !!sym(c1$target)) %>%
    mutate(label = case_when(
      !!sym(pred_var) == 0 & !!sym(c1$target) == 0 ~ "TN",
      !!sym(pred_var) == 0 & !!sym(c1$target) == 1 ~ "FN",
      !!sym(pred_var) == 1 & !!sym(c1$target) == 0 ~ "FP",
      !!sym(pred_var) == 1 & !!sym(c1$target) == 1 ~ "TP")) %>% 
    select(-!!sym(pred_var), -!!sym(c1$target)) %>% 
    expander(., fun = funs(label = c("TN", "FN", "FP", "TP"))) %>% 
    replace_na(list(n = 0)) %>% 
    spread(key = "label", value = "n", drop=T, fill=0)
  
  tibble(metric = "Sensitivity", value = cm$TP/(cm$TP + cm$FN)) %>% 
    add_row(metric = "Specificity", value = cm$TN/(cm$TN + cm$FP)) %>% 
    add_row(metric = "Positive Predictive Value", value = cm$TP/(cm$TP + cm$FP)) %>% 
    add_row(metric = "Negative Predictive Value", value = cm$TN/(cm$TN + cm$FN)) %>% 
    add_row(metric = "F1 Score", value = 2*cm$TP / (2*cm$TP + cm$FP + cm$FN)) %>% 
    add_row(metric = "Accuracy", value = (cm$TP + cm$TN) / sum(cm)) %>% 
    add_row(metric = "Threshold", value = c1$final_model$fit$stages[[2]]$threshold) %>% 
    add_row(metric = "No Info Rate", value = max(cm$FP+cm$TP, cm$FN+cm$TN) / sum(cm)) %>% 
    kable(.,
          digits = 3,
          caption = "Confusion Matrix Metrics")
}
```



```{r test-roc, echo=FALSE}

if(! is.null(c1$samples$test_holdout_prct)) {
  
  pos_prob_var <- "prob_1"

  roc <- tpdf %>%
   arrange(-!!sym(pos_prob_var)) %>%
   mutate(n = row_number(),
          tp = cumsum(!!sym(c1$target)),
          fp = n - tp,
          fn = sum(!!sym(c1$target)) - tp,
          tn = n() - n - fn,
          tpr = tp/(tp + fn),
          fpr = 1 - tn/(tn + fp),
          total = tp + fp + fn + tn) 
  
  roc %>%
    gg_line_chart("fpr", "tpr", 
                  title = "ROC Curve",
                  subtitle = paste("Model:", c1$final_model$uid),
                  x_axis_title = "False Positive Rate (1 - Specificity)",
                  y_axis_title = "True Positive Rate (Sensitivity)") +
    geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1),
                 linetype = 5, color = "grey50", size = 1.05) 
}
```


```{r test-gains, echo=FALSE}

if(! is.null(c1$samples$test_holdout_prct)) {
 
  # Calculate Gain & Lifts
  bin_n <- 10
  gains <- roc %>% 
    mutate(bin = ntile(n, bin_n)) %>% 
    group_by(bin) %>% 
    summarise(n  = n(),
              tp = sum(!!sym(c1$target))) %>% 
    ungroup() %>% 
    mutate(decile = cumsum(n) / sum(n),
           cumtp  = cumsum(tp), 
           gain   = cumtp / sum(tp),
           lift   = gain / decile)
  
  p1 <- gg_line_chart(gains, 
                      x_variable = "decile",
                      y_variable = "gain",
                      points = TRUE,
                      point_args = list(size=3, shape=1),
                      title = "Gains Chart",
                      subtitle = paste("Model:", c1$final_model$uid),
                      x_axis_title = "% of Test Holdout Sample",
                      y_axis_title = "Gain (% of Positive Class)",
                      caption = "Sample ordered by positive class probability") +
    geom_ribbon(aes(ymin = decile, ymax = gain), fill = sncr_cols(1), alpha=.2) +
    geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1),
                 linetype = 5, color = "grey50", size = 1.05) +
    scale_x_continuous(limits = c(0, 1), breaks = round(gains$decile, 2)) +
    scale_y_continuous(limits = c(0, 1))
  
  p2 <- gg_line_chart(gains, 
                      x_variable = "decile",
                      y_variable = "lift",
                      color = sncr_cols(9),
                      points = TRUE,
                      point_args = list(size=3, shape=1, color=sncr_cols(9)),
                      title = "Lift Chart",
                      subtitle = paste("Model:", c1$final_model$uid),
                      x_axis_title = "% of Test Holdout Sample",
                      y_axis_title = "Lift (% of Target Identified over Random)",
                      caption = "Sample ordered by positive class probability") +
    geom_ribbon(aes(ymin = 1, ymax = lift), fill = sncr_cols(9), alpha=.2) +
    scale_x_continuous(limits = c(0, 1), breaks = round(gains$decile, 2)) 
  
  gridExtra::grid.arrange(p1, p2, ncol = 2)
  
  gains %>% 
    rename(!!quo_name(paste('Cumulated', c1$target)) := cumtp,
           !!quo_name(c1$target) := tp) %>%
    select(bin, decile, n,
           !!sym(c1$target), !!sym(paste('Cumulated', c1$target)),
           gain, lift) %>% 
    kable(., 
          digits = 3,
          caption = "Gains Table")
}

```


```{r test-deploy, results='asis'}

if(deploy_model) {
  cat("### Deployment\n\n")
  cat("*", c1$name, "saved here:", paste0("_", deploy_path, "_"))
  cat("\n")
}
```




## Configuration

```{r classifier-config, echo=TRUE, eval=FALSE, ref.label=c('inputs', 'classifier-creation', 'measure', 'sampling', 'feature-generation', 'models', 'train-models', 'set-final')}

```


## Appendix

```{r config, echo=TRUE, eval=FALSE, ref.label=c('r-config', 'spark-config')}

```

