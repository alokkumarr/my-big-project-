# Below block is to support both connectors like http & https

{% if sip_secure %}

connector = https

sip.ssl = {
  enable = true
  trust.store="{{ store_path }}/{{ ansible_host }}/truststore.jks"
  trust.password="{{ sip_keystore_password }}"
  alias="sip"
  key.store="{{ store_path }}/{{ ansible_host }}/keystore.jks"
  key.password="{{ sip_keystore_password }}"
}

{% else %}

sip.ssl = {
  enable = false
}

connector = http

{% endif %}

semantic = {
  # hardcoded as http to proceed with testing for now. will be removed with before merging.
  host = "http://localhost:9500"
  endpoint = "/internal/semantic/workbench/"
}

report.executor = {
    path= "{{ sip_var_path | default('/var/sip') }}/services/transport/executor"
  }

{% if saw_execution_history is defined %}
execution.history = "{{ saw_execution_history }}"
{% endif %}

spark = {
  master = "{{ saw_spark_master_url | default('spark://localhost:7077') }}"
{% if saw_spark_driver_port is defined %}
  driver.port = "{{ saw_spark_driver_port }}"
{% endif %}
{% if saw_spark_driver_host is defined %}
  driver.host = "{{ saw_spark_driver_host }}"
{% endif %}
{% if saw_spark_executor_instances is defined %}
  executor.instances = "{{ saw_spark_executor_instances }}"
{% endif %}
{% if saw_spark_driver_bind_address is defined %}
  driver.bindAddress = "{{ saw_spark_driver_bind_address }}"
{% endif %}
{% if saw_spark_driver_blockmanager_port is defined %}
  driver.blockManager.port = "{{ saw_spark_driver_blockmanager_port }}"
{% endif %}
  executor.memory.regular = "{{ saw_executor_memory_regular | default('8G') }}"
  executor.memory.fast = "{{ saw_executor_memory_fast | default('2G') }}"
  cores.max.regular = "{{ saw_executor_cores_regular | default('8') }}"
  cores.max.fast = "{{ saw_executor_cores_fast | default('2') }}"
  {% if saw_spark_executor_instances_regular is defined %}
  executor.instances.regular = "{{ saw_spark_executor_instances_regular }}"
  {% endif %}
  {% if saw_spark_executor_instances_fast is defined %}
  executor.instances.fast = "{{ saw_spark_executor_instances_fast }}"
  {% endif %}
  driver.memory = "{{ sip_executor_driver_memory | default('2G') }}"

  {% if sip_executors_reserved_port is defined %}
  spark.driver.port.fast = "{{ sip_executors_reserved_port | int + play_hosts.index(inventory_hostname) }}"
  spark.driver.port.regular = "{{ sip_executors_reserved_port |int + play_hosts.index(inventory_hostname) | int + ansible_play_hosts | length *1 }}"
  spark.blockManager.port.fast = "{{ sip_executors_reserved_port |int + play_hosts.index(inventory_hostname) | int + ansible_play_hosts | length *2 }}"
  spark.blockManager.port.regular= "{{ sip_executors_reserved_port |int + play_hosts.index(inventory_hostname) | int + ansible_play_hosts | length *3 }}"
  {% endif %}
  yarn = {
    {% if saw_spark_yarn_queue_regular is defined %}
    queue.regular = "{{ saw_spark_yarn_queue_regular }}"
    {% endif %}
    {% if saw_spark_yarn_queue_fast is defined %}
    queue.fast = "{{ saw_spark_yarn_queue_fast }}"
    {% endif %}
    {% if saw_spark_yarn_jars is defined %}
    spark.jars = "{{ saw_spark_yarn_jars }}"
    {% endif %}
    {% if saw_spark_yarn_zips is defined %}
    spark.zips = "{{ saw_spark_yarn_zips }}"
    {% endif %}
    {% if saw_spark_yarn_resource_manager is defined %}
    resourcemanager.hostname = "{{ saw_spark_yarn_resource_manager }}"
    {% endif %}
}
  sql-executor = {
    wait-time = "{{ saw_executor_result_wait_time | default('60') }}"
    inline-data-store-limit-bytes = 268435456
    preview-rows-limit = "{{ saw_preview_rows_limit | default('10000') }}"
    publish-rows-limit = "{{ saw_publish_rows_limit | default('-1') }}"
    executor-restart-threshold = "{{ saw_executor_restart_threshold | default('100') }}"

    # The location is to store results genereted by SQL Executor
    output-location = "{{ sip_var_path | default('/var/sip') }}/services/transport/executor/output"
    semantic-layer-tmp-location = "{{ sip_var_path | default('/var/sip') }}/services/transport/tmp"

    # Use line-delimited JSON as the output format by default, for
    # simplicity of streaming results out of the data lake
    output-type = "json"

    jar-location = "/opt/saw/service/sparklib"
  }
}
