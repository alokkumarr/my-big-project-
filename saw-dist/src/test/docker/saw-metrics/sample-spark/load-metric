#!/bin/sh
#
# Load sample Spark metric (for report)
#
set -eu

dir=$(dirname $0)

# Load common metrics functions
. $(dirname $dir)/lib/common.sh

datalake=$datalake_home/sample-spark

load_semantic() {
    echo "Loading sample Spark semantic metadata"
    local semantic=semantic.json
    local semantic_data=semantic-data.json
    $sudo_mapr hadoop fs -mkdir -p $datalake
    # Load the data object
    local semantic_data_output=$datalake/$semantic_data-output
    $sudo_mapr $hadoop_put $dir/$semantic_data $datalake
    $sudo_mapr $mdcli -i $datalake/$semantic_data -o $semantic_data_output
    # Insert the data object ID into the semantic node
    set -o pipefail
    local id=$(hadoop fs -cat $semantic_data_output | grep SAMPLE \
             | sed -e 's/.*".*":"\(.*\)".*/\1/')
    sed -i -e "s/@DATA_OBJECT_ID@/$id/" $dir/$semantic
    # Load the semantic node
    local semantic_output=$datalake/$semantic-output
    $sudo_mapr $hadoop_put $dir/$semantic $datalake
    $sudo_mapr $mdcli -i $datalake/$semantic -o $semantic_output
}

load_data() {
    echo "Loading sample Spark data"
    $sudo_mapr $hadoop_put $dir/data.ndjson $datalake
}

wait_maprfs
load_semantic
load_data
