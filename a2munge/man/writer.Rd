% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/writer.R
\name{writer}
\alias{writer}
\alias{writer.tbl_spark}
\alias{writer.data.frame}
\title{DataFrame Writer Function}
\usage{
writer(df, ...)

\method{writer}{tbl_spark}(df, path, name, type, mode = NULL,
  partitions = NULL, partition_by = NULL, dfs_mount = "/dfs",
  temp_folder_name = "tmp", ...)

\method{writer}{data.frame}(df, path, name, type, mode = NULL, ...)
}
\arguments{
\item{df}{DataFrame}

\item{...}{optional arguments passed to write function}

\item{path}{file directory path to write files to. Should include protocol if
distributed path. Should not end with forward slash.}

\item{name}{Output file name. Note for partitioned files, the file part number is
appended to this file name}

\item{type}{Output file type. Specifies the writer function used}

\item{mode}{Specifies the behavior when data or table already exists.
Supported values include: 'error', 'append', 'replace' and 'ignore'. Notice
that 'replace' will also change the column structure. default is NULL}

\item{partitions}{Number of file partitions}

\item{partition_by}{Partitions the output by the given columns on the file
system}

\item{dfs_mount}{dfs mount file path. applicable for distributed files only.
default is /dfs}

\item{temp_folder_name}{name of temp folder}
}
\description{
This is a function to write dataframes to files. Supports writing csv, text,
json and parquet file types.
}
\details{
The default values for Spark method with delimited type are the consistent
with spark_write_csv function, comma. For files with a different delimiter,
include the dilimiter argument, i.e. dilimeter="|" for pipe. Parquet files use
default spark_read_parquet funtionality. Supports overwrite, append, and error
modes.

Function writes files to temporary folder and then copies a renamed version to
destination directory
}
