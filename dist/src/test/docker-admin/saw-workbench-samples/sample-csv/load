#!/bin/sh
#
# Load sample CSV file (for Workbench)
#
set -eu

dir=$(dirname $0)

sudo_mapr="sudo -u mapr"
hadoop_put="hadoop fs -put -f"
hadoop_mkdir="hadoop fs -mkdir -p"
datalake_raw=/var/sip/raw
spark_app_dir=/apps/spark

wait_maprfs() {
    # The MapR container might still be starting up, so wait until MapR-FS
    # has been verified being available
    echo "Waiting for MapR-FS to become available"
    while ! hadoop fs -ls / > /dev/null 2>&1; do
        sleep 5
        echo "Retrying accessing MapR-FS"
    done
}

init_sparkapp () {
    $sudo_mapr $hadoop_mkdir $spark_app_dir
}

init_datalake() {
    $sudo_mapr $hadoop_mkdir $datalake_raw
}

load_data() {

    echo "Loading sample CSV data 1"
    $sudo_mapr $hadoop_put $dir/test.csv $datalake_raw

    echo "Loading sample CSV data 2"
    $sudo_mapr $hadoop_put $dir/RevenueVehicleMaintPerf2.csv $datalake_raw

}

create_project_dirs() {
    echo "Create prohject directory srtructure"
    $sudo_mapr $hadoop_mkdir /main/workbench/dl/fs/data

}


wait_maprfs
init_datalake
init_sparkapp
load_data
create_project_dirs
