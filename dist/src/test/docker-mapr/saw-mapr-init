#!/bin/bash
#
# Initialize MapR container
#
set -eux

yarn_setup() {
    echo "YARN setup in docker"
#
# Install YARN
#
# Note: A local Spark context is used to conserve memory in
# development mode.  Uncomment below to install YARN for testing.
#RUN yum -y -q install mapr-resourcemanager mapr-nodemanager
yum -y -q install mapr-resourcemanager mapr-nodemanager
yum -y -q --nogpgcheck install mapr-spark-master mapr-spark
ln -s /opt/mapr/spark/spark-* /opt/mapr/spark/spark-current

# Workaround: Replace default container hostname with localhost
sed -i -e 's,72ba20be3774,localhost,' \
   /opt/mapr/spark/spark-current/conf/spark-defaults.conf \
   /opt/mapr/spark/spark-current/conf/spark-env.sh
echo localhost > /opt/mapr/spark/spark-current/conf/slaves

# Workaround: The default configuration binds to localhost by default,
# so remove those lines to bind to all interfaces and allow access
# from other containers
sed -i -e '/^export SPARK_MASTER_[A-Z]*=localhost$/d' \
    -e 's/export SPARK_WORKER_MEMORY=16g/export SPARK_WORKER_MEMORY=1g/' \
    /opt/mapr/spark/spark-current/conf/spark-env.sh
/opt/mapr/spark/spark-current/sbin/start-slaves.sh
}

# Start rsyslog to persist system logs
/etc/init.d/rsyslog start

# Start Spark slaves
#
# Note: A local Spark context is used to conserve memory in
# development mode.  If a Spark standalone cluster is enabled,
# uncomment below to start Spark workers.
#/opt/mapr/spark/spark-current/sbin/start-slaves.sh

# Wait until MapR-FS is available
set +x
while ! hadoop fs -ls / > /dev/null 2>&1; do
    sleep 5
    echo "Retrying accessing MapR-FS"
done
set -x

# Create directory for Spark shell and Apache Livy.  Run at boot time
# when MapR-FS available.
sudo -u mapr hadoop fs -mkdir -p /apps/spark

if [ "$1" == "true" ]; then
yarn_setup
fi
# Workaround: Using "/usr/sbin/lsof -i :7222" or similar as
# healthcheck indicates success too early, as the MapR image
# /usr/bin/init-script restarts services once, so hook into the init
# script instead
touch /saw-mapr-init-done

