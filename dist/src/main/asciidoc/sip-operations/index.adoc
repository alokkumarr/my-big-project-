= SIP Operations Guide
Version {project-version}
:toc:
:nofooter:
:docinfo: shared
:plantuml-config: plantuml-config

== Introduction

This document describes how to install, configure and troubleshoot SIP
and its modules in an environment.

== Installing and configuring

=== Prerequisites

Before starting an installation of SIP ensure the following has been
provided in the environment:

. Install and configure a MapR version 6.0 cluster in the environment

. Install and configure Apache Spark version 2.2 from the MapR
  Ecosystem Pack on the MapR cluster

. Install and configure Apache Livy from the MapR Ecosystem Pack on
  the MapR cluster, and set `livy.file.upload.max.size = 500000000`
  and `livy.spark.master` in `livy.conf`

. Provision a MapR user in the MapR cluster (with user ID 500)

. Optionally create a MapR volume to hold the SIP data (at the
  location specified by the `sip_var_path` variable in the SIP
  environment configuration, which defaults to `/var/sip`).  Creating
  a MapR volume enables making snapshots of SIP data, for backup and
  restore purposes.  If a dedicated volume is not created, the SIP
  data can be saved as part of a root volume snapshot.

. Provision a mail relay host

. Provision a host to deploy from (the deploy host), running CentOS
  7.4 as the operating system.  This host will be used to run the
  deploy command and store the environment configuration.  The deploy
  host is typically common to the entire environment.

. Provision hosts for running SIP (the target host), with 32 GB of
  memory and CentOS 7.4 as the operating system.  The hosts should be
  dedicated to SIP and not run other applications.  The hosts should
  have time synchronization enabled.

. Install and configure MapR client on the SAW target host.

. Configure SAW server to use UTC timezone in all places where date and timezones are involved.

Please note that the target hosts must be allocated exclusively for
SAW use.  The underlying infrastructure up to the operating system
level are expected to be managed externally, while SAW fully manages
packages and configuration on the target hosts.

Currently SAW only supports deploying to a single target host.

=== SAW

==== Configuring

Each SAW environment requires its own SAW environment configuration
file.  A sample configuration is provided inside the SAW release
package in the `saw-config` file, which defaults to installing SAW on
`localhost`.  At a minimum, copy this sample file and change the
`localhost` entries to the SAW target host.

The sample configuration defaults to installing all available modules:
SAW Web, SAW Services and SAW Security.  To install only a subset of
these modules, edit the environment configuration to only mention the
desired modules.

NOTE: The environment configuration file must be preserved between
installations and upgrades.  It is recommended to put it under version
control that is stored on the deploy host and backed up to another
location.

==== Installing

Execute the following steps to install SAW:

. Get the SAW release package (named `saw-*.tgz`)

. Prepare a SAW environment configuration file, as described in the
  previous section

. Extract the package and execute the deploy command, giving it the
  path to the environment configuration file as an argument

        tar -xzf saw-*.tgz
        cd saw
        ./saw-deploy <config>

TIP: Configure passwordless SSH access to the SAW target host for a
smoother installation experience.  The deploy command should be run as
a normal user.  The deploy command will use sudo to request privileges
for relevant operations.

==== Upgrading

To upgrade an existing installation, follow the same steps as for
installing an entirely new environment.  The deploy command will
detect an already existing installation and upgrade it.

NOTE: Downgrading to an older SAW version is not supported.  If
needed, take a backup of the entire environment before upgrading that
can be used to restore the environment to a state that is supported by
older SAW versions.

==== Upgrading to SAW 2.5

Before upgrading, install and configure Apache Livy from the MapR
Ecosystem Pack as described in the updated <<Prerequisites>>.  Also
add the `saw_workbench_livy_uri` parameter to the SAW environment
configuration (`saw-config`).

==== Upgrading to SAW 2.6

Before upgrading, If SAW require to run saw-executor in YARN mode
configure as described in the updated <<Running SAW on YARN>>. Update
to the below SAW environment configuration (`saw-config`).

    saw_spark_master_url=yarn
    saw_spark_yarn_queue_regular=saw-regular
    saw_spark_yarn_queue_fast=saw-fast
    saw_spark_yarn_jars=/opt/mapr/spark/spark-2.2.1/jars
    saw_spark_yarn_zips=/opt/saw/service/spark.zip
    # Zip file will be automatically get created if not exists in mention location.
    # saw_spark_yarn_resource_manager is optional parameter if we want to explicitly
    # define the resource manager for saw-executor if not configured in yarn-site.xml.
    saw_spark_yarn_resource_manager=sip-mapr

Additional parameter added to control large file export from FTP/email dispatch.
In case of any higher memory/CPU load on (saw-transport service/export service) server,
this parameter can be set lower value.

     saw_export_chunk_size=10000


==== Running SAW on YARN.

Support for running SAW-executors on YARN (Hadoop NextGen) was added to SAW
in version v2.6.0.
To configure and run the saw-executor in yarn mode default configuration properties
are provided in `saw-config` copy those properties change as per enviroment.

To configuring additional properties for spark and yarn,set configuration as java
option since SparkConf loads defaults from system properties(start with spark.*)
and the classpath.'

  Ex: export _JAVA_OPTIONS='-Dspark.executor.instances=5'

==== Upgrading to SIP 3.0

Please note that the SIP data stored by previous versions in MapR-FS
is consolidated under a single directory as part of upgrading to SIP
3.0 (at the location specified by the `sip_var_path` variable in the
SIP environment configuration, which defaults to `/var/sip`).  After
this the SIP data can be saved using a MapR volume snapshot, for
backup and restore purposes.  If no dedicated volume has been
configured for SIP data, it can still be saved as part of a root
volume snapshot which does not require any additional setup steps.

==== Interfaces

The SAW Web module and supporting services are exposed on port 80 of
the SAW target host, i.e. `http://<saw-target-host>/`.  The SAW Web
application will automatically discover the endpoints for SAW Security
and SAW Services based on the URL it is being served from.  Nothing
else in the SAW deployment, except for port 80 on the SAW target host,
is accessed by external parties.

Large header settings: Include the below properties in NGINX server
config file to support, HTTP requests with large headers (more than
8K).

       client_body_buffer_size 32k;
       client_header_buffer_size 16k;
       large_client_header_buffers 8 64k;

File upload limit settings: Include the below properties in NGINX server
config file to support larger files upload (more than 1MB)

       client_max_body_size 25m;

=== XDF-NG

==== Installing

Execute the following steps to install XDF-NG:

. Get the XDF release package (named `bda-xdf-nextgen-*.rpm`)

. Create and prepare a xdf-ng vars configuration file in /etc/bda/xdf-nextgen.vars
  location, as described below

      dl.root:   hdfs:///data/bda
      http.port: 15020
      seeds:     2784

.  execute the RPM package installation command as below to install the XDF-NG package

        rpm -ivh bda-xdf-nextgen-*.rpm

=== RTPS

==== Installing

RTPS is installed as part of SIP.  Ensure the SIP environment
configuration (`saw-config`) has an entry for RTPS:

        [sip-rtps]
        host

RTPS will get installed at the location `/opt/bda/sip-rtps` on the
given host.

==== Upgrading

To upgrade an existing installation, follow the same steps as for
installing an entirely new environment.  The deploy command will
detect an already existing installation and upgrade it.

NOTE: Downgrading to an older RTPS version is not supported.  If
needed, take a backup of the entire environment before upgrading that
can be used to restore the environment to a state that is supported by
older RTPS versions.

==== Configuring

RTPS must be configured to keep memory consumption within the limits
provided by the Spark configuration.  The memory consumption of RTPS
depends on the number of records in each batch and the size of each
record.  The number of records included into each batch can be
configured in the RTPS configuration file `appl.conf` as shown below:

        spark {
          streaming.kafka.maxRatePerPartition = 1000
        }

The `maxRatePerPartition` parameter sets an upper limit on the number
of records fetched per second per MapR stream partition.  If the limit
is lowered, the upper bound of RTPS memory consumption is also
decreased.

NOTE: If the RTPS has been down for an extended period of time, a
large backlog of messages might have been built up.  To prevent the
RTPS from running into memory errors while processing a large backlog
of messages, ensure it has been properly configured to limit memory
consumption.

=== RTIS

==== Installing

Execute the following steps to install RTIS:

. Get the RTIS release package (named `bda-rtis-*.rpm`)

. Create and prepare a RTIS vars configuration file in  /etc/bda/cluster.vars
  location, as described below

     app.key             =  sip-rtis
     streams_1.topic     =  data
     streams_1.queue     =  bda/data/streams/sip-rtis
     streams_2.topic     =  data
     streams_2.queue     =  bda/data/streams/sip-rtis
     app.class           =  synchronoss.handlers.charter.smartcare.CharterEventHandler
     key.serializer      =  org.apache.kafka.common.serialization.StringSerializer
     value.serializer    =  org.apache.kafka.common.serialization.StringSerializer

.  Execute the RPM package installation command as below to install the RTIS package

        rpm -ivh bda-rtis-*.rpm

    RTIS will get installed in location `/opt/bda/rtis/`

.  Run /opt/bda/rtis-*/sbin/rtis_runner.sh & to start RTIS.

==== Upgrading

To upgrade an existing installation, follow the same steps as for
installing an entirely new environment.  The deploy command will
detect an already existing installation and upgrade it.

NOTE: Downgrading to an older RTIS version is not supported.  If
needed, take a backup of the entire environment before upgrading that
can be used to restore the environment to a state that is supported by
older RTIS versions.

== Monitoring

To support monitoring of a SIP environment, services expose health
checks.

=== Health checks

SIP consists of services that are each performing some function,
either by responding to incoming requests or executing something on a
schedule.  The services are typically expected to be functioning
normally, meaning they are in a healthy state.  However, in certain
situations the services might not be able to perform the functions
expected from them, in which case they are deemed to be in an
unhealthy state.  Services expose health checks that allow external
actors to verify the health state of a service.  This can be used in
operations to detect issues early and pinpoint the source of a
problem.  The health checks are additionally internally used for high
availability, to route requests to services that are in a healthy
state.

Health checks of services are accessed through a REST API.  Each
service's endpoint exposes a `/actuator/health` resource, which
indicates the health status of the service.  If the response is `HTTP
200 OK` and the contents is a JSON object with the property `status`
set to `UP`, the service is healthy.  Any other HTTP response code or
status value indicates the service is not healthy.

The following shows a response indicating the Security Service is in a
healthy state:

        $ curl https://<sip-proxy>/saw/security/actuator/health
        HTTP/1.1 200 
        Content-Type: application/json; charset=UTF-8
        <...>
        {
            "status": "UP"
        }

== Maintenance and troubleshooting

=== Accessing application logs

The SAW systemd services system logs can be accessed using the `sudo
journalctl` command.  To view the logs of individual services, use the
`-u` option:

        $ sudo journalctl -u saw-\*

=== Listing services and their statuses

To list services and check the status of all SAW systemd units,
execute the following commands:

        $ sudo systemctl list-units saw-\*

NOTE: Some services use
http://0pointer.de/blog/projects/socket-activation.html[socket
activation] to reduce memory usage and shorten deploy times.  These
services will be listed as not running (inactive dead) until the first
connection is made over the network.  This is normal for
socket-activated services and does not indicate a problem.

=== Starting, stopping and restarting services

Under normal circumstances there should be no need to start, stop or
restart SAW services manually.  However, if needed it can be done
using the following commands:

        $ sudo systemctl start <saw-service>
        $ sudo systemctl stop <saw-service>
        $ sudo systemctl restart <saw-service>

Where `<saw-service>` is one of the SAW systemd services (for example
`saw-gateway`), which can be listed using the `sudo systemctl
list-units saw-\*` command shown in the previous section.

=== Location of SIP data in MapR-FS

SIP data is stored under a single specific directory in MapR-FS.  The
location of this directory is configured using the `sip_var_path`
variable in the SIP environment configuration (the `saw-config` file)
and its default value is `/var/sip`.

By knowing where the SIP data is located, it is possible to configure
MapR volumes that can be used to snapshot SIP data, for backup and
restore purposes.  Even if no dedicated MapR volume has been
configured for SIP, the data can still be snapshotted as part of the
root volume.  Please refer to MapR documentation for instructions on
creating volume snapshots.

NOTE: SIP data (both plain files and MapR-DB tables) can be moved
between MapR-FS locations using the standard filesystem tools, as long
as it is within the same volume.  Moving SIP data across MapR volumes
requires using the MapR-DB CopyTable tool.  Please refer to MapR
documentation for instructions on using that.

=== High availability and restoring failed nodes

SIP provides high availability, so that if a single node in the
environment fails or becomes unavailable services will keep working
normally.

After a node failure, the SIP environment should be restored to its
original node count and layout as soon as possible to ensure high
availability.  Do this using the following steps:

. Provision a new node with the same specifications as the failed node

. If the hostname of the new node is different from the hostname of
  the failed node, update references to the old hostname in the SIP
  environment configuration (`saw-config`) to refer to the new
  hostname

. Rerun the SIP deployment to restore software and configuration on
the new node

=== Clearing the Transport Service Executor queues

If the SAW report execution queue has filled up, for example due to
many long-running queries being executed, the queues can be cleared
using the following commands:

        $ ssh <mapr-host>
        $ stream=<report-executor-path>/saw-transport-executor-regular-stream
        $ sudo -u mapr maprcli stream topic delete -path $stream -topic executions
        $ stream=<report-executor-path>/saw-transport-executor-fast-stream
        $ sudo -u mapr maprcli stream topic delete -path $stream -topic executions

* <report-executor-path> can be found in saw-transport service configuration file.

Please note that clearing the queues affects all users of the system
and report execution types.

=== Clearing the onetime execution result for data lake

Saw execution result for data lake analysis can be cleaned-up if output location
contains large amount of onetime(preview) execution results.

 $ ssh <mapr-host>
 $ hadoop fs -rm -r <output-location>/preview-*

Note: <output-location> can be found in saw-host, saw-transport service conf file
in location /opt/saw/service/conf/application.conf properties name
`output-location`. Please do not delete anything which doesn't contains
preview in directory name.

=== Automatically generated credentials

Automatically generated credentials, such as for internal service and
administrator accounts, can be found in the `/etc/bda` directory on
the respective host.

== Loading semantic metadata

To enable creating analyses in SAW, load semantic metadata as follows:

        $ ssh <saw-services-host>
        $ sudo -u mapr /opt/saw/service/bin/mdcli.sh -i \
            file://<nodes-json> -o file:///tmp/log.json

The semantic metadata JSON is stored in the `<nodes-json>` file.

=== Semantic metadata format

Semantic metadata supports the following values for the `type`
property:

* `integer`
* `long`
* `float`
* `double`
* `string`
* `date`

NOTE: Paths to files in the data lake must not contain spaces.

== Onboarding customer

We can utilise customer_onboard.sh script in order to execute the command with current environment setup.

    cd /opt/bda/saw-security/bin/
    bash customer_onboard.sh

Features of spring boot shell:

. Type in "help" and it will show you all the available commands

. Tab based auto completion is supported.


    shell:>help
    AVAILABLE COMMANDS
    Built-In Commands
            clear: Clear the shell screen.
            exit, quit: Exit the shell.
            help: Display help about available commands.
            script: Read and execute commands from a file.
            stacktrace: Display the full stacktrace of the last error.
    Saw Security Shell
            onboard-customer: Onboard the customer
    shell:>


Once you are inside the shell, type in onboard-customer and it will start the process of creating customer and related products/components in the system.

In below example, it starts with showing you which products are present in system and asks for basic customer information.


    shell:>onboard-customer
    Customer information:
    1
    {PRODUCT_ID=1, PRODUCT_NAME=MCT Insights}
    {PRODUCT_ID=2, PRODUCT_NAME=SnT Insighjts}
    {PRODUCT_ID=3, PRODUCT_NAME=Smart Care Insights}
    {PRODUCT_ID=4, PRODUCT_NAME=SAW Demo}
    {PRODUCT_ID=5, PRODUCT_NAME=Channel Insights}
    ====== CUSTOMERS INFORMATION ======
    Enter CUSTOMER_CODE: (UNIQUE CODE TO IDENTIFY your company / division) temp
    Enter COMPANY NAME: temp
    Enter COMPANY BUSINESS: temp
    Enter PRODUCT ID from above for default landing page: 4
    Enter DOMAIN_NAME: abc.com
    Generated CUSTOMER_SYS_ID: 2
    2018-01-03 10:09:43.676  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Created user with ID: 2


In this case the generated customer_sys_id is 16. It continues to show product information as we need to associate these products with customers, in my case I chose 4 which is for saw demo.

    {PRODUCT_ID=1, PRODUCT_NAME=MCT Insights}
    {PRODUCT_ID=2, PRODUCT_NAME=SnT Insighjts}
    {PRODUCT_ID=3, PRODUCT_NAME=Smart Care Insights}
    {PRODUCT_ID=4, PRODUCT_NAME=SAW Demo}
    {PRODUCT_ID=5, PRODUCT_NAME=Channel Insights}
    ====== CUSTOMER_PRODUCTS TABLE ======
    Enter PRODUCT_SYS_ID: 4
    class org.springframework.jdbc.support.GeneratedKeyHolder
    2
    Generated CUST_PROD_SYS_ID: 2
    2018-01-03 12:42:32.522  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Created CUST_PROD entry with ID: 2

In this example the generated customer product linkage ID is 11. It continues with displaying modules of all products, sicne we chose saw demo i.e. 4 in previous case. It makes sense to select modules of that product only. i.e. in this case either 4, 7 or 8.

    {MODULE_ID=1, PRODUCT_NAME=MCT Insights, MODULE_NAME=OBSERVE}
    {MODULE_ID=2, PRODUCT_NAME=SnT Insighjts, MODULE_NAME=OBSERVE}
    {MODULE_ID=3, PRODUCT_NAME=Smart Care Insights, MODULE_NAME=OBSERVE}
    {MODULE_ID=4, PRODUCT_NAME=SAW Demo, MODULE_NAME=ANALYZE}
    {MODULE_ID=5, PRODUCT_NAME=Channel Insights, MODULE_NAME=OBSERVE}
    {MODULE_ID=6, PRODUCT_NAME=MCT Insights, MODULE_NAME=ANALYZE}
    {MODULE_ID=7, PRODUCT_NAME=SAW Demo, MODULE_NAME=ALERT}
    {MODULE_ID=8, PRODUCT_NAME=SAW Demo, MODULE_NAME=OBSERVE}
    ====== CUSTOMER PRODUCT MODULES ======
    Enter MODULE_ID (from above shown values):
    4
    Enter more? (yes/no): yes
    Enter MODULE_ID (from above shown values):
    7
    Enter more? (yes/no): yes
    Enter MODULE_ID (from above shown values):
    8
    Enter more? (yes/no): no

It continues with displaying that it's creating the relationships and admin role in background followed by creating admin user for the customer.


    ====== ASSOCIATING DEFAULT FEATURES ======
    ====== CREATING ADMIN ROLE ======
    2018-01-03 12:42:50.059  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Created Admin Role for above customer with ID: 5
    ====== USERS TABLE for ADMIN USER ======
    Enter MASTER_LOGIN:
    temp@abc.com
     Enter EMAIL: temp@abc.com
    Enter PASSWORD: pleasechangepassword
    Enter FIRST_NAME:
    temp
    Enter MIDDLE_NAME:
    temp_mn
    Enter LAST_NAME:
    temp_ln
    Generated User ID for current user is: 5
    2018-01-03 12:43:28.084  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Created Admin user with ID: 5
    ====== CREATING PRIVILEGES FOR ADMIN ======
    2018-01-03 12:43:28.110  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Generated Privilege ID for Admin user: 43
    shell:>
    shell:>


==  SAW SSO Authentication

SAW supports external systems to authenticate users (single sign-on).The shared secret key is read from the SAW environment configuration, as a base64 encoded string (while ensuring Synchronoss Global Information Security standards for storing secret keys are adhered to).
Recommended key size is 256 bits.

   Command to generate key : openssl rand 32 -base64
   Dgus5PoaEHm2tKEjy0cUGnzQlx86qiutmBZjPbI4y0U=

After generating the key, add it to the SAW environment configuration ({{saw-config}}) in the {{saw_security_sso_secret}} parameter and redeploy.

== Export Configuration

=== email and ftp extract size configuration

SAW supports *exporting* reports and pivots:

. from UI
. to email
. to ftp/sftp servers

In saw-config, we can configure how many number of rows we want to extract for all the
reports / pivots:

.saw-config
[source, yaml]
----
saw_ui_export_size=10000
saw_email_export_size=50000
saw_ftp_export_size=1000000
----

Saw reports are exported chunks of rows, we can configure how many rows to take
at a time for processign reports, can be configured using following config parameter:


.saw-config
[source, yaml]
----
saw_export_chunk_size=10000
----


=== FTP server configuration

SAW supports exporting of pivots and reports to ftp/sftp servers.
By default an empty configuration is installed in
`/opt/bda/saw-export-service/conf/ftp-details.json` file on saw nodes.

The contents of this configuration can be changed using `saw-config`.
An example configuration has been included in config file.

.saw-config
----
# FTP JSON config
# ##########################################
#
#
# DO NOT SPLIT THIS INTO MULTIPLE LINES
#
#
# ##########################################
# ftp_json_config='{"ftpList":[{"customerName":"CUSTUNIQUEID","alias":"ftpsrv1","host":"srv1","port":21,"username":"usr1","password":"pwd1","location":"/path/to/dir/","type":"ftp"}]}'
----

Example contents (in pretty format):

.ftp-details.json
[source, json]
----
{
  "ftpList": [
    {
        "customerName":"UNIQUE_IDENTIFIER1",
        "alias" : "server1",
        "host": "server1.customer1.com",
        "port": 21,
        "username": "usr1",
        "password": "pwd1",
        "location": "/some/location/",
        "type": "ftp"
    },
    {
        "customerName":"UNIQUE_IDENTIFIER1",
        "alias" : "server2",
        "host": "server2.customer1.com",
        "port": 22,
        "username": "usr2",
        "password": "pwd2",
        "location": "/some/location/",
        "type": "sftp"
    },
    {
        "customerName":"UNIQUE_IDENTIFIER2",
        "alias" : "server1",
        "host": "server1.customer2.com",
        "port": 21,
        "username": "imuser1",
        "password": "pwd3",
        "location": "/home/ubuntu",
        "type": "ftp"
    }
  ]
}
----

In above example, `customerName` is the unique identifier given at
the time of onboarding customer. Note that based on this unique
identifier, customers are differentiated. Each FTP/SFTP
server is required to have unique entry which gets presented to front
end, this is maintained by means of `alias` entry. *Note* that each
server entry per customer is required to have a unique alias entry
which gets presented in front end.

NOTE: Please make sure to put minified JSON in configuration file.
