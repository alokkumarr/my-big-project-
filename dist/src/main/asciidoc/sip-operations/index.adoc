= SIP Operations Guide
include::header.adoc[]

== Introduction

This document describes how to install, configure and troubleshoot SIP
and its modules in an environment.

== Installing and configuring

=== Prerequisites

Before starting an installation of SIP ensure the following has been
provided in the environment:

. Install and configure a MapR version 6.0 cluster in the environment

. Install and configure Apache Spark version 2.2 from the MapR
  Ecosystem Pack on the MapR cluster

. Install and configure Apache Livy from the MapR Ecosystem Pack on
  the MapR cluster, and set `livy.file.upload.max.size = 500000000`
  and `livy.spark.master` in `livy.conf`

. Provision a MapR user in the MapR cluster (with user ID 500 and
  group ID 5000)

. Optionally create a MapR volume to hold the SIP data (at the
  location specified by the `sip_var_path` variable in the SIP
  environment configuration, which defaults to `/var/sip`).  Creating
  a MapR volume enables making snapshots of SIP data, for backup and
  restore purposes.  If a dedicated volume is not created, the SIP
  data can be saved as part of a root volume snapshot.

. Provision a mail relay host

. Provision a host to deploy from (the deploy host), running CentOS
  7.4 as the operating system.  This host will be used to run the
  deploy command and store the environment configuration.  The deploy
  host is typically common to the entire environment.

. Provision hosts for running SIP (the target host), with 32 GB of
  memory and CentOS 7.4 as the operating system.  The hosts should be
  dedicated to SIP and not run other applications.  The hosts should
  have time synchronization enabled.

. Install and configure MapR client on the SAW target host.

. Configure SAW server to use UTC timezone in all places where date and timezones are involved.

. On all saw nodes, volumes i.e. MapRFS using NFS gateway service
  has to be mounted. This is required by ingestion service.

Please note that the target hosts must be allocated exclusively for
SAW use.  The underlying infrastructure up to the operating system
level are expected to be managed externally, while SAW fully manages
packages and configuration on the target hosts.

Currently SAW only supports deploying to a single target host.

=== SAW

==== Configuring

Each SAW environment requires its own SAW environment configuration
file.  A sample configuration is provided inside the SAW release
package in the `saw-config` file, which defaults to installing SAW on
`localhost`.  At a minimum, copy this sample file and change the
`localhost` entries to the SAW target host.

The sample configuration defaults to installing all available modules:
SAW Web, SAW Services and SAW Security.  To install only a subset of
these modules, edit the environment configuration to only mention the
desired modules.

NOTE: The environment configuration file must be preserved between
installations and upgrades.  It is recommended to put it under version
control that is stored on the deploy host and backed up to another
location.

==== Installing

Execute the following steps to install SAW:

. Get the SAW release package (named `saw-*.tgz`)

. Prepare a SAW environment configuration file, as described in the
  previous section

. Extract the package and execute the deploy command, giving it the
  path to the environment configuration file as an argument

        tar -xzf saw-*.tgz
        cd saw
        ./saw-deploy <config>

TIP: Configure passwordless SSH access to the SAW target host for a
smoother installation experience.  The deploy command should be run as
a normal user.  The deploy command will use sudo to request privileges
for relevant operations.

==== Offline installation

To support offline installation SIP allows specifying alternative URLs
for used YUM repositories and RPM packages, for example local mirrors.
These can be configured in the SIP environment configuration
(`saw-config`) as follows:

        [saw-services:vars]
        sip_epel_rpm_url = https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
        [saw-security:vars]
        sip_mariadb_yum_url = http://yum.mariadb.org/10.3/centos7-amd64
        [saw-security-arbitrator:vars]
        sip_mariadb_yum_url = http://yum.mariadb.org/10.3/centos7-amd64

==== Upgrading

To upgrade an existing installation, follow the same steps as for
installing an entirely new environment.  The deploy command will
detect an already existing installation and upgrade it.

NOTE: Downgrading to an older SAW version is not supported.  If
needed, take a backup of the entire environment before upgrading that
can be used to restore the environment to a state that is supported by
older SAW versions.

==== Upgrading to SIP 3.1

The SIP environment configuration file must contain the roles
`sip-rtis` and `sip-rtps`, as shown below:

        [sip-rtis]
        [sip-rtps]

The environment configurations of previous SIP versions might not
contain these roles, so they have to be added to the configuration
file before deploying a newer SIP version.  The roles can however be
left empty (no hosts listed under them), if RTIS and RTPS are not to
be installed.

==== Upgrading to SAW 2.5

Before upgrading, install and configure Apache Livy from the MapR
Ecosystem Pack as described in the updated <<Prerequisites>>.  Also
add the `saw_workbench_livy_uri` parameter to the SAW environment
configuration (`saw-config`).

==== Upgrading to SAW 2.6

Before upgrading, If SAW require to run saw-executor in YARN mode
configure as described in the updated <<Running SAW on YARN>>. Update
to the below SAW environment configuration (`saw-config`).

    saw_spark_master_url=yarn
    saw_spark_yarn_queue_regular=saw-regular
    saw_spark_yarn_queue_fast=saw-fast
    saw_spark_yarn_jars=/opt/mapr/spark/spark-2.2.1/jars
    saw_spark_yarn_zips=/opt/saw/service/spark.zip
    # Zip file will be automatically get created if not exists in mention location.
    # saw_spark_yarn_resource_manager is optional parameter if we want to explicitly
    # define the resource manager for saw-executor if not configured in yarn-site.xml.
    saw_spark_yarn_resource_manager=sip-mapr

Additional parameter added to control large file export from FTP/email dispatch.
In case of any higher memory/CPU load on (saw-transport service/export service) server,
this parameter can be set lower value.

     saw_export_chunk_size=10000


==== Running SAW on YARN.

Support for running SAW-executors on YARN (Hadoop NextGen) was added to SAW
in version v2.6.0.
To configure and run the saw-executor in yarn mode default configuration properties
are provided in `saw-config` copy those properties change as per enviroment.

To configuring additional properties for spark and yarn,set configuration as java
option since SparkConf loads defaults from system properties(start with spark.*)
and the classpath.'

  Ex: export _JAVA_OPTIONS='-Dspark.executor.instances=5'

==== Upgrading to SIP 3.0

Please note that the SIP data stored by previous versions in MapR-FS
is consolidated under a single directory as part of upgrading to SIP
3.0 (at the location specified by the `sip_var_path` variable in the
SIP environment configuration, which defaults to `/var/sip`).  After
this the SIP data can be saved using a MapR volume snapshot, for
backup and restore purposes.  If no dedicated volume has been
configured for SIP data, it can still be saved as part of a root
volume snapshot which does not require any additional setup steps.

==== Interfaces

The SAW Web module and supporting services are exposed on port 80 of
the SAW target host, i.e. `http://<saw-target-host>/`.  The SAW Web
application will automatically discover the endpoints for SAW Security
and SAW Services based on the URL it is being served from.  Nothing
else in the SAW deployment, except for port 80 on the SAW target host,
is accessed by external parties.

Large header settings: Include the below properties in NGINX server
config file to support, HTTP requests with large headers (more than
8K).

       client_body_buffer_size 32k;
       client_header_buffer_size 16k;
       large_client_header_buffers 8 64k;

File upload limit settings: Include the below properties in NGINX server
config file to support larger files upload (more than 1MB)

       client_max_body_size 25m;

=== XDF-NG

==== Prerequisites
XDF-NG requires `jq` to be installed in the host cluster
(all nodes of the cluster, because jobs can be submitted on any node).
jq can be installed using the below command:

    yum install jq

Once the installtion is complete, use the below command to validate the installation:

    $ jq --version
    jq-1.5

==== Installing XDF-NG

Execute the following steps to install XDF-NG:

. Get the XDF release package (named `bda-xdf-nextgen-*.rpm`)

. Create and prepare a xdf-ng vars configuration file in /etc/bda/xdf-nextgen.vars
  location, as described below

      dl.root:   hdfs:///data/bda
      http.port: 15020
      seeds:     2784

.  execute the RPM package installation command as below to install the XDF-NG package

        rpm -ivh bda-xdf-nextgen-*.rpm

=== RTPS

==== Installing

RTPS is installed as part of SIP.  Ensure the SIP environment
configuration (`saw-config`) has an entry for RTPS:

        [sip-rtps]
        host

RTPS will get installed at the location `/opt/bda/sip-rtps` on the
given host.

==== Upgrading

To upgrade an existing installation, follow the same steps as for
installing an entirely new environment.  The deploy command will
detect an already existing installation and upgrade it.

NOTE: Downgrading to an older RTPS version is not supported.  If
needed, take a backup of the entire environment before upgrading that
can be used to restore the environment to a state that is supported by
older RTPS versions.

==== Configuring

RTPS must be configured to keep memory consumption within the limits
provided by the Spark configuration.  The memory consumption of RTPS
depends on the number of records in each batch and the size of each
record.  The number of records included into each batch can be
configured in the RTPS configuration file `appl.conf` as shown below:

        spark {
          streaming.kafka.maxRatePerPartition = 1000
        }

The `maxRatePerPartition` parameter sets an upper limit on the number
of records fetched per second per MapR stream partition.  If the limit
is lowered, the upper bound of RTPS memory consumption is also
decreased.

NOTE: If the RTPS has been down for an extended period of time, a
large backlog of messages might have been built up.  To prevent the
RTPS from running into memory errors while processing a large backlog
of messages, ensure it has been properly configured to limit memory
consumption.

=== RTIS

==== Installing

RTIS is installed as part of SIP.  Ensure the SIP environment
configuration (`saw-config`) has an entry for RTIS:

        [sip-rtis]
        host

RTIS will get installed at the location `/opt/bda/sip-rtis` on the
given host.

See the SIP environment configuration (`saw-config`) for how to set
RTIS configuration parameters.

==== Upgrading

To upgrade an existing installation, follow the same steps as for
installing an entirely new environment.  The deploy command will
detect an already existing installation and upgrade it.

NOTE: Downgrading to an older RTIS version is not supported.  If
needed, take a backup of the entire environment before upgrading that
can be used to restore the environment to a state that is supported by
older RTIS versions.

== Installing SIP product modules

Execute the following steps to install SIP product modules:

. Get the SIP product module package (`sip-module-*.tgz`)

. Copy the SIP product module package to the SIP deploy host, where
  the SIP package and SIP environment configuration (`saw-config`) is
  located

. Execute the product module deploy command, giving it the path to the
  SIP environment configuration file as the first argument and the SIP
  product module package as the second argument:

        cd saw
        ./sip-deploy-module <sip-config> <sip-module-package>

== Monitoring

To support monitoring of a SIP environment, services expose health
checks.

=== Health checks

SIP consists of services that are each performing some function, for
example by responding to incoming requests, consuming a queue or
executing something on a schedule.  The services are typically
expected to be functioning normally, meaning they are in a healthy
state.  However, in certain situations the services might not be able
to perform the functions expected from them, in which case they are
deemed to be in an unhealthy state.  Services expose health checks
that allow external actors to verify the health state of a service.
This can be used in operations to detect issues early and pinpoint the
source of a problem.  The health checks are additionally internally
used for high availability, to route requests to services that are in
a healthy state.

Health checks of services are accessed through a REST API.  Each
service's endpoint exposes a `/actuator/health` resource, which
indicates the health status of the service.  If the response is `HTTP
200 OK` and the contents is a JSON object with the property `status`
set to `UP`, the service is healthy.  Any other HTTP response code or
status value indicates the service is not healthy.

The following shows a response indicating the Security Service is in a
healthy state:

        $ curl https://<sip-proxy>/saw/security/actuator/health
        HTTP/1.1 200
        Content-Type: application/json; charset=UTF-8
        <...>
        {
            "status": "UP"
        }

== High availability

SIP supports high availability, so that if a single node in the
environment fails or becomes unavailable, services will keep working
normally.  This requires deploying the SIP environment in a highly
available configuration.

=== Highly available configuration

To ensure a highly available SIP environment, each role in the SIP
environment configuration (`saw-config`) must have at least two hosts.
Additionally the `saw-security-arbitrator` role should have a separate
third host (required for the MariaDB Galera Cluster).

The sample configuration provided in the SIP package contains the
hosts `sip-host1` and `sip-host2` in each role, plus `sip-host3` in
the arbitrator role, which makes it a highly available configuration.
The following is an example of a role that has been configured to be
highly available:

        [sip-role]
        sip-host1
        sip-host2

NOTE: The arbitrator role has low resource requirements and can
therefore be placed on a host whose primary purpose is something else,
as long as it is separate form the other hosts in the SIP
configuration.

=== Full high availability

SIP can be deployed with multiple hosts in the `saw-proxy` role, which
will ensures redundancy for every host in the SIP environment.
However, this requires clients of the SIP Services to be aware of each
host in the `saw-proxy` role.  Additionally, the SAW Web module
currently is only able to use a single endpoint, making it susceptible
to being unavailable of that single endpoint becomes unavailable.

Due to the above, SIP is currently recommended to be deployed with a
load balancer in front of the hosts in the `saw-proxy` role, that
exposes all of the hosts as a single endpoint reachable with a single
IP address.  This will allow clients and the SAW Web module to
continue working even if one of the hosts in the `saw-proxy` role
become unavailable.

=== Adding and removing hosts

After a host fails or becomes unavailable, the SIP environment should
be restored to its original host count and layout as soon as possible
to ensure high availability.  Do this using the following steps:

. Provision a new host with the same specifications as the failed host

. If the hostname of the new host is different from the hostname of
  the failed host, update references to the old hostname in the SIP
  environment configuration (`saw-config`) to refer to the new
  hostname

. Rerun the SIP deployment (`saw-deploy <config>`) to install software
  and configuration on the new host and update the configurations of
  other hosts to include the new host

=== Health checks for high availability

The high availability support for SIP is based on health checks of
services.  Requests are routed through the environment to services
based on the health check status.  If a service is unhealthy, it will
not receive requests.  Please see <<Health checks>> in the Monitoring
section for an overview about health checks in the SIP environment.
Also see <<Troubleshooting health checks>> in the Maintenance and
troubleshooting section for how to investigate failing health checks.

=== MariaDB high availability

The SIP environment includes a highly available MariaDB Galera
Cluster.  It is automatically managed by the SIP deployment and
configuration and requires no manual handling in normal operation.

In case all hosts in the SIP environment have been shut down and
brought up again, please see <<Bootstrapping the MariaDB Galera cluster>>
section for instructions on how to bootstrap the cluster again.

=== Service level

After a host fails or becomes unavailable, services of the SIP
environment should be restored within 60 seconds.  During a short time
period after a failure, services might be temporarily unavailable or
return errors, before they are fully restored again.

== Maintenance and troubleshooting

=== Accessing application logs

The SAW systemd services system logs can be accessed using the `sudo
journalctl` command.  To view the logs of individual services, use the
`-u` option:

        $ sudo journalctl -u saw-\*

=== Listing services and their statuses

To list services and check the status of all SAW systemd units,
execute the following commands:

        $ sudo systemctl list-units saw-\*

NOTE: Some services use
http://0pointer.de/blog/projects/socket-activation.html[socket
activation] to reduce memory usage and shorten deploy times.  These
services will be listed as not running (inactive dead) until the first
connection is made over the network.  This is normal for
socket-activated services and does not indicate a problem.

=== Starting, stopping and restarting services

Under normal circumstances there should be no need to start, stop or
restart SAW services manually.  However, if needed it can be done
using the following commands:

        $ sudo systemctl start <saw-service>
        $ sudo systemctl stop <saw-service>
        $ sudo systemctl restart <saw-service>

Where `<saw-service>` is one of the SAW systemd services (for example
`saw-gateway`), which can be listed using the `sudo systemctl
list-units saw-\*` command shown in the previous section.

=== Location of SIP data in MapR-FS

SIP data is stored under a single specific directory in MapR-FS.  The
location of this directory is configured using the `sip_var_path`
variable in the SIP environment configuration (the `saw-config` file)
and its default value is `/var/sip`.

By knowing where the SIP data is located, it is possible to configure
MapR volumes that can be used to snapshot SIP data, for backup and
restore purposes.  Even if no dedicated MapR volume has been
configured for SIP, the data can still be snapshotted as part of the
root volume.  Please refer to MapR documentation for instructions on
creating volume snapshots.

NOTE: SIP data (both plain files and MapR-DB tables) can be moved
between MapR-FS locations using the standard filesystem tools, as long
as it is within the same volume.  Moving SIP data across MapR volumes
requires using the MapR-DB CopyTable tool.  Please refer to MapR
documentation for instructions on using that.

=== Troubleshooting health checks

Health checks are initiated from HAProxy every 10 seconds.  The
HAProxy configuration can be inspected in the
`/etc/haproxy/haproxy.cfg` file.  Execute the following command to see
logs from HAProxy:

        $ sudo journalctl -u haproxy

HAProxy additionally provides a statistics page located at
`http://<sip-url>/haproxy_stats` showing the status of the
environment.

Each health check initiated from HAProxy causes a cascade of health
checks of upstream dependencies.  To find out what the upstream
services are, look at the configuration of the service.

TIP: For example the Gateway Service configuration is located in
`/opt/bda/saw-gateway-service/conf/application.yml` and contains the
URLs of its upstream services.

Once the URLs of the upstream services have been identified, append
`/actuator/health` to get the health check location, which can then be
used to manuall inspect it:

        $ curl http://<service-url>/actuator/health

The health check responses contain a `detail` propery with a
human-readable explanation of the health check status, which can be
used to further narrow down the cause for a possible failure.

=== Troubleshooting the MariaDB cluster

To troubleshoot MariaDB, inspect the MariaDB logs on each of the hosts
in the cluster:

        $ sudo journalctl -u mariadb

The logs will also contain messages related to the MariaDB Galera
cluster.

=== Bootstrapping the MariaDB Galera cluster

If all MariaDB server instances in the SIP environment are shut down,
the MariaDB Galera Cluster might not be able to start up itself
automatically.  In such cases bootstrap the cluster manually using the
following command on one of the hosts:

        $ sudo galera_new_cluster

The host to run that command on should be the host that has
`safe_to_bootstrap: 1` in its `/var/lib/mysql/grastate.dat` file.

:mariadb_doc: https://mariadb.com/kb/en/library
:galera_doc: {mariadb_doc}/getting-started-with-mariadb-galera-cluster

NOTE: For more information, see the
{galera_doc}#restarting-the-cluster[restarting the cluster] section in
the MariaDB Galera Cluster documentation.

=== Clearing the Transport Service Executor queues

If the SAW report execution queue has filled up, for example due to
many long-running queries being executed, the queues can be cleared
using the following commands:

        $ ssh <mapr-host>
        $ stream=<report-executor-path>/saw-transport-executor-regular-stream
        $ sudo -u mapr maprcli stream topic delete -path $stream -topic executions
        $ stream=<report-executor-path>/saw-transport-executor-fast-stream
        $ sudo -u mapr maprcli stream topic delete -path $stream -topic executions

* <report-executor-path> can be found in saw-transport service configuration file.

Please note that clearing the queues affects all users of the system
and report execution types.

=== Clearing the onetime execution result for data lake

Saw execution result for data lake analysis can be cleaned-up if output location
contains large amount of onetime(preview) execution results.

 $ ssh <mapr-host>
 $ hadoop fs -rm -r <output-location>/preview-*

Note: <output-location> can be found in saw-host, saw-transport service conf file
in location /opt/saw/service/conf/application.conf properties name
`output-location`. Please do not delete anything which doesn't contains
preview in directory name.

=== Automatically generated credentials

Automatically generated credentials, such as for internal service and
administrator accounts, can be found in the `/etc/bda` directory on
the respective host.

=== Batch ingestion: SFTP

The Batch Ingestion Service can connect to SFTP sources to ingest
files.  Ingesting a large number of files requires the Batch Ingestion
Service to be able to use a large number of sessions per SFTP
connection.  The default settings of OpenSSH only allows ten
concurrent sessions per SFTP connection.  To allow Batch Ingestion
Service to use the required number of session, add the below to the
`/etc/ssh/sshd_config` file:

    MaxSessions 5000

After that run `systemctl reload sshd` to reload the SSH daemon
configuration.

== Loading semantic metadata

To enable creating analyses in SAW, load semantic metadata as follows:

        $ ssh <saw-services-host>
        $ sudo -u mapr /opt/saw/service/bin/mdcli.sh -i \
            file://<nodes-json> -o file:///tmp/log.json

The semantic metadata JSON is stored in the `<nodes-json>` file.

=== Semantic metadata format

Semantic metadata supports the following values for the `type`
property:

* `integer`
* `long`
* `float`
* `double`
* `string`
* `date`

NOTE: Paths to files in the data lake must not contain spaces.

== Onboarding customer

We can utilise customer_onboard.sh script in order to execute the command with current environment setup.

    cd /opt/bda/saw-security/bin/
    bash customer_onboard.sh

Features of spring boot shell:

. Type in "help" and it will show you all the available commands

. Tab based auto completion is supported.


    shell:>help
    AVAILABLE COMMANDS
    Built-In Commands
            clear: Clear the shell screen.
            exit, quit: Exit the shell.
            help: Display help about available commands.
            script: Read and execute commands from a file.
            stacktrace: Display the full stacktrace of the last error.
    Saw Security Shell
            onboard-customer: Onboard the customer
    shell:>


Once you are inside the shell, type in onboard-customer and it will start the process of creating customer and related products/components in the system.

In below example, it starts with showing you which products are present in system and asks for basic customer information.


    shell:>onboard-customer
    Customer information:
    1
    {PRODUCT_ID=1, PRODUCT_NAME=MCT Insights}
    {PRODUCT_ID=2, PRODUCT_NAME=SnT Insighjts}
    {PRODUCT_ID=3, PRODUCT_NAME=Smart Care Insights}
    {PRODUCT_ID=4, PRODUCT_NAME=SAW Demo}
    {PRODUCT_ID=5, PRODUCT_NAME=Channel Insights}
    ====== CUSTOMERS INFORMATION ======
    Enter CUSTOMER_CODE: (UNIQUE CODE TO IDENTIFY your company / division) temp
    Enter COMPANY NAME: temp
    Enter COMPANY BUSINESS: temp
    Enter PRODUCT ID from above for default landing page: 4
    Enter DOMAIN_NAME: abc.com
    Generated CUSTOMER_SYS_ID: 2
    2018-01-03 10:09:43.676  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Created user with ID: 2


In this case the generated customer_sys_id is 16. It continues to show product information as we need to associate these products with customers, in my case I chose 4 which is for saw demo.

    {PRODUCT_ID=1, PRODUCT_NAME=MCT Insights}
    {PRODUCT_ID=2, PRODUCT_NAME=SnT Insighjts}
    {PRODUCT_ID=3, PRODUCT_NAME=Smart Care Insights}
    {PRODUCT_ID=4, PRODUCT_NAME=SAW Demo}
    {PRODUCT_ID=5, PRODUCT_NAME=Channel Insights}
    ====== CUSTOMER_PRODUCTS TABLE ======
    Enter PRODUCT_SYS_ID: 4
    class org.springframework.jdbc.support.GeneratedKeyHolder
    2
    Generated CUST_PROD_SYS_ID: 2
    2018-01-03 12:42:32.522  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Created CUST_PROD entry with ID: 2

In this example the generated customer product linkage ID is 11. It continues with displaying modules of all products, sicne we chose saw demo i.e. 4 in previous case. It makes sense to select modules of that product only. i.e. in this case either 4, 7 or 8.

    {MODULE_ID=1, PRODUCT_NAME=MCT Insights, MODULE_NAME=OBSERVE}
    {MODULE_ID=2, PRODUCT_NAME=SnT Insighjts, MODULE_NAME=OBSERVE}
    {MODULE_ID=3, PRODUCT_NAME=Smart Care Insights, MODULE_NAME=OBSERVE}
    {MODULE_ID=4, PRODUCT_NAME=SAW Demo, MODULE_NAME=ANALYZE}
    {MODULE_ID=5, PRODUCT_NAME=Channel Insights, MODULE_NAME=OBSERVE}
    {MODULE_ID=6, PRODUCT_NAME=MCT Insights, MODULE_NAME=ANALYZE}
    {MODULE_ID=7, PRODUCT_NAME=SAW Demo, MODULE_NAME=ALERT}
    {MODULE_ID=8, PRODUCT_NAME=SAW Demo, MODULE_NAME=OBSERVE}
    ====== CUSTOMER PRODUCT MODULES ======
    Enter MODULE_ID (from above shown values):
    4
    Enter more? (yes/no): yes
    Enter MODULE_ID (from above shown values):
    7
    Enter more? (yes/no): yes
    Enter MODULE_ID (from above shown values):
    8
    Enter more? (yes/no): no

It continues with displaying that it's creating the relationships and admin role in background followed by creating admin user for the customer.


    ====== ASSOCIATING DEFAULT FEATURES ======
    ====== CREATING ADMIN ROLE ======
    2018-01-03 12:42:50.059  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Created Admin Role for above customer with ID: 5
    ====== USERS TABLE for ADMIN USER ======
    Enter MASTER_LOGIN:
    temp@abc.com
     Enter EMAIL: temp@abc.com
    Enter PASSWORD: pleasechangepassword
    Enter FIRST_NAME:
    temp
    Enter MIDDLE_NAME:
    temp_mn
    Enter LAST_NAME:
    temp_ln
    Generated User ID for current user is: 5
    2018-01-03 12:43:28.084  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Created Admin user with ID: 5
    ====== CREATING PRIVILEGES FOR ADMIN ======
    2018-01-03 12:43:28.110  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Generated Privilege ID for Admin user: 43
    shell:>
    shell:>


==  SAW SSO Authentication

SAW supports external systems to authenticate users (single sign-on).The shared secret key is read from the SAW environment configuration, as a base64 encoded string (while ensuring Synchronoss Global Information Security standards for storing secret keys are adhered to).
Recommended key size is 256 bits.

   Command to generate key : openssl rand 32 -base64
   Dgus5PoaEHm2tKEjy0cUGnzQlx86qiutmBZjPbI4y0U=

After generating the key, add it to the SAW environment configuration ({{saw-config}}) in the {{saw_security_sso_secret}} parameter and redeploy.

== Export Configuration

=== email and ftp extract size configuration

SAW supports *exporting* reports and pivots:

. from UI
. to email
. to ftp/sftp servers

In saw-config, we can configure how many number of rows we want to extract for all the
reports / pivots:

.saw-config
[source, yaml]
----
saw_ui_export_size=10000
saw_email_export_size=50000
saw_ftp_export_size=1000000
----

Saw reports are exported chunks of rows, we can configure how many rows to take
at a time for processign reports, can be configured using following config parameter:


.saw-config
[source, yaml]
----
saw_export_chunk_size=10000
----


=== FTP server configuration

SAW supports exporting of pivots and reports to ftp/sftp servers.
By default an empty configuration is installed in
`/opt/bda/saw-export-service/conf/ftp-details.json` file on saw nodes.

The contents of this configuration can be changed using `saw-config`.
An example configuration has been included in config file.

.saw-config
----
# FTP JSON config
# ##########################################
#
#
# DO NOT SPLIT THIS INTO MULTIPLE LINES
#
#
# ##########################################
# ftp_json_config='{"ftpList":[{"customerName":"CUSTUNIQUEID","alias":"ftpsrv1","host":"srv1","port":21,"username":"usr1","password":"pwd1","location":"/path/to/dir/","type":"ftp"}]}'
----

Example contents (in pretty format):

.ftp-details.json
[source, json]
----
{
  "ftpList": [
    {
        "customerName":"UNIQUE_IDENTIFIER1",
        "alias" : "server1",
        "host": "server1.customer1.com",
        "port": 21,
        "username": "usr1",
        "password": "pwd1",
        "location": "/some/location/",
        "type": "ftp"
    },
    {
        "customerName":"UNIQUE_IDENTIFIER1",
        "alias" : "server2",
        "host": "server2.customer1.com",
        "port": 22,
        "username": "usr2",
        "password": "pwd2",
        "location": "/some/location/",
        "type": "sftp"
    },
    {
        "customerName":"UNIQUE_IDENTIFIER2",
        "alias" : "server1",
        "host": "server1.customer2.com",
        "port": 21,
        "username": "imuser1",
        "password": "pwd3",
        "location": "/home/ubuntu",
        "type": "ftp"
    }
  ]
}
----

In above example, `customerName` is the unique identifier given at
the time of onboarding customer. Note that based on this unique
identifier, customers are differentiated. Each FTP/SFTP
server is required to have unique entry which gets presented to front
end, this is maintained by means of `alias` entry. *Note* that each
server entry per customer is required to have a unique alias entry
which gets presented in front end.

NOTE: Please make sure to put minified JSON in configuration file.
