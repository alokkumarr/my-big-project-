= SIP Operations Guide
include::header.adoc[]

== Introduction

This document describes how to install, configure and troubleshoot SIP
and its modules in an environment.

== Installing and configuring

=== Prerequisites

Before starting an installation of SIP ensure the following has been
provided in the environment:

. Install and configure a MapR version 6.1.0 cluster in the environment

. Install and configure a MapR Ecosystem MEP version 6.3.0 in the environment

. Install and configure Apache Spark version 2.4.4 from the MapR
  Ecosystem Pack on the MapR cluster


. Provision a MapR user in the MapR cluster (with user ID 500 and
  group ID 5000)

. Optionally create a MapR volume to hold the SIP data (at the
  location specified by the `sip_var_path` variable in the SIP
  environment configuration, which defaults to `/var/sip`).  Creating
  a MapR volume enables making snapshots of SIP data, for backup and
  restore purposes.  If a dedicated volume is not created, the SIP
  data can be saved as part of a root volume snapshot.

. Provision a mail relay host

. Provision a host to deploy from (the deploy host), running CentOS
  7.4 as the operating system.  This host will be used to run the
  deploy command and store the environment configuration.  The deploy
  host is typically common to the entire environment.

. Provision hosts for running SIP (the target host), with 32 GB of
  memory and CentOS 7.7 as the operating system.  The hosts should be
  dedicated to SIP and not run other applications.  The hosts should
  have time synchronization enabled.

. Configure networking to block incoming connections to SIP nodes
  (from other than SIP nodes themselves) on all ports, except for port
  80 on the SIP proxy hosts

. Install and configure MapR client on the SIP target host.

. Configure SIP server to use UTC timezone in all places where date
and timezones are involved.

. On all saw nodes, volumes i.e. MapRFS using NFS gateway service
  has to be mounted. This is required by ingestion service.

Please note that the target hosts must be allocated exclusively for
SIP use.  The underlying infrastructure up to the operating system
level are expected to be managed externally, while SIP fully manages
packages and configuration on the target hosts.

Currently SIP only supports deploying to a single target host.

=== SIP

==== Configuring

Each SIP environment requires its own SIP environment configuration
file.  A sample configuration is provided inside the SIP release
package in the `sip-config` file, which defaults to installing SIP on
`localhost`.  At a minimum, copy this sample file and change the
`localhost` entries to the SIP target host.

The sample configuration defaults to installing all available modules:
SIP Web, SIP Services and SIP Security.  To install only a subset of
these modules, edit the environment configuration to only mention the
desired modules.

NOTE: The environment configuration file must be preserved between
installations and upgrades.  It is recommended to put it under version
control that is stored on the deploy host and backed up to another
location.

==== Installing

Execute the following steps to install SIP:

. Get the SIP release package (named `sip-*.tgz`)

. Prepare a SIP environment configuration file, as described in the
  previous section

. Extract the package and execute the deploy command, giving it the
  path to the environment configuration file as an argument

        tar -xzf sip-*.tgz
        cd saw
        ./sip-deploy <config>

TIP: Configure passwordless SSH access to the SIP target host for a
smoother installation experience.  The deploy command should be run as
a normal user.  The deploy command will use sudo to request privileges
for relevant operations.

==== Offline installation

To support offline installation SIP allows specifying alternative URLs
for used YUM repositories and RPM packages, for example local mirrors.
These can be configured in the SIP environment configuration
(`sip-config`) as follows:

        [saw-proxy:vars]
        sip_epel_yum_url = https://download.fedoraproject.org/pub/epel/$releasever/$basearch
        [saw-security:vars]
        sip_mariadb_yum_url = http://yum.mariadb.org/10.3/centos7-amd64
        [saw-security-arbitrator:vars]
        sip_mariadb_yum_url = http://yum.mariadb.org/10.3/centos7-amd64

TIP: If the EPEL repository is already provided on the system by other
means, installation of the EPEL repository file in `/etc/yum.repos.d`
by SIP can be skipped by adding `sip_epel_yum_provided = true` under
the `saw-proxy:vars` section in the SIP environment configuration
`sip-config`.

==== Upgrading

To upgrade an existing installation, follow the same steps as for
installing an entirely new environment.  The deploy command will
detect an already existing installation and upgrade it.

NOTE: Downgrading to an older SIP version is not supported.  If
needed, take a backup of the entire environment before upgrading that
can be used to restore the environment to a state that is supported by
older SIP versions.

==== Upgrading to SIP 2.5

Before upgrading, install and configure Apache Livy from the MapR
Ecosystem Pack as described in the updated <<Prerequisites>>.  Also
add the `saw_workbench_livy_uri` parameter to the SIP environment
configuration (`sip-config`).

==== Upgrading to SIP 2.6

Before upgrading, If SIP require to run saw-executor in YARN mode
configure as described in the updated <<Running SIP on YARN>>. Update
to the below SIP environment configuration (`sip-config`).

    saw_spark_master_url=yarn
    saw_spark_yarn_queue_regular=saw-regular
    saw_spark_yarn_queue_fast=saw-fast
    saw_spark_yarn_jars=/opt/mapr/spark/spark-2.2.1/jars
    saw_spark_yarn_zips=/opt/saw/service/spark.zip
    # Zip file will be automatically get created if not exists in mention location.
    # saw_spark_yarn_resource_manager is optional parameter if we want to explicitly
    # define the resource manager for saw-executor if not configured in yarn-site.xml.
    saw_spark_yarn_resource_manager=sip-mapr
    saw_workbench_lib_path=/opt/bda/saw-workbench-service/lib/

Additional parameter added to control large file export from FTP/email dispatch.
In case of any higher memory/CPU load on (saw-transport service/export service) server,
this parameter can be set lower value.

     saw_export_chunk_size=10000

==== Running SIP on YARN

Support for running SIP-executors on YARN (Hadoop NextGen) was added
to SIP in version v2.6.0.  To configure and run the saw-executor in
yarn mode default configuration properties are provided in
`sip-config` copy those properties change as per enviroment.

To configuring additional properties for spark and yarn,set
configuration as java option since SparkConf loads defaults from
system properties(start with spark.*) and the classpath.'

  Ex: export _JAVA_OPTIONS='-Dspark.executor.instances=5'

===== SIP executors reserved ports

SIP executors uses random ports for flexibility to increase the number
of sip executors in future if required, but in case any SIP enviroment
has limited port accessibility and other ports are blocked for
security reasons. SIP executors required to be allocated with reserved
series of port to SIP Executors to work properly.
sip_executors_reserved_port properties can be set in `sip-config` file
for starting port number and SIP will auto calculate the number of
required port based on number of SIP-Nodes, like Number of open series
ports required as (Number of SIP-node * 4 port).  For example: if we
have assigned 9801 is starting port and number of sip-node is 2 then
(2* 4) , so 9801 to 9808 port will be used by spark executors.

==== Upgrading to SIP 3.0

Please note that the SIP data stored by previous versions in MapR-FS
is consolidated under a single directory as part of upgrading to SIP
3.0 (at the location specified by the `sip_var_path` variable in the
SIP environment configuration, which defaults to `/var/sip`).  After
this the SIP data can be saved using a MapR volume snapshot, for
backup and restore purposes.  If no dedicated volume has been
configured for SIP data, it can still be saved as part of a root
volume snapshot which does not require any additional setup steps.

==== Upgrading to SIP 3.1

The SIP environment configuration file must contain the roles
`sip-rtis` and `sip-rtps`, as shown below:

        [sip-rtis]
        [sip-rtps]

The environment configurations of previous SIP versions might not
contain these roles, so they have to be added to the configuration
file before deploying a newer SIP version.  The roles can however be
left empty (no hosts listed under them), if RTIS and RTPS are not to
be installed.

==== Upgrading to SIP 3.3.0
SIP 3.3.0 needs mapr 6.1.0 and spark 2.3.2. Please look into the
<<Prerequisites, prerequisites>> section for more information.

There are possibilities where semantic definitions are created manually by
users and missed during semantic migration, this breaks the relationship
of semantic ID's for existing reports, as the reports would store the info semantic id
as part of its definition. To overcome this, run the following shell
script, which updates the semantic id info of each reports with matching
semantic node present in semanticDataStore.

Run the below command manually in sip-node as mapr user

 /opt/bda/sip-metadata-service semanticIdMigration <metadata base path>

For Example:

       bash semanticIdMigration maprfs:///var/sip

NOTE: This script has not enabled for auto run with SIP installation
due special scenario where existing semantic node was deleted and recreated
manually instead of using previous SIP version auto migration.

****
Some content here
****



==== Interfaces

The SIP Web module and supporting services are exposed on port 80 of
the SIP target host, i.e. `http://<sip-target-host>/`.  The SIP Web
application will automatically discover the endpoints for SIP Security
and SIP Services based on the URL it is being served from.  Nothing
else in the SIP deployment, except for port 80 on the SIP target host,
is accessed by external parties.

Large header settings: Include the below properties in NGINX server
config file to support, HTTP requests with large headers (more than
8K).

       client_body_buffer_size 32k;
       client_header_buffer_size 16k;
       large_client_header_buffers 8 64k;

File upload limit settings: Include the below properties in NGINX server
config file to support larger files upload (more than 1MB)

       client_max_body_size 25m;

=== XDF-NG

==== Prerequisites

XDF-NG requires `jq` to be installed in the host cluster
(all nodes of the cluster, because jobs can be submitted on any node).
jq can be installed using the below command:

    yum install jq

Once the installtion is complete, use the below command to validate
the installation:

    $ jq --version
    jq-1.5

==== Installing XDF-NG

Execute the following steps to install XDF-NG:

. Get the XDF release package (named `bda-xdf-nextgen-*.rpm`)

. Create and prepare a xdf-ng vars configuration file in
  /etc/bda/xdf-nextgen.vars location, as described below

      dl.root:   hdfs:///data/bda
      http.port: 15020
      seeds:     2784

. Execute the RPM package installation command as below to install the
  XDF-NG package

        rpm -ivh bda-xdf-nextgen-*.rpm
        
==== Configuring XDF-NG

XDF Pipeline executes sequence of components like parser, transformer,sql 
..etc against the data set. 

XDF-NG batch pipeline can be configured as below.

	{
		"pipeline": [
			{
				"component": "parser",
				"configuration": "file:///dfs/opt/bda/apps/test/
				    test_parse_temperature.template.jconf",
				"persist": "true"
			},
			{
				"component": "transformer",
				"configuration": "file:///dfs/opt/bda/apps/test/
				    test_transformer_temperature.template.jconf",
				"persist": "true"
			},
			{
				"component": "sql",
				"configuration": "file:///dfs/opt/bda/apps/test/
				    test_sql_temperature.template.jconf",
				"persist": "true"
			},
			{
				"component": "esloader",
				"configuration": "file:///dfs/opt/bda/apps/test/
				       test_esloader_temperature.template.jconf",
				"persist": "true"
			}
	
		]
	}
	
==== Configuring XDF-RTPS

  XDF-RTA can be run as a systemctl service as it continuously listens
  to messages. Follow below steps to create systemctl service

  1. Create a systemctl service by referring to 
  'sip-xdf/xdf/bin/templates/xdf-rta.service.template'.
  
  2. Add execute-component command with arguments in the service.
  
  3. Use systemctl commands to start/stop/restart or check status of rta service.

  RTPS gets installed as part of XDF-NG. If RTPS is configured as one of the
  component in pipeline, then RTA service gets started when pipeline is
  triggered. RTA service starts listening to real time data and executes
  pipline components with the real time data whenever data is pushed to
  RTA via RTIS

		{
			"pipeline": [{
				"component": "rtps",
				"configuration": "file:///dfs/opt/bda/apps/test/
				    test_RTPS.jconf.template",
				    
			},
			{
				"component": "parser",
				"configuration": "file:///dfs/opt/bda/apps/test/
				    test_temperature.template.jconf",
				"persist": "true"
			},
			{
				"component": "transformer",
				"configuration": "file:///dfs/opt/bda/apps/test/
				    test_transformer_temperature.template.jconf",
				"persist": "true"
			},
			{
				"component": "sql",
				"configuration": "file:///dfs/opt/bda/apps/test/
				    test_sql_temperature.template.jconf",
				"persist": "true"
			},
			{
				"component": "esloader",
				"configuration": "file:///dfs/opt/bda/apps/test/
				   test_esloader_temperature.template.jconf",
				"persist": "true"
			}
	
		]
	}
		      
	    
	        
   
	      
==== Configuring XDF-RTA for multiple types

   XDF-RTPS can be configured to invoke different pipeline based on value
   of one of the columns in real time dataset. 
   Ex: Based on event type in IOT, each event type invokes separately
   configured pipeline.
     
{
	"pipeline": {
		"rta": {
			"component": "rtps",
			"configuration": "file:///dfs/opt/bda/apps/test/test_RTPS.jconf",
			"keyColumn": "EVENT_TYPE",
			"isTimeSeries": "true"
		},

		"tempHumidity": [{
				"component": "parser",
				"configuration": "file:///dfs/opt/bda/apps/test/temp_parser.jconf",
				"persist": "true"
			},
			{
				"component": "transformer",
				"configuration": "file:///dfs/opt/bda/apps/test/temp_transformer.jconf",
				"persist": "true"
			},
			{
				"component": "sql",
				"configuration": "file:///dfs/opt/bda/apps/test/temp__sql.jconf",
				"persist": "true"
			},
			{
				"component": "esloader",
				"configuration": "file:///dfs/opt/bda/apps/apps/test/temp__esloader.jconf",
				"persist": "true"
			}
		],

		"motion": [{
				"component": "parser",
				"configuration": "file:///dfs/opt/bda/apps/apps/test/motion_parse.jconf",
				"persist": "true"
			},
			{
				"component": "transformer",
				"configuration": "file:///dfs/opt/bda/apps/test/motion_transformer.jconf",
				"persist": "true"
			},
			{
				"component": "sql",
				"configuration": "file:///dfs/opt/bda/apps/test/motion__sql.jconf",
				"persist": "true"
			},
			{
				"component": "esloader",
				"configuration": "file:///dfs/opt/bda/apps/test/motion_esloader.jconf",
				"persist": "true"
			}
		]
	}
}


=== RTPS

==== Installing

RTPS is installed as part of SIP.  Ensure the SIP environment
configuration (`sip-config`) has an entry for RTPS:

        [sip-rtps]
        host

RTPS will get installed at the location `/opt/bda/sip-rtps` on the
given host.

==== Upgrading

To upgrade an existing installation, follow the same steps as for
installing an entirely new environment.  The deploy command will
detect an already existing installation and upgrade it.

NOTE: Downgrading to an older RTPS version is not supported.  If
needed, take a backup of the entire environment before upgrading that
can be used to restore the environment to a state that is supported by
older RTPS versions.

==== Configuring

RTPS must be configured to keep memory consumption within the limits
provided by the Spark configuration.  The memory consumption of RTPS
depends on the number of records in each batch and the size of each
record.  The number of records included into each batch can be
configured in the RTPS configuration file `appl.conf` as shown below:

        spark {
          streaming.kafka.maxRatePerPartition = 1000
        }

The `maxRatePerPartition` parameter sets an upper limit on the number
of records fetched per second per MapR stream partition.  If the limit
is lowered, the upper bound of RTPS memory consumption is also
decreased.

NOTE: If the RTPS has been down for an extended period of time, a
large backlog of messages might have been built up.  To prevent the
RTPS from running into memory errors while processing a large backlog
of messages, ensure it has been properly configured to limit memory
consumption.

=== RTIS

==== Installing

RTIS is installed as part of SIP.  Ensure the SIP environment
configuration (`sip-config`) has an entry for RTIS:

        [sip-rtis]
        host

RTIS will get installed at the location `/opt/bda/sip-rtis` on the
given host.

See the SIP environment configuration (`sip-config`) for how to set
RTIS configuration parameters.

==== Upgrading

To upgrade an existing installation, follow the same steps as for
installing an entirely new environment.  The deploy command will
detect an already existing installation and upgrade it.

NOTE: Downgrading to an older RTIS version is not supported.  If
needed, take a backup of the entire environment before upgrading that
can be used to restore the environment to a state that is supported by
older RTIS versions.

==== RTIS App Key Registration

RTIS requires app key to submit the events. Follow the below steps to create the app key:

. Login to SIP application and navigate to workbench.
. Navigate to RTIS App keys page. This will fetch all the app keys for the customer.
. Click on RTIS registration page to create a new app key. Fill the form with the following fields.
.. Model - SIP support models (Countly|Generic|Simple|Simple-Json)
.. Batch Size - It measures batch size in total bytes instead of the number of messages.
.. Buffer Full Size - It maintains buffers of unsent records for each partition.
.. Event URL - It is the URL to submit the events.
.. Timeout in MS - It is the timeout configured on the leader in the Kafka cluster. This is the timeout on the server side.
.. Stream/Queue - Name of the Queue.
.. Topic - Name of the topic.

NOTE: App key is auto generated, user does not allow to specify the app key.

=== System time zone

Nodes can be configured to use any desired system time zone.  To set
the system time zone, use the following command:

    $ timedatectl set-timezone <timezone>

NOTE: The available time zones can be listed using the `timedatectl
list-timezones` command.

The configured time zone will be reflected in system and application
logs viewed using for example the `journalctl` command.

== Installing SIP product modules

Execute the following steps to install SIP product modules:

. Get the SIP product module package (`sip-module-*.tgz`)

. Copy the SIP product module package to the SIP deploy host, where
  the SIP package and SIP environment configuration (`sip-config`) is
  located

. Execute the product module deploy command, giving it the path to the
  SIP environment configuration file as the first argument and the SIP
  product module package as the second argument:

        cd saw
        ./sip-deploy-module <sip-config> <sip-module-package>

== Security

SIP sends data over the network and store data in persistent storage
in an unencrypted form by default.  Optionally, SIP can be configured
to encrypt data in flight and at rest.  Encryption can be enabled by
adding the following to the SIP environment configuration file
(`sip-config`), before the initial installation.

        [all:vars]
        sip_secure=True

NOTE: The `sip_secure` configuration parameter must be set before the
first installation of SIP and not changed thereafter.  Changing the
value after the initial installation for an existing environment is
not supported.

=== Data in flight

Enabling the secure mode will make SIP use HTTPS for communication
between services and when acting as a client to the internal MariaDB
Galera cluster, Elasticsearch cluster and MapR cluster.

Connections between SIP services are secured using certificates signed
by an internal certificate authority (CA).  The CA is located on the
SIP node with the `sip-admin` role.

=== Data at rest

Enabling the secure mode will make the SIP internal MariaDB Galera
cluster encrypt data stored in the database.  Database encryption is
set up automatically on the first installation and happens thereafter
transparently.

=== Mapr secured custer authentication ticket

MapR uses authentication with tickets to access the mapr cluster, file system
in secure mode. sip-deployment configured for setting the mapr ticket in
sip-nodes gracefully if exists, in case mapr ticket require to setup
with respect to any changes in mapr cluster below steps can be followed for `mapr`
user.

     # copy the ticket for mapr user
      cp /opt/mapr/conf/mapruserticket /opt/mapr/conf/maprticket_5000
     # provide permission to mapr user
      chown mapr:mapr /opt/mapr/conf/maprticket_5000
     # set the mapr ticket location in enviroment variable for mapr user .
      export MAPR_TICKETFILE_LOCATION=/opt/mapr/conf/maprticket_5000

=== Elasticsearch SSL configuration

SIP communication with SSL enabled elasticsearch cluster require
elasticsearch SSL keystore and password (in case of secured with
password). We need to specify the below properties in sip-config
file

                saw_elasticsearch_protocol=https
                sip_elasticsearch_keyStorePath=<JKS Keystorepath>
                sip_elasticsearch_keyStorePassword=<keystore password>

SIP accepts JKS as keystore format, in case of any key format
another like PEM key, Keys needs to be converted into jks format and
configured

== Monitoring

To support monitoring of a SIP environment, services expose health
checks.

=== Health checks

SIP consists of services that are each performing some function, for
example by responding to incoming requests, consuming a queue or
executing something on a schedule.  The services are typically
expected to be functioning normally, meaning they are in a healthy
state.  However, in certain situations the services might not be able
to perform the functions expected from them, in which case they are
deemed to be in an unhealthy state.  Services expose health checks
that allow external actors to verify the health state of a service.
This can be used in operations to detect issues early and pinpoint the
source of a problem.  The health checks are additionally internally
used for high availability, to route requests to services that are in
a healthy state.

Health checks of services are accessed through a REST API.  Each
service's endpoint exposes a `/actuator/health` resource, which
indicates the health status of the service.  If the response is `HTTP
200 OK` and the contents is a JSON object with the property `status`
set to `UP`, the service is healthy.  Any other HTTP response code or
status value indicates the service is not healthy.

The following shows a response indicating the Security Service is in a
healthy state:

        $ curl https://<sip-proxy>/saw/security/actuator/health
        HTTP/1.1 200
        Content-Type: application/json; charset=UTF-8
        <...>
        {
            "status": "UP"
        }

=== Monitoring interface

The health status of SIP services can be accessed programmatically
through a monitoring REST API.  The health status of each service
instance is available as a separate monitoring metric.  It is also
possible to query for an aggregation of health statuses, providing a
single result indicating the combined health status of all SIP service
instances.

:prometheus: https://prometheus.io/
:prometheus_query: https://prometheus.io/docs/prometheus/latest/querying/basics/

The service instance health statuses are available in the metric
`spring_boot_actuator_health_status`.  Its value is zero if the health
check is successful and a non-zero value if the health check is
failing.  This allows constructing a query that aggregates all health
statuses by summing them together, knowing that a zero sum will
indicate all services are healthy while a non-zero sum indicates at
least one service in failing its health check.  The following shows an
example of a query aggregating the health statuses of all service
instances:

        $ curl http://<sip-admin>/prometheus/api/v1/query?query=sum(spring_boot_actuator_health_status)
        {
          "status": "success",
          "data": {
            "resultType": "vector",
            "result": [
              {
                "metric": {},
                "value": [
                  1549911414.824,
                  "0"
                ]
              }
            ]
          }
        }

Note that the value is zero (value `0`), indicating that all SIP
service instances are healthy.

NOTE: For the monitoring REST API to be available in the SIP
environment, there needs to be at least one node in the `sip-admin`
role in the SIP environment configuration.

The monitoring REST API is provided by {prometheus}[Prometheus].  It
is available at the URL `http://<sip-admin>/prometheus/api/v1`, where
`<sip-admin>` refers to a node configured in in the `sip-admin` role.
The {prometheus_query}[Prometheus querying] documentation describes
the full range of querying expressions available.

=== Prometheus Elasticsearch Exporter Plugin

Elasticsearch-exporter is a service which collects all relevant
metricsfrom Elasticsearch cluster and makes them available to
Prometheus via the Elasticsearch REST API. Currently monitoring is
available cluster settings, status of the cluster, nodes and indices.

==== Installation:

On each elasticsearch nodes run the following command to install plugin

        /usr/share/elasticsearch/bin/elasticsearch-plugin install -b https://distfiles.compuscene.net/elasticsearch/elasticsearch-prometheus-exporter-6.3.1.0.zip

Once the Installation is finished metrics are directly available at

        http://<your-elasticsearch-host>:9200/_prometheus/metric

NOTE: These mertics are collected at the Prometheus server and then
visualised on Grafana Dashboard. Prometheus is already configured to
collect the metrics from elasticsearch nodes.

==== Upgrading

Exporter plugin version has to be updated whenever we update the
elasticsearch version on our environments. Compatibility matrix for
plugin version with respective elasticsearch version can be found on
the below location

        https://github.com/vvanholl/elasticsearch-prometheus-exporter#compatibility-matrix

=== Mariadb Prometheus Monitoring:

Prometheus MySQL exporter is configured on sip servers where database
is running for monitoring MariaDB and then visualizing data with
Grafana. This will enable users to have a good view of database
performance and know where to check whenever there is an issues.

== High availability

SIP supports high availability, so that if a single node in the
environment fails or becomes unavailable, services will keep working
normally.  This requires deploying the SIP environment in a highly
available configuration.

=== Highly available configuration

To ensure a highly available SIP environment, each role in the SIP
environment configuration (`sip-config`) must have at least two hosts.
Additionally the `saw-security-arbitrator` role should have a separate
third host (required for the MariaDB Galera Cluster).

NOTE: The `saw-security-arbitrator` role is only needed if there are
two MariaDB hosts.  It is not needed if there are three or more
MariaDB hosts.

The sample configuration provided in the SIP package contains the
hosts `sip-host1` and `sip-host2` in each role, plus `sip-host3` in
the arbitrator role, which makes it a highly available configuration.
The following is an example of a role that has been configured to be
highly available:

        [sip-role]
        sip-host1
        sip-host2

NOTE: The arbitrator role has low resource requirements and can
therefore be placed on a host whose primary purpose is something else,
as long as it is separate form the other hosts in the SIP
configuration.

=== Full high availability

SIP can be deployed with multiple hosts in the `saw-proxy` role, which
will ensures redundancy for every host in the SIP environment.
However, this requires clients of the SIP Services to be aware of each
host in the `saw-proxy` role.  Additionally, the SIP Web module
currently is only able to use a single endpoint, making it susceptible
to being unavailable of that single endpoint becomes unavailable.

Due to the above, SIP is currently recommended to be deployed with a
load balancer in front of the hosts in the `saw-proxy` role, that
exposes all of the hosts as a single endpoint reachable with a single
IP address.  This will allow clients and the SIP Web module to
continue working even if one of the hosts in the `saw-proxy` role
become unavailable.

=== Adding and removing hosts

After a host fails or becomes unavailable, the SIP environment should
be restored to its original host count and layout as soon as possible
to ensure high availability.  Do this using the following steps:

. Provision a new host with the same specifications as the failed host

. If the hostname of the new host is different from the hostname of
  the failed host, update references to the old hostname in the SIP
  environment configuration (`sip-config`) to refer to the new
  hostname

. Rerun the SIP deployment (`sip-deploy <config>`) to install software
  and configuration on the new host and update the configurations of
  other hosts to include the new host

=== Health checks for high availability

The high availability support for SIP is based on health checks of
services.  Requests are routed through the environment to services
based on the health check status.  If a service is unhealthy, it will
not receive requests.  Please see <<Health checks>> in the Monitoring
section for an overview about health checks in the SIP environment.
Also see <<Troubleshooting health checks>> in the Maintenance and
troubleshooting section for how to investigate failing health checks.

=== MariaDB high availability

The SIP environment includes a highly available MariaDB Galera
Cluster.  It is automatically managed by the SIP deployment and
configuration and requires no manual handling in normal operation.

In case all hosts in the SIP environment have been shut down and
brought up again, please see <<Bootstrapping the MariaDB Galera cluster>>
section for instructions on how to bootstrap the cluster again.

=== Service level

After a host fails or becomes unavailable, services of the SIP
environment should be restored within 60 seconds.  During a short time
period after a failure, services might be temporarily unavailable or
return errors, before they are fully restored again.

== Maintenance and troubleshooting

=== Restarting SIP nodes

SIP nodes can be restarted one at a time and will automatically come
back into service when starting up.  However, if multiple SIP nodes
need to be restarted or stopped simultaneously, stop them in the
following specific order:

1. First stop any nodes in the `saw-security-arbitrator` role

2. Then stop nodes in all remaining roles

Applying the above order allows the MariaDB Galera Cluster to start up
automatically when the SIP nodes are started again.

NOTE: If SIP nodes were stopped in another order and the MariaDB
Galera Cluster is unable to start automatically, please see
<<Bootstrapping the MariaDB Galera cluster>> for instructions on how
to start the cluster manually.

=== Accessing application logs

The SIP systemd services system logs can be accessed using the `sudo
journalctl` command.  To view the logs of individual services, use the
`-u` option:

        $ sudo journalctl -u sip-\*

=== Listing services and their statuses

To list services and check the status of all SIP systemd units,
execute the following commands:

        $ sudo systemctl list-units sip-\*

NOTE: Some services use
http://0pointer.de/blog/projects/socket-activation.html[socket
activation] to reduce memory usage and shorten deploy times.  These
services will be listed as not running (inactive dead) until the first
connection is made over the network.  This is normal for
socket-activated services and does not indicate a problem.

=== Starting, stopping and restarting services

Under normal circumstances there should be no need to start, stop or
restart SIP services manually.  However, if needed it can be done
using the following commands:

        $ sudo systemctl start <sip-service>
        $ sudo systemctl stop <sip-service>
        $ sudo systemctl restart <sip-service>

Where `<sip-service>` is one of the SIP systemd services (for example
`sip-gateway`), which can be listed using the `sudo systemctl
list-units sip-\*` command shown in the previous section.

=== Location of SIP data in MapR-FS

SIP data is stored under a single specific directory in MapR-FS.  The
location of this directory is configured using the `sip_var_path`
variable in the SIP environment configuration (the `sip-config` file)
and its default value is `/var/sip`.

By knowing where the SIP data is located, it is possible to configure
MapR volumes that can be used to snapshot SIP data, for backup and
restore purposes.  Even if no dedicated MapR volume has been
configured for SIP, the data can still be snapshotted as part of the
root volume.  Please refer to MapR documentation for instructions on
creating volume snapshots.

NOTE: SIP data (both plain files and MapR-DB tables) can be moved
between MapR-FS locations using the standard filesystem tools, as long
as it is within the same volume.  Moving SIP data across MapR volumes
requires using the MapR-DB CopyTable tool.  Please refer to MapR
documentation for instructions on using that.

=== Troubleshooting health checks

Health checks are initiated from HAProxy every 10 seconds.  The
HAProxy configuration can be inspected in the
`/etc/haproxy/haproxy.cfg` file.  Execute the following command to see
logs from HAProxy:

        $ sudo journalctl -u haproxy

HAProxy additionally provides a statistics page located at
`http://<sip-url>/haproxy_stats` showing the status of the
environment.

Each health check initiated from HAProxy causes a cascade of health
checks of upstream dependencies.  To find out what the upstream
services are, look at the configuration of the service.

TIP: For example the Gateway Service configuration is located in
`/opt/bda/saw-gateway-service/conf/application.yml` and contains the
URLs of its upstream services.

Once the URLs of the upstream services have been identified, append
`/actuator/health` to get the health check location, which can then be
used to manuall inspect it:

        $ curl http://<service-url>/actuator/health

The health check responses contain a `detail` propery with a
human-readable explanation of the health check status, which can be
used to further narrow down the cause for a possible failure.

=== ElasticSearch Max records setting
By default, ElasticSearch limits the number of records to 10000.
If the user tries to query for more, it will throw an error.
To allow this, max_result_window has to be set. Below is the syntax
to set max_result_window:

----
curl -X PUT \
  <ElasticSearch Host>/<index_name>/_settings \
  -H 'Content-Type: application/json' \
  -d '{ "max_result_window" : 300000 }'
----

=== MapR Table Size Setting
Mapr supports default max row size of table `32768KB`. Size can be
checked with the below MapR cli command:
----
maprcli config load -json | grep mfs.db.max.rowsize.kb
----

If the users tries to execute/query Analysis more data (> 32MB) and
tries to save in the table, table will throw an error like:

`ERROR com.mapr.fs.Inode - checkAndReplaceOrDelete() on
'<Table name >'
 failed with error: Argument list too long (7).`

Below is the command to increase the size of mapr table:
----
maprcli config save -values {"mfs.db.max.rowsize.kb":"128000"}
----

=== General guideline for ElasticSearch script support

SIP is using custom scripting of ElasticSearch to build and evaluate
derived metrics,filters etc.

SIP uses inline scripting.
To enable this, the below parameter has to be set in elasticsearch.yml file

----
script.allowed_types: inline
----

Also, only search operations will be performed using script,
 update operation can be disabled for security reason.

----
script.allowed_contexts: search
----

=== Troubleshooting the MariaDB cluster

To troubleshoot MariaDB, inspect the MariaDB logs on each of the hosts
in the cluster:

        $ sudo journalctl -u mariadb

The logs will also contain messages related to the MariaDB Galera
cluster.

=== Bootstrapping the MariaDB Galera cluster

If the MariaDB Galera cluster is unable to determine that it can start
up automatically, it can be started manually instead.  In such cases
bootstrap the cluster manually using the following command:

        $ sudo galera_new_cluster

The host to run that command on should be the host that has
`safe_to_bootstrap: 1` in its `/var/lib/mysql/grastate.dat` file.

NOTE: If no node has `safe_to_bootstrap: 1`, find the node that was
shut down last and edit its `/var/lib/mysql/grastate.dat` file to say
`safe_to_bootstrap: 1`.

:mariadb_doc: https://mariadb.com/kb/en/library
:galera_doc: {mariadb_doc}/getting-started-with-mariadb-galera-cluster

For more information, see the
{galera_doc}#restarting-the-cluster[restarting the cluster] section in
the MariaDB Galera Cluster documentation.

=== Clearing the Transport Service Executor queues

If the SIP report execution queue has filled up, for example due to
many long-running queries being executed, the queues can be cleared
using the following commands:

        $ ssh <mapr-host>
        $ stream=<report-executor-path>/saw-transport-executor-regular-stream
        $ sudo -u mapr maprcli stream topic delete -path $stream -topic executions
        $ stream=<report-executor-path>/saw-transport-executor-fast-stream
        $ sudo -u mapr maprcli stream topic delete -path $stream -topic executions

* <report-executor-path> can be found in saw-transport service configuration file.

Please note that clearing the queues affects all users of the system
and report execution types.

=== Clearing the onetime execution result for data lake

Saw execution result for data lake analysis can be cleaned-up if output location
contains large amount of onetime(preview) execution results.

 $ ssh <mapr-host>
 $ hadoop fs -rm -r <output-location>/preview-*

Note: <output-location> can be found in saw-host, saw-transport service conf file
in location /opt/saw/service/conf/application.conf properties name
`output-location`. Please do not delete anything which doesn't contains
preview in directory name.

=== Automatically generated credentials

Automatically generated credentials, such as for internal service and
administrator accounts, can be found in the `/etc/bda` directory on
the respective host.

=== Batch ingestion: SFTP

The Batch Ingestion Service can connect to SFTP sources to ingest
files.  Ingesting a large number of files requires the Batch Ingestion
Service to be able to use a large number of sessions per SFTP
connection.  The default settings of OpenSSH only allows ten
concurrent sessions per SFTP connection.  To allow Batch Ingestion
Service to use the required number of session, add the below to the
`/etc/ssh/sshd_config` file:

    MaxSessions 5000

After that run `systemctl reload sshd` to reload the SSH daemon
configuration.

The Batch Ingestion Service intitially scans for matching files
with pattern and creates records for processing. Scanned records
will be picked up by worker threads for transfer. Below property
is used to for worker threads schedule to pick up scanned records

    worker.delay: 5000

Any long running 'INPROGRESS' file transfers/jobs will be marked as 'FAILED'
Long runing transfers will be again picked up by retry. Long running job is marked
as failed  after waiting for 1 day(default value) to make sure no impact on
huge files.


    max.inprogress.mins: 45
    max.inprogress.job.mins: 1440


== Batch Ingestion : High Availability

The below parameters in Batch Ingestion Service gives the control the
configurations related to resiliency & avoid inconsistency across bis
instances. To control what should be the time difference to declare a
data in the system is corrupted, & needs to be downloaded again
(default is 5 mins). While configuring this parameter we need account
for all customers using the BIS & routes is being configured.

Specifically a use case which needs to be taken into account as
follows: Let's say, we have a daily schedule at `5:00` PM for all
`*.txt` files and as clean up job customers have a scheduled job on
the customer data location to archive those files daily at `10:00
PM`. then if there is a failure or network glitches or may be due to
some other reasons data source is not available until `10:00 PM` and
we just marked status as failed there might be chances of data loss as
`*.txt` file which has been archived before next schedule. The reason
to mention this scenario so that we can configure the
`sip_bis_retry_diffs_in_minutes` accordingly.

    sip_bis_retry_diffs_in_minutes=5



The below parameter will switch on or off to activate secure or non-secure
communication in sip platform

    sip_secure=True/False

Note: This is mandatory field it has to be either 'False' or 'True'. Please observe the case of `T & F`
of `True` & `False` Note: true or false will result into an error. And to make sure only TLS v1.2 are
in use. IT team has to disable TLS v1.1 while installing JDK using the following COMMANDS

The below command will append the line in java.security file at the end
`echo "jdk.tls.disabledAlgorithms=SSLv2Hello, TLSv1, TLSv1.1, SSLv3, RC4, DES, MD5withRSA, DH keySize < 1024, EC keySize < 224, anon, NULL" >> /usr/lib/jvm/java-1.8.0-amazon-corretto/jre/lib/security/java.security`

This line will verify whether it has been added properly or not.
`grep jdk.tls.disabledAlgorithms /usr/lib/jvm/java-1.8.0-amazon-corretto/jre/lib/security/java.security`

== Loading semantic metadata

To enable creating analyses in SIP, load semantic metadata as follows:

        $ ssh <sip-services-host>
        $ sudo -u mapr /opt/saw/service/bin/mdcli.sh -i \
            file://<nodes-json> -o file:///tmp/log.json

The semantic metadata JSON is stored in the `<nodes-json>` file.

=== Semantic metadata format

Semantic metadata supports the following values for the `type`
property:

* `integer`
* `long`
* `float`
* `double`
* `string`
* `date`

NOTE: Paths to files in the data lake must not contain spaces.

== Onboarding customer

We can utilise customer_onboard.sh script in order to execute the
command with current environment setup.

    cd /opt/bda/saw-security/bin/
    bash customer_onboard.sh

Features of spring boot shell:

. Type in "help" and it will show you all the available commands

. Tab based auto completion is supported.


    shell:>help
    AVAILABLE COMMANDS
    Built-In Commands
            clear: Clear the shell screen.
            exit, quit: Exit the shell.
            help: Display help about available commands.
            script: Read and execute commands from a file.
            stacktrace: Display the full stacktrace of the last error.
    Saw Security Shell
            onboard-customer: Onboard the customer
            auto-execution: Make Charts,Pivots and ES Reports Execute each time when land on View Analysis Page
    shell:>


Once you are inside the shell, type in "help onboard-customer" and it will show you all required parameters the command accepts.

    shell:>help onboard-customer

    NAME
    	onboard-customer - Onboard the customer

    SYNOPSYS
    	onboard-customer [--C] string  [--P] string  [--PC] string  [--E] string  [--F] string  [--M] string  [--L] string [--JV] string [--FC] string

    OPTIONS
    	--C  string
    		Customer-Code
    		[Mandatory]

    	--P  string
    		Product-Name
    		[Mandatory]

    	--PC  string
    		Product-Code
    		[Mandatory]

    	--E  string
    		User-Email
    		[Mandatory]

    	--F  string
    		User First Name
    		[Mandatory]

    	--M  string
    		User Middle Name
    		[Mandatory]

    	--L  string
    		User Last Name
    		[Mandatory]

    	--JV  string
    		Super Admin? (No customerCode filtering)
    		[Mandatory]

    	--FC  string
    		Filter By Customer Code Flag
    		[Mandatory]

Run the "onboard-customer" command with all required parameters stated
above and it will start the process of creating customer and related
products/components in the system.

If the product code already exists in the system, then the customer is tied to the existing product,
else new Product will be created and linked to the customer.

The below is one such example to call onboard-customer with its
required parameters.


    shell:>onboard-customer --C SNCR0001 --P SIPDEMO --PC workbench --E abc@xyz.com --F ABC --M XY --L Z --JV 1 --FC 0
    1
    2019-02-19 08:04:41.326  INFO 17665 --- [           main] c.s.saw.security.app.admin.OnBoardShell  : Connection successful !!
    2019-02-19 08:04:41.357  INFO 17665 --- [           main] s.a.r.i.OnBoardCustomerRepositoryDaoImpl : res

    ====== ONBOARD CUSTOMER : Successful ======

Note : A parameter containing a space in between can be enclosed in either single quotes('') or double quotes("").

        Ex : onboard-customer --C SNCR0001 --P SIPDEMO --PC workbench --E abc@xyz.com --F ABC --M 'X Y' --L Z --JV 1 --FC 0

== Turning Auto Execution ON/OFF (Charts, Pivots and ES Reports)

We can utilise customer_onboard.sh script in order to execute the
command with current environment setup.

    cd /opt/bda/saw-security/bin/
    bash customer_onboard.sh

Features of spring boot shell:

. Type in "help" and it will show you all the available commands

. Tab based auto completion is supported.

To Enable/Disable auto Execution for analysis, type "help auto-execution" and it will show all required parameters that the command accepts.

    shell:>help auto-execution

    NAME
    	auto-execution - Make Charts,Pivots and ES Reports Execute each time when land on View Analysis Page

    SYNOPSYS
    	auto-execution [--C] string  [--F] integer

    OPTIONS
    	--C  string
    		Customer-Code
    		[Mandatory]

    	--F  integer
    		 Active Status Indicator flag
    		 [Optional, DefaultValue : 1]

Run the "auto-execution" command with required parameters stated as above.

The below is one such example to call auto-execution with its
required parameters.


    shell:>auto-execution --C SNCR0001 --F 1

NOTE : If the configuration is not present in the system, the command adds a new configuration. If its already present, the corresponding entries will be updated based on the input.

==  SIP SSO Authentication

SIP supports external systems to authenticate users (single
sign-on).The shared secret key is read from the SIP environment
configuration, as a base64 encoded string (while ensuring Synchronoss
Global Information Security standards for storing secret keys are
adhered to).  Recommended key size is 256 bits.

   Command to generate key : openssl rand 32 -base64
   Dgus5PoaEHm2tKEjy0cUGnzQlx86qiutmBZjPbI4y0U=

After generating the key, add it to the SIP environment configuration
({{sip-config}}) in the {{saw_security_sso_secret}} parameter and
redeploy.

== Export Configuration

=== email and ftp extract size configuration

SIP supports *exporting* reports and pivots:

. from UI
. to email
. to ftp/sftp servers
. to S3 Bucket

In sip-config, we can configure how many number of rows we want to
extract for all the reports / pivots:

.sip-config
[source, yaml]
----
saw_ui_export_size=10000
saw_email_export_size=50000
saw_ftp_export_size=1000000
saw_s3_export_size=1000000
----

SIP reports are exported chunks of rows, we can configure how many
rows to take at a time for processign reports, can be configured using
following config parameter:


.sip-config
[source, yaml]
----
saw_export_chunk_size=10000
----


=== FTP server configuration

SIP supports exporting of pivots and reports to ftp/sftp servers.
By default an empty configuration is installed in
`/opt/bda/saw-export-service/conf/ftp-details.json` file on saw nodes.

The contents of this configuration can be changed using `sip-config`.
An example configuration has been included in config file.

.sip-config
----
# FTP JSON config
# (Do not split this into multiple lines)
# ftp_json_config='{"ftpList":[{"customerName":"CUSTUNIQUEID","alias":"ftpsrv1","host":"srv1","port":21,"username":"usr1","password":"pwd1","location":"/path/to/dir/","type":"ftp"}]}'
----

Example contents (in pretty format):

.ftp-details.json
[source, json]
----
{
  "ftpList": [
    {
        "customerName":"UNIQUE_IDENTIFIER1",
        "alias" : "server1",
        "host": "server1.customer1.com",
        "port": 21,
        "username": "usr1",
        "password": "pwd1",
        "location": "/some/location/",
        "type": "ftp"
    },
    {
        "customerName":"UNIQUE_IDENTIFIER1",
        "alias" : "server2",
        "host": "server2.customer1.com",
        "port": 22,
        "username": "usr2",
        "password": "pwd2",
        "location": "/some/location/",
        "type": "sftp"
    },
    {
        "customerName":"UNIQUE_IDENTIFIER2",
        "alias" : "server1",
        "host": "server1.customer2.com",
        "port": 21,
        "username": "imuser1",
        "password": "pwd3",
        "location": "/home/ubuntu",
        "type": "ftp"
    },
    {
        "customerName":"UNIQUE_IDENTIFIER3",
        "alias" : "server1",
        "host": "server1.customer3.com",
        "port": 21,
        "username": "imuser1",
        "privatekeyFile": "key1.pem",
        "location": "/home/ubuntu",
        "type": "sftp"
    },
    {
        "customerName":"UNIQUE_IDENTIFIER4",
        "alias" : "server1",
        "host": "server1.customer4.com",
        "port": 21,
        "username": "imuser1",
        "privatekeyFile": "key1.pem",
        "passPhrase": "Key@123",
        "location": "/home/ubuntu",
        "type": "sftp"
    }
  ]
}
----

In above example, `customerName` is the unique identifier given at
the time of onboarding customer. Note that based on this unique
identifier, customers are differentiated. Each FTP/SFTP
server is required to have unique entry which gets presented to front
end, this is maintained by means of `alias` entry. *Note* that each
server entry per customer is required to have a unique alias entry
which gets presented in front end.

Connection to sftp can be established either using username and password
combination or username and a private key. All the private keys should be placed
along side the sip deployment directory under `sftpkeys`. During deployment, all these
private keys will be placed under `/etc/bda/sftpkeys/` on all the saw nodes with
permission `664`. In the ftp_details configuration, only private key
file name needs to be specified using `privatekeyFile`. If the private
key has an optional pass phrase, that can be specified
using `passPhrase` parameter.

NOTE: Please make sure to put minified JSON in configuration file.

=== S3 Bucket configuration for the

SIP supports exporting of pivots and reports to S3 Bucket.
By default an empty configuration is installed in
`/opt/bda/saw-export-service/conf/S3-details.json` file on SIP nodes.

The contents of this configuration can be changed using `sip-config`.
An example configuration has been included in config file.

.sip-config
----
# S3 JSON config
# (Do not split this into multiple lines)
# s3_json_config='{"s3List":[{"customerCode":"SYNCHRONOSS", "alias" : "xdftest1", "bucketName": "xdftest","region":"us-east-1", "outputLocation": "Analysis", "accessKey": "AKIAJ******4AECBVQ", "secretKey": "Q4QP2V*****moyJ9BtDS+QRmqgR****0OcYNXDIz", "type": "s3"}]}'
----

Example contents (in pretty format):

.S3-details.json
[source, json]
----
{
  "s3List": [
    {
      "customerCode": "SYNCHRONOSS",
      "alias": "xdftest1",
      "bucketName": "xdftest",
      "region": "us-east-1",
      "outputLocation": "Analysis",
      "accessKey": "AKIAJ******4AECBVQ",
      "secretKey": "Q4QP2V*****moyJ9BtDS+QRmqgR****0OcYNXDIz",
      "type": "s3"
    },
    {
      "customerCode": "TenantA",
      "alias": "test1",
      "bucketName": "test1",
      "region": "us-east-1",
      "outputLocation": "Analysis",
      "accessKey": "AKIAJ******4AECBVQ",
      "secretKey": "Q4QP2V*****moyJ9BtDS+QRmqgR****0OcYNXDIz",
      "cannedAcl": "BucketOwnerFullControl",
      "type": "s3"
    }
  ]
}
----

In above example, `customerName` is the unique identifier given at
the time of onboarding customer. Note that based on this unique
identifier, customers are differentiated. Each S3 bucket
is required to have unique entry which gets presented to front
end, this is maintained by means of `alias` entry. *Note* that each
Bucket entry per customer is required to have a unique alias entry
which gets presented in front end.

S3 canned ACLs supported with write permission can be
configured in S3-details.json file , SIP Supported List of canned ACLs List

   1) "cannedAcl": "BucketOwnerFullControl"
   2) "cannedAcl": "PublicReadWrite",
   3) "cannedAcl": "LogDeliveryWrite"
