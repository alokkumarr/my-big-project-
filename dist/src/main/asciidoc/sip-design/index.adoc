= SIP Design Guide
include::header.adoc[]

== Introduction

This document describes the high-level design of the SIP platform,
including its architecture and modules.

== Modules

SIP provides end users three modules: Analyze, Observe and Workbench.
Additionally SIP is organized into three implementation layers: Web,
Services and Security.  The figure below explains the relation between
modules and service layers.

.SIP Modules and layers
ditaa::sip-design/figure-modules.txt[separation=false]

The web applications are implemented using AngularJS, while services
are implemented using Spring Boot.  The Metadata Library is a shared
Java library.

NOTE: The Analysis and Execution Services shown above are currently
bundled together as the Transport Service, implemented in Scala and
Play.

== Interfaces

SIP has three main external interfaces: SIP Web, SIP Services and SIP
Security.  SIP Web provides a web application for creating and
executing analysis.  It builds on top of SIP Services, which provides
a REST API for creating and executing analyses.  SIP Security manages
authentication and privileges of users.

.External SIP interfaces and their use
plantuml::sip-design/figure-interfaces.pum[]

== Security

=== Authentication

Requests to SIP Services require to be authenticated.  Clients that
need authentication first request a token from the SIP Security
Service, which is then included with subsequent requests for
authentication.

The method used is http://www.haproxy.org/[JSON Web Tokens].  The
server side has a secret key, which is used to sign a set of
statements about the client during the initial authentication
sequence.  This token is then passed to the client, which provides it
with subsequent requests.  The server can then validate the token by
inspecting its signature and the secret key.

.Authentication using JSON Web Tokens
plantuml::sip-design/figure-authentication.pum[]

For more details see the <<SAW Gateway Service>> and <<SAW Security>>
sections.

=== Authorization

Clients are authorized by the SIP Gateway Service using the user
database managed by the SIP Security Service.  The database stores
users, their credentials and privileges.  Examples of privileges are
permissions to view folders, access SIP modules and so on.

== High availability

SIP is required to provide high availability of its services.  If one
node in the SIP environment fails or becomes unavailable, services are
expected to keep working normally.

=== Health checks

High availability is achieved using different techniques based on the
type of the service.  Common to all services are the use of health
checks.  Services provide health checks to indicate if they are able
to serve requests normally.  This allows service clients (which can be
external or other SIP services) to only route requests to service
instances which are indicated as being healthy.  See the SIP
Operations Guide for more details on service health checks.

=== High availability proxy

SIP provides a high availability proxy by default, which allows
external clients to connect to a single host, which in turn routes the
request to the appropriate service instance based on health checks.
This is implemented using http://www.haproxy.org/[HAProxy].

Using the high availability proxy simplifies SIP client requirements,
because it removes the need for clients to keep track of multiple SIP
service instances and their health statuses.  The downside of using it
is that if the high availability proxy itself becomes unavailable, for
example due to the node that it is running on failing, clients will
not be able to connect at all.  To make also the HAProxy service
highly available, configure redundant HAProxy nodes that listen to a
virtual IP maintained by Keepalived.

.High availability proxy for services
plantuml::sip-design/figure-ha-proxy.pum[]

=== High availability for stateless services

SIP provides high availability for stateless services by running
multiple redundant instances of them on separate nodes.  If one node
fails and makes the service running on that node unavailable, the
service instance on the other node can keep serving requests.

:quartz_doc: http://www.quartz-scheduler.org/documentation/
:quartz_cluster: quartz-2.x/configuration/ConfigJDBCJobStoreClustering.html

The Scheduler Service additionally runs the Quartz scheduler in
{quartz_doc}/{quartz_cluster}[clustered mode], which ensures that
scheduled tasks continue to be executed even if one of the Scheduler
Service instances becomes unavailable.  The clustered mode setting
ensures that each scheduled task is executed by only one of the
instances.  The Quartz scheduler is configured to use MariaDB as its
job store, which itself is highly available through the use of a
MariaDB Galera cluster setup.

.High availability for Scheduler Service
plantuml::sip-design/figure-ha-scheduler.pum[]

=== High availability for stateful services

Services that store some kind of state, such as a database, require
each their own solutions to provide high availability.  The only
stateful service is the SIP Security Service, which manages a MariaDB
database.

The SIP Security Service provides high availability by running the
database in a multi-master mode using a MariaDB Galera cluster setup.
In case a single cluster member becomes unavailable, the rest of the
cluster members will continue providing service.  All cluster members
are equal and can be used for both read and write operations.  Clients
can therefore connect to any cluster member, and do not need to
contain logic for following or failing over to a specific member.  All
writes are replicated synchronously across the entire cluster to
ensure consistency and availability.

.High availability for MariaDB
plantuml::sip-design/figure-ha-mariadb.pum[]

== SAW Web

The SAW Web module provides a user interface for creating and
executing analyses.  It is implemented it AngularJS and makes calls to
the SAW Services REST API over HTTP.  It additionally calls SAW
Security to authenticate users and manage privileges.

The SAW Web user interface is organized into two modules: analyze and
observe.  Additionally it provides a user interface for managing users
and privileges in SAW Security.

== SAW Services

SAW Services are a collection of microservices exposed over HTTP REST
APIs.  They enable creating and executing analyses.  They are
implemented in Java and the Spring Framework.  As an exception, the
SAW Transport Service is implemented in Scala and the Play framework.

.Figure: SAW Services and their dependencies
plantuml::sip-design/figure-services.pum[]

SAW Services use MapR-DB for persistence, using the OJAI interface.
As an exception, the SAW Transport Service uses the MapR-DB binary
tables.  Additionally SAW Services access files on the MapR-FS.

=== SAW Gateway Service

The SAW Gateway Service acts as single entry point for all upstream
micro services.  It is a Spring Boot based microservice. It upholds
the concerns regarding security.  It acts as edge service and
authenticates every request that passes by it and makes sure that it
is valid.

.Gateway Service routes requests to upstream services
plantuml::sip-design/figure-gateway.pum[]

It comes with the following benefits:

. Insulates the clients from how the application is partitioned into
  micro-services

. Insulates the clients from the problem of determining the locations
  of service instances

. Provides the optimal API for each client

. Reduces the number of requests/roundtrips.  For example, the API
  gateway enables clients to retrieve data from multiple services with
  a single round-trip.  Fewer requests also means less overhead and
  improves the user experience.  An API gateway is essential for
  mobile applications.

. Simplifies the client by moving logic for calling multiple services
  from the client to API gateway

. Translates from a "standard" public web-friendly API protocol to
  whatever protocols are used internally

The SAW gateway pattern has some drawbacks:

. Increased complexity - the API gateway is yet another moving part
  that must be developed, deployed and managed.

. Increased response time due to the additional network hop through
  the API gateway - however, for most applications the cost of an
  extra roundtrip is insignificant.

=== SAW Dataset Service

The SAW Dataset Service provides starting points for creating
analyses.  Administrators load information about datasets (including
so called semantic metadata) into SAW, which is used to create
analyses.  The information about datasets includes the location of the
data, its schema in the form of columns, data types of columns and
so on.  Clients can enumerate datasets and retrieve descriptions of
them for use when creating analyses.

=== SAW Analysis Service

The SAW Analysis Services allows creating, reading, updating and
deleting analyses.  Analyses are used to execute queries on data.  An
analysis is created based on information about a dataset, also known
as semantic metadata.  An analysis can contain one or more artifacts,
each of which contain a set of columns.  Each column in an analysis
has a number of properties, for example if it is selected, or joined
with another column.  These properties affect how the analysis is
translated into a query that is exected.

=== SAW Execution Service

The SAW Execution Service enables executing analyses.  It takes an
analysis to execute as input, translates it into a query and executes
the query and finally provides the results back to the client.

.Figure: Executing an analysis using the SAW Execution Service
plantuml::sip-design/figure-execution-sequence.pum[]

The SAW Execution Service supports two types of storage: Apache Spark
and Elasticsearch.  Analyses of type report are executed on Apache
Spark clusters, while analyses of type pivot and chart are executed on
Elasticsearch clusters.

=== SAW Workbench Service

The SAW Workbench Service enables executing Workbench related operation.  It takes an
project definition to execute as input, translates it into a Datalake operation and records
the operation's activity in MaprDB and finally provides the results back to the client.

.Figure: Executing an analysis using the SAW Workbench Service
plantuml::sip-design/figure-workbench-sequence.pum[]

The SAW Workbench Service supports three types of storage: Apache Spark,
MaprDB & Elasticsearch. This module supports & exposes xdf-nextgen related REST API,
semantic metadata configuration REST APU & data wrangling related REST API.

The Workbench Service provides a REST API on top of which the various
Workbench operations in the Web user interface are implemented.  The
Workbench Service internally uses the Metadata Library to list and
work with dataset information in MapR-DB.  Additionally it uses a
queuing mechanism for executing dataset transformations through the
Workbench Executor component.

.SIP Workbench Components
plantuml::sip-design/figure-components.pum[]

.SIP Workbench Executor
plantuml::sip-design/figure-executor.pum[]

Workbench transformations are implemented using XDF components.
Invoking an XDF component involves input and output to the data lake,
or another location such as Elasticsearch.  The Workbench Service
invokes XDF-NG components using the XDF-NG component interface.
Datasets are internally stored as JSON documents in MapR-DB. The
entries are created by XDF components as part of execution and
accessed using the BDA Metadata library.

=== SAW Storage Proxy Service

SAW Proxy Service will act as proxy for our storage i.e to make consumables application agnostic to any specific store.
The intention of this services to provide access behind our gateway for our polyglot persistence
layer *(ElasticSearch, Datalake, RDMS & MapRDB).*

[%hardbreaks]
One of the reason to access this service to perform transformation at backend service and API consumer
just needs to deal with two common formats i.e JSON or Tabular (flat structure CSV) irrespective of storage layer.

.Figure: Executing an data query using the SAW Proxy Service via SIP Gateway Service
plantuml::sip-design/figure-proxy-sequence.pum[]

The request (link:sip-developer/index.html[SIP Developer Guide]) body shall provide the query,
storage type & other details. The below are salient feature for the service which are as follows:

[[goals]]
[role="incremental"]
* It will return either in JSON or Tabular Format.
   ES returns in JSON format in terms of search, it should be converted into Tabular format if in
   the request body tabular format is requested.
* Input JSON Schema Validation.
* support create, search, update, delete & aggregate operations.
* It should support to flatten our in house build pivot format.
* Search results will provided in paginated format either in JSON or Tabular format.
* Implicit ES Query validation.
* Every incoming request to this story proxy service will be validated in gateway service layer.
* It provides CRUD REST API on top of MapRDB store. The user of the API can store any form of JSON data adhering to certain structure i.e request & response specification.

=== SAW Batch Ingestion Service

SAW Batch Ingestion Service allows to pull batch of data at uniform interval (can be defined by user) of time.
It will facilitates to pull data from remote source like *(SFTP, SCP, S3, JDBC etc).*
Currently with it supports only SCP/SFTP.

[%hardbreaks]
The REST API of this service is HATEOS Complaint API. To get an overview of the self descriptive API, it can
accessed using URL (http://hostname/saw/services/batch, http://hostname/saw/services/batch/profiles)

.Figure: Saw Batch ingestion request & response flow
plantuml::sip-design/figure-batch-sequence.pum[]

Below are the salient feature which are as follows:

[[goals]]
[role="incremental"]
* Ability to setup a channel and provide channel information
* Ability to setup a route and provide route information
* Test connectivity of both channel & source.
* Tracking from source to drop.
* Monitoring.
* Scheduling the data retrieval from the source.
* Supports various protocol to pull data.

.Figure: SAW Batch Ingestion ER Diagram
image::sip/bis-er-diagram.png[entities,400,400]

The above entities are used to hold the base of Batch Ingestion module.
Entity BIS_ROUTE is used to hold the details of route i.e it holds the details of destination, frequency etc etc.
Entity BIS_CHANNEL is used to hold the details of channel i.e it holds the details of data channel.
Entity BIS_FILE_LOGS is used to log details related to the files or data retrieved from the custom.

.Figure: SAW Batch Ingestion Component Diagram
image::sip/bis-component-diagram.png[component,400,400]

ingestion-registration-service: The internal service will handle all the request for registration of channel & route
ingestion-provenance-service: The internal service will handle all the scheduled jobs.
ingestion-notification-service: Not yet started.

.Figure: SAW Batch Ingestion Flow Diagram
image::sip/bis-flow-diagram.png[flowChart,400,400]

=== SAW Semantic Service

The SAW Semantic Service enables the consumer to store the semantic metadata of SIP. The intention of this services
to provide CRUD operations to deal with semantic metadata

.Figure: Executing an data query using the SAW Proxy Service
plantuml::sip-design/figure-semantic-sequence.pum[]

1. Create Integrated Semantic Node JSON structure for both elastic search as well as data lake data pods.
2. The structure is consistent with that used by SAW Analyze module and Observe modules in the store.
3. This service should be consumable by
  . SAW Analyze Module
  . SAW Workbench Module

==== Apache Spark

The Apache Spark executor supports analyses of type report.

Reports are executed as Spark SQL queries running on an Apache Spark
cluster.  The queried data is stored as Parquet files in the data
lake.  The report execution functionality is provided by two
components: the Transport Service and the Transport Service Executor.

The Transport Service provides an internal REST API for SAW Web to
use, including operations to execute a report.  When a report is
executed, the Transport Service writes a message requesting execution
to a message queue.  The message queue is implemented using MapR
streams.  The Transport Service Executor consumes messages from the
queue and executes queries accordingly.

Executors are run in two different modes: fast and regular.  The fast
executors read from the fast queue to which preview and onetime
executions are sent, with expectations of lower latency using
techniques such as preallocated Spark contexts.  The regular executors
read from the regular queue to which scheduled executions are sent.
Using two different queues limits the resources provided to
potentially heavy and long-running scheduled executions to avoid
blocking the more time-sensitive preview and onetime executions.

The queue approach with executors in separate processes is used due to
the limitation of having one Spark context per Java virtual machine.
The number of executors of each type is configured statically in the
SAW environment configuration and used during deployment.  The report
execution concurrency limit follows from the number of executors
configured for each type.

As a preventive measure, executors restart the Java virtual machine
after handling an execution.  This avoids building up state between
executions that can be a source of errors.

When an analysis of type report is executed by the Transport Service,
the results are stored as newline-delimited JSON in the data lake.
When results need to be read back by the Transport Service, it reads
the newline-delimited JSON file in the data lake over the MapR-FS.
The results can then be streamed to avoid reading the entire results
into memory at the same time which might lead to out of memory errors.

==== Elasticsearch

The Elasticsearch executor supports analyses of types pivot and chart.

=== SAW Export Service

The SAW Export Service enables exporting analysis executions to file
formats such as Microsoft Excel.  It calls the SAW Execution Service
to retrieve the execution result, generates the desired output file
format and finally provides it to the client over email and/or on
FTP/SFTP configured locations.

=== SAW Scheduler Service

The SAW Scheduler Service triggers execution of analyses
based on their configured schedule.  The SchedulerService is a Spring
Boot based micro-service, which provides Api to create, manage and trigger schedules.
It also triggers dispatch request to saw-export service, if analysis execution result
needs to be dispatched.

Internally it uses the Quartz scheduler framework for create, manage and trigger
analysis schedules with mariaDB as job store. The Scheduler Service
does not monitor the actual execution or its results, but only
triggers the start of execution.


=== SAW Observe Service

The SAW Observe Service enables creating, reading, updating and
deleting dashboards.

== SAW Security

The SAW Security module provides authentication and privilege services
to other modules.  It is implemented as a microservice in Java and the
Spring Framework and uses a MariaDB database to persist authentication
and privilege information.

.Figure: The SAW Security Service and dependencies
plantuml::sip-design/figure-security.pum[]

A client authenticated to the SAW Security Service by sending a to the
REST API.  The credentials and privileges are checked against the SAW
Security database, after which a token is issued and returned in the
response to the client.

.Figure: Authenticating a client using the SAW Security Service
plantuml::sip-design/figure-security-sequence.pum[]
