= SIP Developer Guide
include::header.adoc[]
// Show only one level of sections due to the large amount of sections
// coming from the automatically generated REST API documentation
:toclevels: 1

= Sample requests

== Authenticate

The HTTP request:

include::{snippets}/authenticate/http-request.adoc[]

The HTTP response:

include::{snippets}/authenticate/http-response.adoc[]

== List metrics

The HTTP request:

include::{snippets}/list-metrics/http-request.adoc[]

The HTTP response:

include::{snippets}/list-metrics/http-response.adoc[]

== SSO Authentication using jwt token

=== JWT token preparation for request

 sample java code
 public String createToken() {
     Long tokenValid = 5 l;
     String secretKey = "Dgus5PoaEHm2tKEjy0cUGnzQlx86qiutmBZjPbI4y0U="
     Map < String, Object > map = new HashMap();
     map.put("valid", true);
     map.put("validUpto", System.currentTimeMillis() + tokenValid * 60 * 1000);
     map.put("validityReason", "");
     map.put("masterLoginId", "sawadmin@synchronoss.com");
     logger.info("Request received to process single sign-on");
     String jwt = Jwts.builder()
         .setSubject("sawadmin@synchronoss.com")
         .claim("ticket", map)
         .setIssuedAt(new Date())
         .signWith(SignatureAlgorithm.HS256, secretKey)
         .compact();
     return jwt;
 }

=== General guideline for SSO request:
- The JWT secret key to use is generated and set by the SAW administrator in the environment configuration, as described in the SAW Operations Guide.
- Signature algorithm should be used as (HS256, symmetric) to generate JWT token.
- Clients should set "Cache-Control: no-store" to their header requests.
- Clients should set the token expiration ("validUpto" claim in the JWT) to a low value,
ensure tickets have a limited lifetime.



=== SSO request sample URL:

http://localhost/web/#!/authenticate?jwt=eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJzYXdhZG1pbkBzeW5jaHJvbm9zcy5jb20iLCJ0aWNrZXQiOnsidmFsaWRVcHRvIjoxNTE5MTI3NjkwNzUzLCJ2YWxpZCI6dHJ1ZSwidmFsaWRpdHlSZWFzb24iOm51bGwsIm1hc3RlckxvZ2luSWQiOiJzYXdhZG1pbkBzeW5jaHJvbm9zcy5jb20ifSwiaWF0IjoxNTE5MDQxMjkwfQ.KPQ-tairJG7c7iWPZ-qxwTVwjPd4yo4kBRsNHwenLCw

The HTTP request:

include::{snippets}/sso-authentication/http-request.adoc[]

The HTTP response:

include::{snippets}/sso-authentication/http-response.adoc[]

== SAW scheduler API :

=== Create schedule
The HTTP request:

include::{snippets}/create-schedule/http-request.adoc[]

The HTTP response:

include::{snippets}/create-schedule/http-response.adoc[]

=== Update schedule
The HTTP request:

include::{snippets}/update-schedule/http-request.adoc[]

The HTTP response:

include::{snippets}/update-schedule/http-response.adoc[]

=== List schedule

The HTTP request:

include::{snippets}/list-schedule/http-request.adoc[]

The HTTP response:

include::{snippets}/list-schedule/http-response.adoc[]

== Enable case insensitive filter
SAW supports the case insensitive filter options for data lake and elasticsearch
data sources.

=== Elasticsearch
To support case insensitive filter in elasticsearch type analysis below settings
needs to be included, while creating the elasticsearch index mapping to allow the
search as case insensitive.

    "settings": {
        "analysis": {
          "analyzer": {
            "analyzer_custom": {
              "type": "custom",
              "filter": [
                "lowercase"
              ],
              "tokenizer": "keyword"
            }
          }
        }
      }

Update the ES fields mapping with custom analyzer as below for those fields,
which are all eligible for case insensitive (String type fields)

    "product" : {
           "type" : "keyword",
           "fields" : {
             "keyword" : {
               "type" : "keyword",
               "ignore_above" : 256
             },
             "filter" : {
               "type": "text",
               "analyzer": "analyzer_custom"
             }
           }
         }
       }

NOTE: The above settings are optional to support case insensitive filtering.
 If these settings are not applied, elasticsearch analysis filters will be case
  sensitive.

=== Data Lake
To support case insensitive filter for data lake analysis, there are no
additional setting required. By default saw has native support for this option.


= Security Service API

include::{snippets}/apidoc-security/paths.adoc[]
include::{snippets}/apidoc-security/definitions.adoc[]
include::{snippets}/apidoc-security/security.adoc[]

= Semantic Service API

include::{snippets}/apidoc-semantic/paths.adoc[]
include::{snippets}/apidoc-semantic/definitions.adoc[]
include::{snippets}/apidoc-semantic/security.adoc[]

= Scheduler Service API

include::{snippets}/apidoc-scheduler/paths.adoc[]
include::{snippets}/apidoc-scheduler/definitions.adoc[]
include::{snippets}/apidoc-scheduler/security.adoc[]

= Observe Service API

include::{snippets}/apidoc-observe/paths.adoc[]
include::{snippets}/apidoc-observe/definitions.adoc[]
include::{snippets}/apidoc-observe/security.adoc[]

= Proxy Service API

include::{snippets}/apidoc-proxy/paths.adoc[]
include::{snippets}/apidoc-proxy/definitions.adoc[]
include::{snippets}/apidoc-proxy/security.adoc[]

= Workbench Service API

include::{snippets}/apidoc-workbench/paths.adoc[]
include::{snippets}/apidoc-workbench/definitions.adoc[]
include::{snippets}/apidoc-workbench/security.adoc[]
