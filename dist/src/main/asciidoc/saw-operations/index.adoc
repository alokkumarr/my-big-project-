= SAW Operations Guide
Version {project-version}
:toc:
:nofooter:
:docinfo: shared
:plantuml-config: plantuml-config

== Introduction

This document describes how to install, configure and troubleshoot SAW
and its modules in an environment.

== Installing and configuring

=== Prerequisites

Before starting an installation of SAW ensure the following has been
provided in the environment:

. Install and configure a MapR version 5.2 cluster in the environment

. Install and configure Apache Spark version 2.1 from the MapR
  Ecosystem Pack on the MapR cluster

. Install and configure Apache Livy from the MapR Ecosystem Pack on
  the MapR cluster, and set `livy.file.upload.max.size = 500000000` in
  `livy.conf`

. Provision a MapR user in the MapR cluster (with user ID 500)

. Provision a mail relay host

. Provision a host to deploy from (the deploy host), running CentOS 7
  as the operating system.  This host will be used to run the deploy
  command and store the environment configuration.  The deploy host is
  typically common to the entire environment.

. Provision a host for running SAW (the target host), with 32 GB of
  memory and CentOS 7 as the operating system.  The host should be
  dedicated to SAW and not run other applications.

. Install and configure MapR client on the SAW target host.

. Configure SAW server to use UTC timezone in all places where date and timezones are involved.

Please note that the target hosts must be allocated exclusively for
SAW use.  The underlying infrastructure up to the operating system
level are expected to be managed externally, while SAW fully manages
packages and configuration on the target hosts.

Currently SAW only supports deploying to a single target host.

=== Configuring

Each SAW environment requires its own SAW environment configuration
file.  A sample configuration is provided inside the SAW release
package in the `saw-config` file, which defaults to installing SAW on
`localhost`.  At a minimum, copy this sample file and change the
`localhost` entries to the SAW target host.

The sample configuration defaults to installing all available modules:
SAW Web, SAW Services and SAW Security.  To install only a subset of
these modules, edit the environment configuration to only mention the
desired modules.

NOTE: The environment configuration file must be preserved between
installations and upgrades.  It is recommended to put it under version
control that is stored on the deploy host and backed up to another
location.

=== Installing

Execute the following steps to install SAW:

. Get the SAW release package (named `saw-*.tgz`)

. Prepare a SAW environment configuration file, as described in the
  previous section

. Extract the package and execute the deploy command, giving it the
  path to the environment configuration file as an argument

        tar -xzf saw-*.tgz
        cd saw
        ./saw-deploy <config>

TIP: Configure passwordless SSH access to the SAW target host for a
smoother installation experience.  The deploy command should be run as
a normal user.  The deploy command will use sudo to request privileges
for relevant operations.

=== Upgrading

To upgrade an existing installation, follow the same steps as for
installing an entirely new environment.  The deploy command will
detect an already existing installation and upgrade it.

NOTE: Downgrading to an older SAW version is not supported.  If
needed, take a backup of the entire environment before upgrading that
can be used to restore the environment to a state that is supported by
older SAW versions.

==== Upgrading to SAW 2.5

Before upgrading, install and configure Apache Livy from the MapR
Ecosystem Pack as described in the updated <<Prerequisites>>.  Also
add the `saw_workbench_livy_uri` parameter to the SAW environment
configuration (`saw-config`).

=== Interfaces

The SAW Web module and supporting services are exposed on port 80 of
the SAW target host, i.e. `http://<saw-target-host>/`.  The SAW Web
application will automatically discover the endpoints for SAW Security
and SAW Services based on the URL it is being served from.  Nothing
else in the SAW deployment, except for port 80 on the SAW target host,
is accessed by external parties.

Large header settings: Include the below properties in NGINX server
config file to support, HTTP requests with large headers (more than
8K).

       client_body_buffer_size 32k;
       client_header_buffer_size 16k;
       large_client_header_buffers 8 64k;

File upload limit settings: Include the below properties in NGINX server
config file to support larger files upload (more than 1MB)

       client_max_body_size 25m;

== Maintenance and troubleshooting

=== Accessing application logs

The SAW systemd services system logs can be accessed using the `sudo
journalctl` command.  To view the logs of individual services, use the
`-u` option:

        $ sudo journalctl -u saw-\*

=== Listing services and their statuses

To list services and check the status of all SAW systemd units,
execute the following commands:

        $ sudo systemctl list-units saw-\*

NOTE: Some services use
http://0pointer.de/blog/projects/socket-activation.html[socket
activation] to reduce memory usage and shorten deploy times.  These
services will be listed as not running (inactive dead) until the first
connection is made over the network.  This is normal for
socket-activated services and does not indicate a problem.

=== Starting, stopping and restarting services

Under normal circumstances there should be no need to start, stop or
restart SAW services manually.  However, if needed it can be done
using the following commands:

        $ sudo systemctl start <saw-service>
        $ sudo systemctl stop <saw-service>
        $ sudo systemctl restart <saw-service>

Where `<saw-service>` is one of the SAW systemd services (for example
`saw-gateway`), which can be listed using the `sudo systemctl
list-units saw-\*` command shown in the previous section.

=== Clearing the Transport Service Executor queues

If the SAW report execution queue has filled up, for example due to
many long-running queries being executed, the queues can be cleared
using the following commands:

        $ ssh <mapr-host>
        $ stream=<report-executor-path>/saw-transport-executor-regular-stream
        $ sudo -u mapr maprcli stream topic delete -path $stream -topic executions
        $ stream=<report-executor-path>/saw-transport-executor-fast-stream
        $ sudo -u mapr maprcli stream topic delete -path $stream -topic executions

* <report-executor-path> can be found in saw-transport service configuration file.

Please note that clearing the queues affects all users of the system
and report execution types.

=== Automatically generated credentials

Automatically generated credentials, such as for internal service and
administrator accounts, can be found in the `/etc/bda` directory on
the respective host.

== Loading semantic metadata

To enable creating analyses in SAW, load semantic metadata as follows:

        $ ssh <saw-services-host>
        $ sudo -u mapr /opt/saw/service/bin/mdcli.sh -i \
            file://<nodes-json> -o file:///tmp/log.json

The semantic metadata JSON is stored in the `<nodes-json>` file.

=== Semantic metadata format

Semantic metadata supports the following values for the `type`
property:

* `integer`
* `long`
* `float`
* `double`
* `string`
* `date`

NOTE: Paths to files in the data lake must not contain spaces.

== Creating data security keys (DSK)

SAW supports row level filtering using a data security key configured
in SAW Security.

=== Prerequisites

DSK configured columns should be present in ALL of the data
objects/artifacts referenced in the metrics.

=== Instructions

. Create the security group in the SEC_GROUP table in the SAW Security
database:

    INSERT INTO `SEC_GROUP` (`SEC_GROUP_SYS_ID`, `ACTIVE_STATUS_IND`, `CREATED_DATE`, `CREATED_BY`) VALUES ('1', '1', '2017-10-04', 'system');

. Create DSK attribute (fields/columns name) for corresponding
security group (SEC_GROUP created in step 1):

    INSERT INTO `sec_group_dsk_attribute` (`SEC_GROUP_DSK_ATTRIBUTE_SYS_ID`, `SEC_GROUP_SYS_ID`, `ATTRIBUTE_NAME`) VALUES ('1', '1', 'SESSION_ID');
    INSERT INTO `sec_group_dsk_attribute` (`SEC_GROUP_DSK_ATTRIBUTE_SYS_ID`, `SEC_GROUP_SYS_ID`, `ATTRIBUTE_NAME`) VALUES ('2', '1', 'CONTENT_CLASS');

. Create DSK values for corresponding DSK attribute (DSK attribute
created in step 2):

    INSERT INTO `sec_group_dsk_value` (`SEC_GROUP_DSK_VALUE_SYS_ID`, `SEC_GROUP_DSK_ATTRIBUTE_SYS_ID`, `DSK_VALUE`) VALUES ('1', '1', 'AFF2948C-DCFF-4944-8553-51435518AF67');
    INSERT INTO `sec_group_dsk_value` (`SEC_GROUP_DSK_VALUE_SYS_ID`, `SEC_GROUP_DSK_ATTRIBUTE_SYS_ID`, `DSK_VALUE`) VALUES ('2', '1', '945ca612-d3ad-4e6e-9c92-7cff86730235');
    INSERT INTO `sec_group_dsk_value` (`SEC_GROUP_DSK_VALUE_SYS_ID`, `SEC_GROUP_DSK_ATTRIBUTE_SYS_ID`, `DSK_VALUE`) VALUES ('3', '2', 'VIDEOS');

. Map the SEC_GROUP to users to apply the DSK filter:

    UPDATE USERS SET SEC_GROUP_SYS_ID = '3' WHERE USER_ID = 'analyst.dsk.example_table.report';

NOTE: If any metrics contains more than one data object as analysis
for report then DSK attribute should be configured with
`dataObjectName.columnName`.  Example: For EXAMPLE_TABLE data object,
the DSK attribute name should be EXAMPLE_TABLE.ID.


== Onboarding customer

We can utilise customer_onboard.sh script in order to execute the command with current environment setup.

    cd /opt/bda/saw-security/bin/
    bash customer_onboard.sh

Features of spring boot shell:

. Type in "help" and it will show you all the available commands

. Tab based auto completion is supported.


    shell:>help
    AVAILABLE COMMANDS
    Built-In Commands
            clear: Clear the shell screen.
            exit, quit: Exit the shell.
            help: Display help about available commands.
            script: Read and execute commands from a file.
            stacktrace: Display the full stacktrace of the last error.
    Saw Security Shell
            onboard-customer: Onboard the customer
    shell:>


Once you are inside the shell, type in onboard-customer and it will start the process of creating customer and related products/components in the system.

In below example, it starts with showing you which products are present in system and asks for basic customer information.


    shell:>onboard-customer
    Customer information:
    1
    {PRODUCT_ID=1, PRODUCT_NAME=MCT Insights}
    {PRODUCT_ID=2, PRODUCT_NAME=SnT Insighjts}
    {PRODUCT_ID=3, PRODUCT_NAME=Smart Care Insights}
    {PRODUCT_ID=4, PRODUCT_NAME=SAW Demo}
    {PRODUCT_ID=5, PRODUCT_NAME=Channel Insights}
    ====== CUSTOMERS INFORMATION ======
    Enter CUSTOMER_CODE: (UNIQUE CODE TO IDENTIFY your company / division) temp
    Enter COMPANY NAME: temp
    Enter COMPANY BUSINESS: temp
    Enter PRODUCT ID from above for default landing page: 4
    Enter DOMAIN_NAME: abc.com
    Generated CUSTOMER_SYS_ID: 2
    2018-01-03 10:09:43.676  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Created user with ID: 2


In this case the generated customer_sys_id is 16. It continues to show product information as we need to associate these products with customers, in my case I chose 4 which is for saw demo.

    {PRODUCT_ID=1, PRODUCT_NAME=MCT Insights}
    {PRODUCT_ID=2, PRODUCT_NAME=SnT Insighjts}
    {PRODUCT_ID=3, PRODUCT_NAME=Smart Care Insights}
    {PRODUCT_ID=4, PRODUCT_NAME=SAW Demo}
    {PRODUCT_ID=5, PRODUCT_NAME=Channel Insights}
    ====== CUSTOMER_PRODUCTS TABLE ======
    Enter PRODUCT_SYS_ID: 4
    class org.springframework.jdbc.support.GeneratedKeyHolder
    2
    Generated CUST_PROD_SYS_ID: 2
    2018-01-03 12:42:32.522  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Created CUST_PROD entry with ID: 2

In this example the generated customer product linkage ID is 11. It continues with displaying modules of all products, sicne we chose saw demo i.e. 4 in previous case. It makes sense to select modules of that product only. i.e. in this case either 4, 7 or 8.

    {MODULE_ID=1, PRODUCT_NAME=MCT Insights, MODULE_NAME=OBSERVE}
    {MODULE_ID=2, PRODUCT_NAME=SnT Insighjts, MODULE_NAME=OBSERVE}
    {MODULE_ID=3, PRODUCT_NAME=Smart Care Insights, MODULE_NAME=OBSERVE}
    {MODULE_ID=4, PRODUCT_NAME=SAW Demo, MODULE_NAME=ANALYZE}
    {MODULE_ID=5, PRODUCT_NAME=Channel Insights, MODULE_NAME=OBSERVE}
    {MODULE_ID=6, PRODUCT_NAME=MCT Insights, MODULE_NAME=ANALYZE}
    {MODULE_ID=7, PRODUCT_NAME=SAW Demo, MODULE_NAME=ALERT}
    {MODULE_ID=8, PRODUCT_NAME=SAW Demo, MODULE_NAME=OBSERVE}
    ====== CUSTOMER PRODUCT MODULES ======
    Enter MODULE_ID (from above shown values):
    4
    Enter more? (yes/no): yes
    Enter MODULE_ID (from above shown values):
    7
    Enter more? (yes/no): yes
    Enter MODULE_ID (from above shown values):
    8
    Enter more? (yes/no): no

It continues with displaying that it's creating the relationships and admin role in background followed by creating admin user for the customer.


    ====== ASSOCIATING DEFAULT FEATURES ======
    ====== CREATING ADMIN ROLE ======
    2018-01-03 12:42:50.059  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Created Admin Role for above customer with ID: 5
    ====== USERS TABLE for ADMIN USER ======
    Enter MASTER_LOGIN:
    temp@abc.com
     Enter EMAIL: temp@abc.com
    Enter PASSWORD: pleasechangepassword
    Enter FIRST_NAME:
    temp
    Enter MIDDLE_NAME:
    temp_mn
    Enter LAST_NAME:
    temp_ln
    Generated User ID for current user is: 5
    2018-01-03 12:43:28.084  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Created Admin user with ID: 5
    ====== CREATING PRIVILEGES FOR ADMIN ======
    2018-01-03 12:43:28.110  INFO 6307 --- [           main] c.s.s.s.app.admin.SawSecurityShell       : Generated Privilege ID for Admin user: 43
    shell:>
    shell:>


==  SAW SSO Authentication

SAW supports external systems to authenticate users (single sign-on).The shared secret key is read from the SAW environment configuration, as a base64 encoded string (while ensuring Synchronoss Global Information Security standards for storing secret keys are adhered to).
Recommended key size is 256 bits.

   Command to generate key : openssl rand 32 -base64
   Dgus5PoaEHm2tKEjy0cUGnzQlx86qiutmBZjPbI4y0U=

After generating the key, add it to the SAW environment configuration ({{saw-config}}) in the {{saw_security_sso_secret}} parameter and redeploy.

== FTP server configuration

SAW supports exporting of pivots and reports to ftp/sftp servers.
By default an empty configuration is installed in
`/opt/bda/saw-export-service/conf/ftp-details.json` file on saw nodes.

The contents of this configuration can be changed using `saw-config`.
An example configuration has been included in config file.

.saw-config
----
# FTP JSON config
# ##########################################
#
#
# DO NOT SPLIT THIS INTO MULTIPLE LINES
#
#
# ##########################################
# ftp_json_config='{"ftpList":[{"customerName":"CUSTUNIQUEID","alias":"ftpsrv1","host":"srv1","port":21,"username":"usr1","password":"pwd1","location":"/path/to/dir/","type":"ftp"}]}'
----

Example contents (in pretty format):

.ftp-details.json
[source, json]
----
{
  "ftpList": [
    {
        "customerName":"UNIQUE_IDENTIFIER1",
        "alias" : "server1",
        "host": "server1.customer1.com",
        "port": 21,
        "username": "usr1",
        "password": "pwd1",
        "location": "/some/location/",
        "type": "ftp"
    },
    {
        "customerName":"UNIQUE_IDENTIFIER1",
        "alias" : "server2",
        "host": "server2.customer1.com",
        "port": 22,
        "username": "usr2",
        "password": "pwd2",
        "location": "/some/location/",
        "type": "sftp"
    },
    {
        "customerName":"UNIQUE_IDENTIFIER2",
        "alias" : "server1",
        "host": "server1.customer2.com",
        "port": 21,
        "username": "imuser1",
        "password": "pwd3",
        "location": "/home/ubuntu",
        "type": "ftp"
    }
  ]
}
----

In above example, `customerName` is the unique identifier given at
the time of onboarding customer. Note that based on this unique
identifier, customers are differentiated. Each FTP/SFTP
server is required to have unique entry which gets presented to front
end, this is maintained by means of `alias` entry. *Note* that each
server entry per customer is required to have a unique alias entry
which gets presented in front end.

[Note]
====
Please make sure to put minified JSON in configuration file.
====
