% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/reader.R
\name{reader}
\alias{reader}
\title{Spark Reader Function}
\usage{
reader(sc, name, path, type, repartition = 0, memory = TRUE,
  overwrite = TRUE, ...)
}
\arguments{
\item{sc}{A spark_connection.}

\item{name}{The name to assign to the newly generated table}

\item{path}{The path to the file. If data is distributed, supply the dfs
protocol}

\item{type}{required file type. Specifiecs the sub reader function. Accepts
csv, parquet, json, jdbc, source, table and text}

\item{repartition}{The number of partitions used to distribute the generated
table. Use 0 (the default) to avoid partitioning.}

\item{memory}{Logical; should the data be loaded eagerly into memory? (That
is, should the table be cached?)}

\item{overwrite}{Logical; overwrite the table with the given name if it
already exists?}

\item{...}{additional arguments to read function}
}
\value{
Spark Dataframe
}
\description{
Function to read files into Spark. Supports several file types, including
text, csv, parquet, and json
}
\details{
If directory path provided, reader will scan the directory and calculate the
most frequency file type and apply the appropriate read function
}
