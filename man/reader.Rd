% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/reader.R
\name{reader}
\alias{reader}
\title{Spark Reader Function}
\usage{
reader(sc, name, path, repartition = 0, memory = TRUE, overwrite = TRUE,
  ...)
}
\arguments{
\item{sc}{A spark_connection.}

\item{name}{The name to assign to the newly generated table}

\item{path}{The path to the file. Needs to be accessible from the cluster.
Supports the "hdfs://", "s3a://" and "file://" protocols}

\item{repartition}{The number of partitions used to distribute the generated
table. Use 0 (the default) to avoid partitioning.}

\item{memory}{Logical; should the data be loaded eagerly into memory? (That
is, should the table be cached?)}

\item{overwrite}{Logical; overwrite the table with the given name if it
already exists?}

\item{...}{}
}
\description{
Function to read files into Spark. Supports several file types, including
text, csv, parquet, and json
}
\details{
If directory path provided, reader will scan the directory and calculate the
most frequency file type and apply the appropriate read function
}
