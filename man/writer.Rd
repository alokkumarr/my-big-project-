% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/writer.R
\name{writer}
\alias{writer}
\alias{writer.tbl_spark}
\alias{writer.data.frame}
\title{DataFrame Writer Function}
\usage{
writer(df, path, dfs_mount, type, mode, partitions, partition_by,
  temp_folder_name)

\method{writer}{tbl_spark}(df, path, dfs_mount = "", type, mode = NULL,
  partitions = NULL, partition_by = NULL, temp_folder_name = "tmp", ...)

\method{writer}{data.frame}(df, path, type, mode = NULL, ...)
}
\arguments{
\item{df}{DataFrame}

\item{path}{file path to write file to. Should include protocol if distributed
path}

\item{dfs_mount}{dfs mount file path. applicable for distributed files only}

\item{type}{file type. type specifies the spark writer function. Only
applicable for spark writer method}

\item{mode}{Specifies the behavior when data or table already exists.
Supported values include: 'error', 'append', 'overwrite' and 'ignore'.
Notice that 'overwrite' will also change the column structure. default is
NULL}

\item{partition_by}{Partitions the output by the given columns on the file
system}

\item{temp_folder_name}{name of temp folder}

\item{...}{optional arguments passed to write function}
}
\description{
This is a function to write dataframes to files. Supports writing csv, text,
json and parquet file types.
}
\details{
The default values for Spark method with delimited type are the consistent
with spark_write_csv function, comma. For files with a different delimiter,
include the dilimiter argument, i.e. dilimeter="|" for pipe. Parquet files use
default spark_read_parquet funtionality. Supports overwrite, append, and error
modes.

Function writes files to temporary folder and then copies a renamed version to
destination directory
}
