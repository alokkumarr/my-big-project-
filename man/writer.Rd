% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/writer.R
\name{writer}
\alias{writer}
\alias{writer.tbl_spark}
\title{DataFrame Writer Function}
\usage{
writer(df, path, mode, partition_by, temp_folder_name)

\method{writer}{tbl_spark}(df, path, mode = NULL, partition_by = NULL,
  temp_folder_name = "tmp", ...)
}
\arguments{
\item{mode}{Specifies the behavior when data or table already exists.
Supported values include: 'error', 'append', 'overwrite' and 'ignore'. Notice
that 'overwrite' will also change the column structure. default is NULL}

\item{partition_by}{Partitions the output by the given columns on the file
system}

\item{temp_folder_name}{name of temp folder}

\item{...}{optional arguments passed to write function}

\item{data}{Spark DataFrame}

\item{folder}{directory to the write files to (directory only).}

\item{file}{file name. file type inferred by extension provided. invokes
specific writer function}
}
\description{
This is a function to write dataframes to files. Supports writing csv, text,
json and parquet file types.
}
\details{
The default values for Spark method with delimited type are the consistent
with spark_write_csv function, comma. For files with a different delimiter,
include the dilimiter argument, i.e. dilimeter="|" for pipe. Parquet files use
default spark_read_parquet funtionality. Supports overwrite, append, and error
modes.

Function writes files to temporary folder and then copies a renamed version to
destination directory
}
